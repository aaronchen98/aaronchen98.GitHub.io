<!DOCTYPE html>
<html lang>
  <head><meta name="generator" content="Hexo 3.9.0">
    
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,minimum-scale=1,maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="description" content="主成分分析">




  <meta name="keywords" content="机器学习,">





  <link rel="alternate" href="/atom.xml" title="浩瀚宇宙·AaronChen">




  <link rel="shortcut icon" type="image/x-icon" href="/img/1.jpg?v=1.1">



<link rel="canonical" href="http://hhyz.me/2019/07/03/2019-04-22-PCA/">


<meta name="description" content="PCA的思想就是将n维特征映射到k维上(k&amp;lt;n)，这k维是重新构造出来的全新维度特征，而不是简单的从n维特征去除n-k维特征，这k维就是主成分。 在三维坐标系中，四种颜色的标记界限并不直观，但是在重新定义的二维坐标系中，四种颜色标记界限非常直观，这就是一个典型的PCA，在降维的同时最大程度的保留了数据的特征，为后续的分析提供更直观的支持。  基础数学知识方差$${Var}(X)=\frac{">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="主成分分析">
<meta property="og:url" content="http://hhyz.me/2019/07/03/2019-04-22-PCA/index.html">
<meta property="og:site_name" content="浩瀚宇宙·AaronChen">
<meta property="og:description" content="PCA的思想就是将n维特征映射到k维上(k&amp;lt;n)，这k维是重新构造出来的全新维度特征，而不是简单的从n维特征去除n-k维特征，这k维就是主成分。 在三维坐标系中，四种颜色的标记界限并不直观，但是在重新定义的二维坐标系中，四种颜色标记界限非常直观，这就是一个典型的PCA，在降维的同时最大程度的保留了数据的特征，为后续的分析提供更直观的支持。  基础数学知识方差$${Var}(X)=\frac{">
<meta property="og:locale" content="zh">
<meta property="og:image" content="http://hhyz.me/img/2019-04-22-PCA-1.jpg">
<meta property="og:image" content="http://hhyz.me/img/2019-04-22-PCA-2.jpg">
<meta property="og:image" content="http://hhyz.me/img/2019-04-22-PCA-3.jpg">
<meta property="og:image" content="http://hhyz.me/img/2019-04-22-PCA-4.jpg">
<meta property="og:updated_time" content="2019-07-02T17:05:20.731Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="主成分分析">
<meta name="twitter:description" content="PCA的思想就是将n维特征映射到k维上(k&amp;lt;n)，这k维是重新构造出来的全新维度特征，而不是简单的从n维特征去除n-k维特征，这k维就是主成分。 在三维坐标系中，四种颜色的标记界限并不直观，但是在重新定义的二维坐标系中，四种颜色标记界限非常直观，这就是一个典型的PCA，在降维的同时最大程度的保留了数据的特征，为后续的分析提供更直观的支持。  基础数学知识方差$${Var}(X)=\frac{">
<meta name="twitter:image" content="http://hhyz.me/img/2019-04-22-PCA-1.jpg">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1">
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">





<script type="text/javascript">
  var themeConfig = {
    fancybox: {
      enable: false
    },
  };
</script>




  





  


    <title> 主成分分析 - 浩瀚宇宙·AaronChen </title>
  </head>

  <body>
    <div id="page">
      <header id="masthead"><div class="site-header-inner">
    <h1 class="site-title">
        <a href="/." class="logo">浩瀚宇宙·AaronChen</a>
    </h1>

    <nav id="nav-top">
        
            <ul id="menu-top" class="nav-top-items">
                
                    <li class="menu-item">
                        <a href="http://huangyijiegallery.lofter.com/">
                            
                            
                                Gallery
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/about">
                            
                            
                                About
                            
                        </a>
                    </li>
                
            </ul>
        
  </nav>
</div>

      </header>
      <div id="content">
        
    <div id="primary">
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          主成分分析
        
      </h1>

      <time class="post-time">
          Jul 3 2019
      </time>
    </header>



    
            <div class="post-content">
            <p>PCA的思想就是将n维特征映射到k维上(k&lt;n)，这k维是重新构造出来的全新维度特征，而不是简单的从n维特征去除n-k维特征，这k维就是主成分。</p>
<p>在三维坐标系中，四种颜色的标记界限并不直观，但是在重新定义的二维坐标系中，四种颜色标记界限非常直观，这就是一个典型的PCA，在降维的同时最大程度的保留了数据的特征，为后续的分析提供更直观的支持。</p>
<p><img src="/img/2019-04-22-PCA-1.jpg" alt></p>
<h2 id="基础数学知识"><a href="#基础数学知识" class="headerlink" title="基础数学知识"></a>基础数学知识</h2><h3 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h3><p>$$<br>{Var}(X)=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2}<br>$$</p>
<h3 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h3><p>$$<br>{Cov}(X, Y)=\frac{1}{n} \sum_{i=1}^{n} X_{i} Y_{i}=\frac{1}{n} X \cdot Y<br>$$</p>
<p>当 \(Cov(a,b)=0\)  时，表示两个字段完全独立，这也是我们的优化目标。</p>
<h3 id="特征值分解"><a href="#特征值分解" class="headerlink" title="特征值分解"></a>特征值分解</h3><p>如果说一个向量 \(v\) 是方阵 \(A\) 的特征向量，将一定可以表示成下面的形式：</p>
<p>$$<br>A v=\lambda v<br>$$</p>
<p>这时候 \(λ\) 就被称为特征向量 \(v\) 对应的特征值。<br>一个矩阵的一组<strong>特征向量</strong>是一组<strong>正交向量</strong>。</p>
<p><strong>特征分解：</strong></p>
<p>$$<br>A=Q \Sigma Q^{-1}<br>$$</p>
<p>其中 \(Q\) 是这个矩阵 \(A\) 的特征向量组成的矩阵，正交矩阵是可逆的。</p>
<p>\(\Sigma=diag\left(\lambda_{1}, \lambda_{2}, \dots, \lambda_{n}\right)\) 是一个对角阵，每一个对角线上的元素就是一个特征值。</p>
<blockquote>
<p>一个矩阵其实就是一个线性变换，因为一个矩阵乘以一个向量后得到的向量，其实就相当于将这个向量进行了线性变换。    </p>
<p><strong>特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么</strong></p>
</blockquote>
<h3 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h3><p>特征值分解是一个提取矩阵特征很不错的方法，但是它只<strong>适用于方阵</strong>。而在现实的世界中，我们看到的大部分矩阵都不是方阵，而奇异值分解是一个能<strong>适用于任意的矩阵</strong>的一种分解的方法。</p>
<p>奇异值分解实际上把矩阵的变换分为了三部分：</p>
<ul>
<li>旋转</li>
<li>拉伸</li>
<li>投影</li>
</ul>
<p><img src="/img/2019-04-22-PCA-2.jpg" alt></p>
<p>$$<br>A=U \Sigma V^{T}<br>$$</p>
<p><img src="/img/2019-04-22-PCA-3.jpg" alt></p>
<ul>
<li>\(A\) 是一个 \(M * N\) 的矩阵</li>
<li>\(U\) 是一个 \(M * M\) 的方阵（里面的向量是正交的，U里面的向量称为<strong>左奇异向量</strong>），</li>
<li>\(Σ\) 是一个 \(M * N\) 的实数对角矩阵（对角线以外的元素都是 \(0\) ，对角线上的元素称为<strong>奇异值</strong>），</li>
<li>\(V^{T}\) 是一个 \(N * N\) 的矩阵，里面的向量也是正交的， \(V\) 里面的向量称为<strong>右奇异向量</strong>）</li>
</ul>
<p>奇异值和特征值是怎么对应起来的呢？</p>
<p>首先，我们将一个矩阵A的转置 \(\mathrm{A}^{\top} * \mathrm{A}\)，将会得到 \(\mathrm{A}^{\top} \mathrm{A}\) 是一个方阵，我们用这个方阵求特征值可以得到：</p>
<p>$$<br>\left(A^{T} A\right) v_{i}=\lambda_{i} v_{i}<br>$$<br>并且有：<br>$$<br>\sigma_{i}=\sqrt{\lambda_{i}}<br>$$<br>$$<br>u_{i}=\frac{1}{\sigma_{i}} A v_{i}<br>$$</p>
<ul>
<li>\(σ_i\)  是就是奇异值， </li>
<li>\(u_i\) 是左奇异向量</li>
<li>\(v\) 是右奇异向量</li>
</ul>
<p>常见的做法是将奇异值由大而小排列。如此 \(Σ\) 便能由 \(M\) 唯一确定。</p>
<p>$$<br>A=U \Sigma V^{T} \Rightarrow A V=U \Sigma V^{T} V \Rightarrow A V=U \Sigma \Rightarrow A v_{i}=\sigma_{i} u_{i} \Rightarrow \sigma_{i}=A v_{i} / u_{i}<br>$$</p>
<p>奇异值 \(σ\) 跟特征值类似，在矩阵 \(Σ\) 中也是从大到小排列，而且 \(σ\) 的减少特别的快，在很多情况下，前 10% 甚至 1% 的奇异值的和就占了全部的奇异值之和的 99% 以上了。</p>
<p>也就是说，我们也可以用前 \(r\) 大的奇异值来近似描述矩阵</p>
<p><strong>部分奇异值分解：</strong></p>
<p>$$<br>A_{m \times n} \approx U_{m \times r} \Sigma_{r \times r} V_{r \times n}^{T}<br>$$</p>
<p> \(r\) 是一个远小于 \(m\)、 \(n\) 的数，这样矩阵的乘法看起来像是下面的样子：</p>
<p><img src="/img/2019-04-22-PCA-4.jpg" alt></p>
<p>右边的三个矩阵相乘的结果将会是一个接近于 \(A\) 的矩阵，在这儿， \(r\) 越接近于 \(n\) ，则相乘的结果越接近于 \(A\) 。而这三个矩阵的面积之和（在存储观点来说，矩阵面积越小，存储量就越小）要远远小于原始的矩阵 \(A\)。 </p>
<p>如果想要压缩空间来表示原矩阵 \(A\)，只需存下三个矩阵：\(U、Σ、V\)。</p>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"></script>

            </div>
          

    
      <footer class="post-footer">
        <div class="post-tags">
          
            <a href="/tags/机器学习/">机器学习</a>
          
        </div>

        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2019/07/03/2019-04-25-Clustering/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">聚类</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/2019/07/03/2019-04-10-SVM/">
        <span class="next-text nav-default">支持向量机</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

        
  <div class="comments" id="comments">
    
  </div>


      </footer>
    
  </article>

    </div>

        
      <!--noindex-->
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#基础数学知识"><span class="nav-text">基础数学知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#方差"><span class="nav-text">方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#协方差"><span class="nav-text">协方差</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特征值分解"><span class="nav-text">特征值分解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#奇异值分解"><span class="nav-text">奇异值分解</span></a></li></ol></li></ol></div>
            

          </div>
      <!--/noindex-->
      
      </div>
      <footer id="colophon"><span class="copyright-year">
    
        &copy;
    
        2017 -
    
    2019
    <span class="footer-author">haoyu.</span>
    <span class="power-by">
        Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a> and <a class="theme-link" href="https://github.com/henryhuang/hexo-theme-polarbearsimple">Polar Bear Simple</a>
    </span>
</span>

      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>
    


    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  

    <script type="text/javascript" src="/js/src/theme.js?v=1.1"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=1.1"></script>

  </body>
</html>
