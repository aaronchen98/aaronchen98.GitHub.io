<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="一直在上路">
    <meta name="keyword" content>
    <link rel="shortcut icon" href="/img/favicon.ico">

    <title>
        
        深度学习模型压缩 Model Compression - 浩瀚宇宙的博客 | AaronChen&#39;s Blog
        
    </title>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/aircloud.css">
    <link rel="stylesheet" href="/css/gitment.css">
    <!--<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">-->
    <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
    <!-- ga & ba script hoook -->
    <script></script>
</head>

<body>

<div class="site-nav-toggle" id="site-nav-toggle">
    <button>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
        <span class="btn-bar"></span>
    </button>
</div>

<div class="index-about">
    <i> 一直在上路 </i>
</div>

<div class="index-container">
    
    <div class="index-left">
        
<div class="nav" id="nav">
    <div class="avatar-name">
        <div class="avatar ">
            <img src="/" />
        </div>
        <div class="name">
            <i>haoyu</i>
        </div>
    </div>
    <div class="contents" id="nav-content">
        <ul>
            <li >
                <a href="/">
                    <i class="iconfont icon-shouye1"></i>
                    <span>主页</span>
                </a>
            </li>
            <li >
                <a href="/tags">
                    <i class="iconfont icon-biaoqian1"></i>
                    <span>标签</span>
                </a>
            </li>
            <li >
                <a href="/archives">
                    <i class="iconfont icon-guidang2"></i>
                    <span>存档</span>
                </a>
            </li>
            <li >
                <a href="/about/">
                    <i class="iconfont icon-guanyu2"></i>
                    <span>关于</span>
                </a>
            </li>
            
            <li>
                <a id="search">
                    <i class="iconfont icon-sousuo1"></i>
                    <span>搜索</span>
                </a>
            </li>
            
        </ul>
    </div>
    
        <div id="toc" class="toc-article">
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-前言"><span class="toc-text">1 前言</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-网络剪枝"><span class="toc-text">2 网络剪枝</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#方式-一"><span class="toc-text">方式 一</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#算法流程"><span class="toc-text">算法流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#全连接层剪枝"><span class="toc-text">全连接层剪枝</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#卷积层剪枝"><span class="toc-text">卷积层剪枝</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#实验结果"><span class="toc-text">实验结果</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#方式-二"><span class="toc-text">方式 二</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#剪枝的敏感度（Sensitivity）"><span class="toc-text">剪枝的敏感度（Sensitivity）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#多层剪枝的策略"><span class="toc-text">多层剪枝的策略</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#方式-三"><span class="toc-text">方式 三</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考"><span class="toc-text">参考</span></a></li></ol>
</div>
    
</div>


<div class="search-field" id="search-field">
    <div class="search-container">
        <div class="search-input">
            <span id="esc-search"> <i class="icon-fanhui iconfont"></i></span>
            <input id="search-input"/>
            <span id="begin-search">搜索</span>
        </div>
        <div class="search-result-container" id="search-result-container">

        </div>
    </div>
</div>

        <div class="index-about-mobile">
            <i> 一直在上路 </i>
        </div>
    </div>
    
    <div class="index-middle">
        <!-- Main Content -->
        


<div class="post-container">
    <div class="post-title">
        深度学习模型压缩 Model Compression
    </div>

    <div class="post-meta">
        <span class="attr">发布于：<span>2019-07-03 01:05:20</span></span>
        
        <span class="attr">标签：/
        
        <a class="tag" href="/tags/#深度学习" title="深度学习">深度学习</a>
        <span>/</span>
        
        
        </span>
        <span class="attr">访问：<span id="busuanzi_value_page_pv"></span>
</span>
</span>
    </div>
    <div class="post-content ">
        <h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1 前言"></a>1 前言</h2><p>目前深度学习模型压缩方法的研究主要可以分为以下几个方向： </p>
<ul>
<li><p><strong>更精细模型的设计</strong>   </p>
<p>  目前的很多网络都具有模块化的设计，在深度和宽度上都很大，这也造成了参数的冗余很多，因此有很多关于模型设计的研究，如 <code>SqueezeNet</code>、<code>MobileNet</code> 等，使用更加细致、高效的模型设计，能够很大程度的减少模型尺寸，并且也具有不错的性能。 </p>
</li>
<li><p><strong>参数修剪和共享（parameter pruning and sharing）</strong></p>
<p>  结构复杂的网络具有非常好的性能，其参数也存在冗余，因此对于已训练好的模型网络，可以寻找一种有效的评判手段，将不重要的 connection 或者 filter 进行裁剪来去除冗余和不重要的项。 </p>
</li>
</ul>
<ul>
<li><p><strong>核的稀疏化</strong>    </p>
<p>  在训练过程中，对权重的更新进行诱导，使其更加稀疏，对于稀疏矩阵，可以使用更加紧致的存储方式，如 CSC，但是使用稀疏矩阵操作在硬件平台上运算效率不高，容易受到带宽的影响，因此加速并不明显。</p>
</li>
</ul>
<blockquote>
<p>稀疏矩阵是指矩阵中的元素大部分是0的矩阵，事实上，实际问题中大规模矩阵基本上都是稀疏矩阵，很多稀疏度在90%甚至99%以上。因此我们需要有高效的稀疏矩阵存储格式。    </p>
</blockquote>
<ul>
<li><p><strong>权值共享</strong></p>
<p>  就是让一些边共用一个权值，达到缩减参数个数的目的。假设相邻两层之间是全连接，每层有 \(1000\) 个节点，那么这两层之间就有 \(1000×1000=100万\) 个权重参数。可以将这一百万个权值做聚类，利用每一类的均值代替这一类中的每个权值大小，这样同属于一类的很多边共享相同的权值，假设把一百万个权值聚成一千类，则可以把参数个数从一百万降到一千个。<br>  可以直接使用简单的 K-means，对每一层都做一个weight的聚类，属于同一个 cluster 的就共享同一个权值大小。 注意的一点：跨层的weight不进行共享权值；</p>
</li>
</ul>
<ul>
<li><p><strong>量化</strong></p>
<p>  一般而言，神经网络模型的参数都是用的32bit长度的浮点型数表示，实际上不需要保留那么高的精度，可以通过量化，比如用0~255表示原来32个bit所表示的精度，通过牺牲精度来降低每一个权值所需要占用的空间。<br>  如果我们存储的参数为256个，那么只需要8-bit整数就可以索引，相比于所有位置都存32bit的浮点数，模型的存储量可以下降到原来的1/4。</p>
</li>
</ul>
<p>除此之外，<strong>Low-rank分解</strong>、<strong>迁移学习</strong>等方法也有很多研究，并在模型压缩中起到了非常好的效果。</p>
<h2 id="2-网络剪枝"><a href="#2-网络剪枝" class="headerlink" title="2 网络剪枝"></a>2 网络剪枝</h2><h3 id="方式-一"><a href="#方式-一" class="headerlink" title="方式 一"></a>方式 一</h3><blockquote>
<p>论文地址：<a href="http://arxiv.org/abs/1512.01891" target="_blank" rel="noopener">Sparsifying Neural Network Connections for Face Recognition</a></p>
</blockquote>
<h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a><strong>算法流程</strong></h4><p>从网络的最后一层开始，根据一定规则对该层进行剪枝，然后retrain网络，循环上述过程。</p>
<p><strong>剪枝的实现方法</strong></p>
<p>就是为权重施加一个相同大小的Mask, Mask中只有激活的地方才是1，其余全0.</p>
<div align="center">
![](/img/2018-06-26-ModelCompression-1.jpg)
<center><small><font color="gray"> 算法流程图   </font></small></center>
</div>

<h4 id="全连接层剪枝"><a href="#全连接层剪枝" class="headerlink" title="全连接层剪枝"></a><strong>全连接层剪枝</strong></h4><p>对于如全连接和局部连接这些没有权值共享的层，我们可以很简单的计算神经元之间的相关性：</p>
<p>假设  \(a_i\)  是当前层的一个神经元， 上一层有  \(K\)  个神经元，则此时 \(a_i\) 与上一层之间应该有 \(K\) 个连接，即 \(K\) 个权重参数：  \(b_{i1},b_{i2} … b_{iK}\)  。 于是我们可以用下式计算 \(a_i\) 与每一个  \(b_{ik}\)  的相关系数 ：</p>
<p>$$r_{ik} = \frac{E[a_i-\mu_{ai}][_i-\mu_{bik}]}{\sigma_{ai}\sigma_{bik}}$$</p>
<p>其中  \(μ\)  和  \(\sigma\) 分别是在验证集上计算得到的均值与方差。</p>
<p><strong>正相关和负相关同样重要，而且实验发现保留一些相关性较小的权重也会提高实验效果。</strong></p>
<p>于是，作者首先将所有正相关的  \(r_{ik}\)  降序排列，然后均分为两部分，在前一部分随机采样  \(λSK+\)  个，在后面一部分随机采样  \((1−λ)SK+\)  个, 其中  \(S\)  为事先确定的稀疏度，  \(λ\) 文中设定为 \(0.75\)  。对负相关采取同样操作。据此，我们可以创建出表示剪枝的掩膜矩阵。</p>
<h4 id="卷积层剪枝"><a href="#卷积层剪枝" class="headerlink" title="卷积层剪枝"></a><strong>卷积层剪枝</strong></h4><p>卷积层剪枝稍微复杂一点，因为存在权值共享。</p>
<p>设  \(a_{im}\)  是当前层第  \(i\)  个 feature map 中的第  \(m\)  神经元，该 feature map 中的共有  \(M\)  个神经元（ \(m=1,2,…,M\) ）。显然，根据卷积规则，这  \(M\)  个神经元都只与一个卷积核有关，即  \(K\)  个权值有关 (  \(K\)  为 filter size，例如  \(3 × 3\)，再乘以输入 channel 的数量)。\(b_{mk}\)  应该是上一层 feature map 中卷积的部分，该部分的位置与  \(m\)  有关，且包含  \(K\)  个元素（ \(k=1,2,…,K\) ）。</p>
<p>最后，相关系数通过平均的方式计算：</p>
<p>$$r_{ik}=\sum_{m=1}^M \left\vert \frac{E[a_{im}−μ_{aim}][b_{mk}−μ-{bmk}]}{σ_{aim}σ_{bmk}} \right\vert$$</p>
<h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><p>下面是在LFW人脸验证的实验（整个实验都没有去碰卷积层，因为对于作者所用的VGG来说，全连接占据了90%的参数量）：</p>
<div align="center">
![](/img/2018-06-26-ModelCompression-2.jpg)
<center><small><font color="gray">    </font></small></center>
</div>


<div align="center">
![](/img/2018-06-26-ModelCompression-3.jpg)
<center><small><font color="gray">    </font></small></center>
</div>

<ul>
<li>\(1/256\)  表示稀疏度</li>
<li>默认为主要选择相关性高的，小部分为相关性小的</li>
<li>\(r\) 表示全部随机选择</li>
<li>\(h\) 表示只选择相关性高的</li>
</ul>
<p>可以看出：<strong>权重的幅值并不能很好地指示权重的重要性。</strong></p>
<h3 id="方式-二"><a href="#方式-二" class="headerlink" title="方式 二"></a>方式 二</h3><blockquote>
<p>论文地址：<a href="https://arxiv.org/abs/1608.08710" target="_blank" rel="noopener">Pruning Filters for Efficient ConvNets</a> </p>
</blockquote>
<p>这篇论文中，<strong>作者提出对卷积层进行完全的剪枝。</strong>对第 \(k\) 个卷积输出层进行剪枝，不仅影响当前的卷积层输出，也会影响接下来的网络层，也就是对于之后的网络层没有了原始输入中的第 \(k\) 个。</p>
<div align="center">
![](/img/2018-06-26-ModelCompression-4.jpg)
<center><small><font color="gray">    </font></small></center>
</div>

<p>这篇论文对卷积窗口的贡献度排序的方法很简单，所采用的的排序指标为<strong>卷积窗口经 L1 正则化的权重参数</strong>。</p>
<div align="center">
![](/img/2018-06-26-ModelCompression-5.jpg)
<center><small><font color="gray">    </font></small></center>
</div>


<p>对卷积窗口剪枝的迭代过程中，每一轮迭代会将全部的卷积窗口进行排序（排序指标为卷积核中 L1 正则化的权重参数），舍弃排序后指标最低的 m 个卷积窗口以达到剪枝的目的，然后用剪枝后的卷积窗口进行模型训练，再不断地重复这个过程。</p>
<h4 id="剪枝的敏感度（Sensitivity）"><a href="#剪枝的敏感度（Sensitivity）" class="headerlink" title="剪枝的敏感度（Sensitivity）"></a>剪枝的敏感度（Sensitivity）</h4><p>敏感度指每一卷积层进行单独剪枝，查看在 validation set 上准确度的变化</p>
<p>对于 VGG-16, 一些卷积层的 filter 数量是一样的，所以对于差不多  Sensitivity 的卷积层，使用相同的比例进行剪枝，而对于 Sensitivity  比较大的，选择最小的比例进行剪枝或者不进行剪枝</p>
<div align="center">
![](/img/2018-06-26-ModelCompression-6.jpg)
<center><small><font color="gray">    </font></small></center>
</div>

<h4 id="多层剪枝的策略"><a href="#多层剪枝的策略" class="headerlink" title="多层剪枝的策略"></a>多层剪枝的策略</h4><p>之前的一些剪枝策略是逐层剪枝，然后进行retraining，但是这样是非常耗时的</p>
<p><strong>两种策略：</strong> </p>
<ul>
<li>独立剪枝：就是每一层是独立的，然后进行剪枝</li>
<li>贪心剪枝：就是考虑到上一层被剪掉的情况</li>
</ul>
<p>如下图，第一种方法就是不考虑已经前面已经移除的filters（蓝色的），黄色的kernel仍然参与计算；而对于贪心剪枝就不用计算黄色的kernel。</p>
<div align="center">
![](/img/2018-06-26-ModelCompression-7.jpg)
<center><small><font color="gray">    </font></small></center>
</div>

<h3 id="方式-三"><a href="#方式-三" class="headerlink" title="方式 三"></a>方式 三</h3><blockquote>
<p><strong>对接近0的权重进行剪枝</strong></p>
</blockquote>
<ol>
<li>首先对正常模型进行训练</li>
<li>然后对接近 0 的权值设置 mask，使其相当于0</li>
<li>最后进行小型的 retrain</li>
</ol>
<p>项目地址：</p>
<ol>
<li><a href="https://github.com/ex4sperans/pruning_with_tensorflow" target="_blank" rel="noopener">pruning_with_tensorflow</a></li>
<li><a href="https://github.com/garion9013/impl-pruning-TF" target="_blank" rel="noopener">impl-pruning-TF</a></li>
</ol>
<br>
## 3 模型蒸馏

<blockquote>
<p>论文地址：<a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a></p>
</blockquote>
<p>模型蒸馏直接设计了一个简单结构的小网络，那小网络的准确率怎么和大网络比呢？    </p>
<blockquote>
<p>模型蒸的主要思想是用预训练好的网络(通常结构较复杂，准确率较高)，来指导小网络的训练，并使小网络达到与复杂网络相近的准确率。</p>
</blockquote>
<p>大网络类比于老师，小网络类比于学生，老师经过漫长时间的“训练”摸索出一套适用于某个任务的方法，于是将方法提炼成“知识”传授给学生，帮助学生更快地学会处理相似的任务。</p>
<p>整个思想中最大的难题在于如何有效地表达“知识”，并有效地指导小网络的训练。其整体结构如下图所示：</p>
<div align="center">
![](/img/2018-06-26-ModelCompression-8.jpg)
<center><small><font color="gray"> 模型蒸馏结构  </font></small></center>
</div>

<p>整个网络的损失函数包括原本任务的损失函数，和大网络对小网络的指导损失函数，其中指导损失函数为每个网络块输出特征图的均方误差，如下式所示：</p>
<p>$$L_{TS}={\frac{1}{2}}\Vert{u_{Teacher}-r_{student}}\Vert ^2$$</p>
<ul>
<li>\(L_{TS}(Block)\) 表示指导损失函数</li>
<li>\(u_{Teacher}\) 表示大网络输出特征图</li>
<li>\(r_{student}\) 表示小网络的输出特征图</li>
</ul>
<p><strong>整体网络的损失函数如下式所示：</strong></p>
<p>$$L_{total} =\lambda L_{orig}+（1-\lambda） L_{TS}$$</p>
<ul>
<li>\(L_{orig}\) 为直接训练网络的损失函数</li>
<li>\(\lambda\) 为提前设定的超参数，表示大网络对小网络指导损失函数的重要性</li>
</ul>
<p><strong>对于 \(\lambda\)  的取值：</strong></p>
<ul>
<li>当 \(\lambda\) 过小时，总损失函数与原损失函数几乎相同</li>
<li>当 \(\lambda\) 过大时，总损失函数与指导损失函数几乎相同，每次迭代的参数更新值几乎全部取决于指导损失函数，这种训练将完全陷入模仿训练误区。此时，小网络学习重点偏向于模仿大网络而忽略了任务本身，导致实际训练效果下降甚至发生错误。</li>
<li>推荐 \(0.1至0.5\) </li>
</ul>
<blockquote>
<p>如果先单独对指导损失函数进行训练，然后再加入任务损失函数联合训练，得到的模型效果<strong>可能</strong>将会比直接联合训练得到的模型好很多。</p>
</blockquote>
<div align="center">
![](/img/2018-06-26-ModelCompression-9.jpg)
<center><small><font color="gray">    </font></small></center>
</div>


<p><strong>细节部分</strong>， \(softmax\) 层的公式如下：</p>
<p> $$q_{i}=\frac{exp(\frac {z_i}{T})}{\sum_j exp(\frac{z_j}{T})}$$</p>
<ul>
<li>\(z_i : \mathrm {the\ logit,\ i.e.\ the\ input\ to\ the\ softmax\ layer}\)</li>
<li>\(q_i : \mathrm {the\ class\ probability\ computed\ by\ the\ softmax}\) </li>
<li>\(T : \mathrm {a\ temperture\ that\ is\ normally\ set\ to\ 1}\) </li>
</ul>
<p>\(T\) 就是调节参数，一般设为 1。 \(T\) 越大，分类的概率分布越“软” </p>
<p><strong>“蒸馏”最简单的形式就是：</strong>以从复杂模型得到的“软目标”为目标（这时T比较大），用“转化”训练集训练小模型。训练小模型时T不变仍然较大，训练完之后T改为1。    </p>
<p><strong><a href="https://github.com/chengshengchan/model_compression/blob/master/teacher-student.py" target="_blank" rel="noopener">代码实现</a></strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">teacher=nin()</span><br><span class="line">student=lenet()</span><br><span class="line"></span><br><span class="line">one_hot = tf.one_hot(y, n_classes,<span class="number">1.0</span>,<span class="number">0.0</span>)</span><br><span class="line">teacher_tau = tf.scalar_mul(<span class="number">1.0</span>/args.tau, teacher)</span><br><span class="line">student_tau = tf.scalar_mul(<span class="number">1.0</span>/args.tau, student)</span><br><span class="line">objective1 = tf.nn.sigmoid_cross_entropy_with_logits(student_tau, one_hot)</span><br><span class="line">objective2 = tf.scalar_mul(<span class="number">0.5</span>, tf.square(student_tau-teacher_tau))</span><br><span class="line">tf_loss = (args.lamda*tf.reduce_sum(objective1) + (<span class="number">1</span>-args.lamda)*tf.reduce_sum(objective2))/batch_size</span><br></pre></td></tr></table></figure>

<p><strong>项目地址：</strong></p>
<ol>
<li><a href="https://github.com/a7b23/Distilling-the-knowledge-in-neural-network" target="_blank" rel="noopener">Distilling-the-knowledge-in-neural-network</a></li>
<li><a href="https://github.com/chengshengchan/model_compression" target="_blank" rel="noopener">model_compression</a></li>
</ol>
<br>

<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://arxiv.org/pdf/1510.00149.pdf" target="_blank" rel="noopener">DEEP COMPRESSION</a></li>
</ul>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>


        
        <br />
        <div id="comment-container">
        </div>
        <div id="disqus_thread"></div>

        <div id="lv-container">
        </div>

    </div>
</div>

    </div>
</div>


<footer class="footer">
    <ul class="list-inline text-center">
        
        
        <li>
            <a target="_blank" href="https://www.zhihu.com/people/haoyuachen">
                            <span class="fa-stack fa-lg">
                                 <i class="iconfont icon-zhihu"></i>
                            </span>
            </a>
        </li>
        

        

        

        
        <li>
            <a target="_blank"  href="https://github.com/aaronchen98">
                            <span class="fa-stack fa-lg">
                                <i class="iconfont icon-github"></i>
                            </span>
            </a>
        </li>
        

        
        <li>
            <a target="_blank"  href="https://www.linkedin.com/in/浩宇-陈-97b3a9151">
                            <span class="fa-stack fa-lg">
                                <i class="iconfont icon-linkedin"></i>
                            </span>
            </a>
        </li>
        

    </ul>
    
    <p>
        <span id="busuanzi_container_site_pv">
            <span id="busuanzi_value_site_pv"></span>PV
        </span>
        <span id="busuanzi_container_site_uv">
            <span id="busuanzi_value_site_uv"></span>UV
        </span>
        Created By <a href="https://hexo.io/">Hexo</a>  Theme <a href="https://github.com/aircloud/hexo-theme-aircloud">AirCloud</a></p>
</footer>




</body>

<script>
    // We expose some of the variables needed by the front end
    window.hexo_search_path = "search.json"
    window.hexo_root = "/"
    window.isPost = true
</script>
<script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>
<script src="/js/index.js"></script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




</html>
