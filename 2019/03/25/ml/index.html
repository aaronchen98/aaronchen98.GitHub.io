<!DOCTYPE html>
<html lang>
  <head><meta name="generator" content="Hexo 3.9.0">
    
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=0.5, maximum-scale=2.0, user-scalable=yes">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="description" content="机器学习汇总">




  <meta name="keywords" content="机器学习,">





  <link rel="alternate" href="/atom.xml" title="浩瀚宇宙·AaronChen">




  <link rel="shortcut icon" type="image/x-icon" href="/img/1.jpg?v=1.1">



<link rel="canonical" href="https://hhyz.me/2019/03/25/ml/">


<meta name="description" content="基础知识1. 参数法与非参数法机器学习上的方法分：  参数方法   根据先验知识假定模型服从某种分布，然后利用训练集估计出模型参数，也就弄清楚了整个模型。      假设了一个在整个输入空间上有效的模型，将问题归结为在样本上估计少量参数，(如：线性模型估计w，高斯分布估计mu和sigma)，参数学习方法假定了一个模型,当模型假定不成立或样本不是一个分组可能导致很大的误差。   例如感知器  非参数">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习汇总">
<meta property="og:url" content="https://hhyz.me/2019/03/25/ml/index.html">
<meta property="og:site_name" content="浩瀚宇宙·AaronChen">
<meta property="og:description" content="基础知识1. 参数法与非参数法机器学习上的方法分：  参数方法   根据先验知识假定模型服从某种分布，然后利用训练集估计出模型参数，也就弄清楚了整个模型。      假设了一个在整个输入空间上有效的模型，将问题归结为在样本上估计少量参数，(如：线性模型估计w，高斯分布估计mu和sigma)，参数学习方法假定了一个模型,当模型假定不成立或样本不是一个分组可能导致很大的误差。   例如感知器  非参数">
<meta property="og:locale" content="zh">
<meta property="og:image" content="https://hhyz.me/img/2019-03-25-ml-1.jpg">
<meta property="og:image" content="https://hhyz.me/img/2019-03-25-ml-2.jpg">
<meta property="og:image" content="https://hhyz.me/img/2019-03-25-ml-3.jpg">
<meta property="og:image" content="https://hhyz.me/img/2019-03-25-ml-4.jpg">
<meta property="og:image" content="https://hhyz.me/img/2019-03-25-ml-5.jpg">
<meta property="og:image" content="https://hhyz.me/img/2019-03-25-ml-6.jpg">
<meta property="og:image" content="https://hhyz.me/img/2019-03-25-ml-7.jpg">
<meta property="og:updated_time" content="2019-07-02T17:05:20.724Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习汇总">
<meta name="twitter:description" content="基础知识1. 参数法与非参数法机器学习上的方法分：  参数方法   根据先验知识假定模型服从某种分布，然后利用训练集估计出模型参数，也就弄清楚了整个模型。      假设了一个在整个输入空间上有效的模型，将问题归结为在样本上估计少量参数，(如：线性模型估计w，高斯分布估计mu和sigma)，参数学习方法假定了一个模型,当模型假定不成立或样本不是一个分组可能导致很大的误差。   例如感知器  非参数">
<meta name="twitter:image" content="https://hhyz.me/img/2019-03-25-ml-1.jpg">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1">
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">





<script type="text/javascript">
  var themeConfig = {
    fancybox: {
      enable: false
    },
  };
</script>




  





  


    <title> 机器学习汇总 - 浩瀚宇宙·AaronChen </title>


    <script>
(function(u, c) {
  var d = document, t = 'script', o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
  o.src = u;
  if (c) { o.addEventListener('load', function(e) { c(e); }); }
  s.parentNode.insertBefore(o, s);
})('//cdn.bootcss.com/pangu/3.3.0/pangu.min.js', function() {
  pangu.spacingPage();
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

  </head>

  <body>
    <div id="page">
      <header id="masthead"><div class="site-header-inner">
    <h1 class="site-title">
        <a href="/." class="logo">浩瀚宇宙·AaronChen</a>
    </h1>

    <nav id="nav-top">
        
            <ul id="menu-top" class="nav-top-items">
                
                    <li class="menu-item">
                        <a href="https://www.hhyz.me">
                            
                            
                                Gallery
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/about">
                            
                            
                                About
                            
                        </a>
                    </li>
                
            </ul>
        
  </nav>
</div>

      </header>
      <div id="content">
        
    <div id="primary">
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          机器学习汇总
        
      </h1>

      <time class="post-time">
          Mar 25 2019
      </time>
    </header>



    
            <div class="post-content">
            <h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><h2 id="1-参数法与非参数法"><a href="#1-参数法与非参数法" class="headerlink" title="1. 参数法与非参数法"></a>1. 参数法与非参数法</h2><p>机器学习上的方法分：</p>
<ul>
<li><p><strong>参数方法</strong></p>
<p>  根据先验知识假定模型服从某种分布，然后利用训练集估计出模型参数，也就弄清楚了整个模型。   </p>
<p>  假设了一个在整个输入空间上有效的模型，将问题归结为在样本上估计少量参数，(如：线性模型估计w，高斯分布估计mu和sigma)，参数学习方法假定了一个模型,当模型假定不成立或样本不是一个分组可能导致很大的误差。</p>
<p>  例如感知器</p>
</li>
<li><p><strong>非参数方法</strong></p>
<p>  不需要知道数据的概率分布，只需要假设：相似的输入具有相似的输出。基于记忆训练集，然后根据训练集预测。</p>
<p>  非参数方法使用合适的距离度量相似性，对于输入样本，从训练集中找出它们的相似示例(输入样本的邻域)，并由相似的实例插值得到正确的输入。</p>
<p>  例如kNN</p>
</li>
</ul>
<h2 id="2-维度灾难"><a href="#2-维度灾难" class="headerlink" title="2. 维度灾难"></a>2. 维度灾难</h2><blockquote>
<p>Curse of Dimensionality</p>
</blockquote>
<p>维度灾难是在数字图像处理中，对于已知样本数目，存在一个特征数目的最大值，当实际使用的特征数目超过这个最大值时，分类器的性能不是得到改善，而是退化。</p>
<p>这种现象正是在识别模式中被称为“维度灾难”的一种表现形式。此外，提取特征向量的维度过<br>高会增加计算的复杂度，给后续的分类问题造成负面影响。</p>
<p>非参数方法在特征数 p 很大时表现不好。</p>
<h2 id="3-分类器性能指标"><a href="#3-分类器性能指标" class="headerlink" title="3. 分类器性能指标"></a>3. 分类器性能指标</h2><h3 id="3-1-ROC-曲线"><a href="#3-1-ROC-曲线" class="headerlink" title="3.1 ROC 曲线"></a>3.1 ROC 曲线</h3><blockquote>
<p>receiver operating characteristics 接收者操作特征</p>
</blockquote>
<p><img src="/img/2019-03-25-ml-1.jpg" alt></p>
<p>由上表可得出横，纵轴的计算公式：</p>
<ol>
<li><p>真正类率 (True Postive Rate)： \(TPR = \frac{TP}{TP+FN}\)， 代表分类器预测的正类中实际正实例占所有正实例的比例。Sensitivity</p>
</li>
<li><p>负正类率 (False Postive Rate) ： \(FPR = \frac{FP}{FP+TN}\)，代表分类器预测的正类中实际负实例占所有负实例的比例。1-Specificity</p>
</li>
<li><p>真负类率 (True Negative Rate) ： \(TNR = \frac{TN}{FP+TN}\)，代表分类器预测的负类中实际负实例占所有负实例的比例， \(TNR=1-FPR\) 。Specificity</p>
</li>
</ol>
<p>假设采用逻辑回归分类器，其给出针对每个实例为正类的概率，那么通过设定一个阈值如0.6，概率大于等于0.6的为正类，小于0.6的为负类。对应的就可以算出一组(FPR,TPR),在平面中得到对应坐标点。</p>
<p>随着阈值的逐渐减小，越来越多的实例被划分为正类，但是这些正类中同样也掺杂着真正的负实例，即TPR和FPR会同时增大。</p>
<p><em>阈值最大时，对应坐标点为 \((0,0)\) ,阈值最小时，对应坐标点 \((1,1)\) 。</em></p>
<p><img src="/img/2019-03-25-ml-2.jpg" alt></p>
<p><strong>横轴</strong>：负正类率 (false postive rate FPR) 特异度，<em>FPR越大，预测正类中实际负类越多。</em><br><strong>纵轴</strong>：真正类率 (true postive rate TPR )灵敏度，<em>TPR越大，预测正类中实际正类越多。</em></p>
<p><strong>理想目标</strong>：     </p>
<p>\(TPR=1，FPR=0\)，即图中 \((0,1)\) 点，故ROC曲线越靠拢 \((0,1)\) 点，越偏离 45 度对角线越好，Sensitivity、Specificity 越大效果越好。</p>
<h3 id="3-2-AUC"><a href="#3-2-AUC" class="headerlink" title="3.2 AUC"></a>3.2 AUC</h3><blockquote>
<p>area under the curve</p>
</blockquote>
<p>常常用AUC来评估二分类模型的性能</p>
<p>Roc 曲线下的面积，介于 0.1 和 1 之间。Auc 作为数值可以直观的评价分类器的好坏，值越大越好。</p>
<p>首先 AUC 值是一个概率值，当你随机挑选一个正样本以及负样本，<strong>当前的分类算法根据计算得到的Score 值将这个正样本排在负样本前面的概率就是 AUC 值，AUC 值越大，当前分类算法越有可能将正样本排在负样本前面，从而能够更好地分类。</strong></p>
<p>AUC 可以看做随机从正负样本中选取一对正负样本，其中正样本的得分大于负样本的概率！</p>
<p><strong>AUC 对正负样本比例不敏感</strong></p>
<p>利用概率解释，还可以得到AUC另外一个性质，对正负样本比例不敏感。 在训练模型的时候，如果正负比例差异比较大，例如正负比例为1:1000，训练模型的时候通常要对负样本进行下采样。当一个模型训练完了之后，用负样本下采样后的测试集计算出来的AUC和未采样的测试集计算的AUC基本一致，或者说前者是后者的无偏估计！ </p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="1-梯度法"><a href="#1-梯度法" class="headerlink" title="1. 梯度法"></a>1. 梯度法</h2><p>次梯度法（subgradient method）是传统梯度下降方法的拓展，用来处理不可微（non-differentiable ）的凸函数。它的优势是比传统方法处理问题范围大，但劣势是算法收敛速度慢。而传统的梯度下降方法只能处理可导函数。</p>
<p><img src="/img/2019-03-25-ml-3.jpg" alt></p>
<blockquote>
<p>其实无论是梯度法还是次梯度法，本质上我们都在使用一阶泰勒展开式的原理去逼近在某点的原函数。正如泰勒展开式的思想所述，将目标函数在某点附近展开为泰勒(Taylor)多项式来逼近原函数。</p>
</blockquote>
<h2 id="1-朴素贝叶斯"><a href="#1-朴素贝叶斯" class="headerlink" title="1. 朴素贝叶斯"></a>1. 朴素贝叶斯</h2><p><img src="/img/2019-03-25-ml-4.jpg" alt></p>
 <div align="center">
 ![](/img/2019-03-25-ml-101.jpg)
<center><small><font color="gray">    </font></small></center>
</div>

<p><strong>实例解析</strong></p>
<p><img src="/img/2019-03-25-ml-5.jpg" alt></p>
<blockquote>
<p>现在给我们的问题是，如果一对男女朋友，男生想女生求婚，男生的四个特点分别是不帅，性格不好，身高矮，不上进，请你判断一下女生是嫁还是不嫁？</p>
</blockquote>
<p>这是典型的二分类问题，按照朴素贝叶斯的求解，转换为 </p>
<ul>
<li>\(P(嫁 | 不帅、性格不好、矮、不上进)\) </li>
<li>\(P(不嫁 | 不帅、性格不好、矮、不上进)\) </li>
</ul>
<p>的概率，最终选择嫁与不嫁的答案。</p>
<p>这里我们根据贝特斯公式:</p>
<p><img src="/img/2019-03-25-ml-6.jpg" alt></p>
<p>$$P(不帅、性格不好、矮、不上进)=P(嫁)P(不帅|嫁)P(性格不好|嫁)P(矮|嫁)P(不上进|嫁)$$<br>$$+P(不嫁)P(不帅|不嫁)P(性格不好|不嫁)P(矮|不嫁)P(不上进|不嫁)$$  </p>
<p>$$P(不帅、性格不好、矮、不上进|嫁)=P(不帅|嫁)P(性格不好|嫁)P(矮|嫁)P(不上进|嫁)$$</p>
<br>
将上面的公式整理一下可得:

<p><img src="/img/2019-03-25-ml-7.jpg" alt></p>
<p>但是由贝叶斯公式可得:对于目标求解为不同的类别，贝叶斯公式的分母总是相同的。所以，只求解分子即可：</p>
<p>$$P(嫁)P(不帅|嫁)P(性格不好|嫁)P(矮|嫁)P(不上进|嫁)=1/2 * 1/2 * 1/6 * 1/6 * 1/6=1/864$$   </p>
<p>对于类别“不嫁”的贝叶斯分子为:</p>
<p>$$P(不嫁)P(不帅|不嫁)P(性格不好|不嫁)P(矮|不嫁)P(不上进|不嫁)=1/2 * 1/3 * 1/2 * 1* 2/3=1/18$$</p>
<p>经代入贝叶斯公式可得：</p>
<p>$$P(嫁|不帅、性格不好、矮、不上进)=(1/864) / (1/864+1/18)=1/49=2.04%$$</p>
<p>$$P(不嫁|不帅、性格不好、矮、不上进)=(1/18) / (1/864+1/18)=48/49=97.96%$$</p>
<p>则 \(P(不嫁|不帅、性格不好、矮、不上进) &gt; P(嫁|不帅、性格不好、矮、不上进)\) ，则该女子选择不嫁！</p>
<br>

<p><strong>优点</strong>：  </p>
<ul>
<li>算法逻辑简单,易于实现（算法思路很简单，只要使用贝叶斯公式转化即可！）</li>
<li>分类过程中时空开销小（假设特征相互独立，只会涉及到二维存储）</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>朴素贝叶斯假设属性之间相互独立，这种假设在实际过程中往往是不成立的。在属性之间相关性越大，分类误差也就越大。</li>
</ul>
<blockquote>
<p><a href="https://blog.csdn.net/fisherming/article/details/79509025" target="_blank" rel="noopener">带你彻彻底底搞懂朴素贝叶斯公式</a><br><a href="https://www.cnblogs.com/pinard/p/6069267.html" target="_blank" rel="noopener">朴素贝叶斯算法原理小结</a></p>
</blockquote>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

            </div>
          

    
      <footer class="post-footer">
        <div class="post-tags">
          
            <a href="/tags/机器学习/">机器学习</a>
          
        </div>

        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2019/03/30/CART/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">CART (Classification And Regression Tree)</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/2019/03/17/cpp/">
        <span class="next-text nav-default">c++ 基础汇总</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

        
  <div class="comments" id="comments">
    
  </div>


      </footer>
    
  </article>

    </div>



        
      <!--noindex-->
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#基础知识"><span class="nav-text">基础知识</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-参数法与非参数法"><span class="nav-text">1. 参数法与非参数法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-维度灾难"><span class="nav-text">2. 维度灾难</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-分类器性能指标"><span class="nav-text">3. 分类器性能指标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-ROC-曲线"><span class="nav-text">3.1 ROC 曲线</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-AUC"><span class="nav-text">3.2 AUC</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#方法"><span class="nav-text">方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-梯度法"><span class="nav-text">1. 梯度法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-朴素贝叶斯"><span class="nav-text">1. 朴素贝叶斯</span></a></li></ol></li></ol></div>
            

            <div class="back-to-top" id="back-to-top">
              <i class="iconfont icon-up"></i>
            </div>

          </div>
      <!--/noindex-->
      



      </div>
      <footer id="colophon"><span class="copyright-year">
    
        &copy;
    
        2017 -
    
    2019
    <span class="footer-author">haoyu.</span>
    <span class="power-by">
        Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a> and <a class="theme-link" href="https://github.com/henryhuang/hexo-theme-polarbearsimple">Polar Bear Simple</a>
    </span>
</span>

      </footer>


    </div>
    


    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  

    <script type="text/javascript" src="/js/src/theme.js?v=1.1"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=1.1"></script>

  </body>
</html>
