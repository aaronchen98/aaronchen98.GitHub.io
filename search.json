[{"title":"论文笔记","url":"/2019/07/13/regression/","content":"\n\n## 1\n\n","tags":["论文"],"categories":["深度学习","论文"]},{"title":"论文笔记","url":"/2019/07/12/papers/","content":"\n","tags":["论文"],"categories":["深度学习","论文"]},{"title":"论文写作参考汇总","url":"/2019/07/06/img/","content":"\n\n\n\n\n## Ablation Study\n\n- [ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks](http://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Wang_ESRGAN_Enhanced_Super-Resolution_Generative_Adversarial_Networks_ECCVW_2018_paper.pdf)\n\n- [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/pdf/1506.01497.pdf)\n\n>**To investigate the behavior of RPNs as a proposal method, we conducted several ablation studies.** **First, we show the effect of** sharing convolutional layers between the RPN and Fast R-CNN detection network. **To do this,** we stop\nafter the second step in the 4-step training process. Using separate networks reduces the result slightly to 58.7% (RPN+ZF, unshared, Table 2).** We observe that** this is because in the third step when the detectortuned features are used to fine-tune the RPN, the proposal quality is improved.     \n**Next, we** disentangle the RPN’s influence on training the Fast R-CNN detection network. **For this purpose,** we train a Fast R-CNN model by using the 2000 SS proposals and ZF net. We fix this detector and evaluate the detection mAP by changing the proposal regions used at test-time. **In these ablation experiments,** the RPN does not share features with the detector    \n...\n\n## 写作\n\n### 单词\n\n- complicated 复杂的\n\n### 句式\n\n- For some cases, a slight improvement can be observed from the 2nd and 3rd columns in Fig. 8       \n- We can further observe that ...\n- We observe that this is because ...\n\n\n## Figure/Table\n\n![](/img/2019-07-06-img-1.png)     \n![](/img/2019-07-06-img-2.png)     \n![](/img/2019-07-06-img-3.png)     \n![](/img/2019-07-06-img-4.png)\n\n","tags":["论文"],"categories":["论文"]},{"title":"Normalization","url":"/2019/06/24/norm/","content":"\n\n\n## Normalization\n\n### 独立同分布与白化\n\n独立同分布，即 independent and identically distributed，简称为 i.i.d. 独立同分布并非所有机器学习模型的必然要求（比如 Naive Bayes 模型就建立在特征彼此独立的基础之上，而 Logistic Regression 和神经网络则在非独立的特征数据上依然可以训练出很好的模型），但独立同分布的数据可以简化常规机器学习模型的训练、提升机器学习模型的预测能力，已经是一个共识。\n\n因此，在把数据喂给机器学习模型之前，“白化（whitening）”是一个重要的数据预处理步骤。白化一般包含两个目的：\n\n1. 去除特征之间的相关性  \\\\(\\longrightarrow\\\\)  **独立**；\n2. 使得所有特征具有相同的均值和方差 \\\\(\\longrightarrow\\\\) **同分布**。\n\n白化最典型的方法就是PCA，可以参考阅读 [PCAWhitening](https://link.zhihu.com/?target=http%3A//ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/)。\n\n### 深度学习中的 Internal Covariate Shift\n\n\n\n\n深度神经网络模型的训练为什么会很困难？其中一个重要的原因是，深度神经网络涉及到很多层的叠加，**而每一层的参数更新会导致上层的输入数据分布发生变化**，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。\n\nGoogle 将这一现象总结为 **Internal Covariate Shift**，简称 ICS.\n\n>大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如 transfer learning / domain adaptation 等。而 covariate shift 就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同，即：对所有 \\\\(x \\in \\mathcal{X}\\\\) ,\n>\n>$$P\\_{s}(Y | X=x)=P\\_{t}(Y | X=x)$$\n>\n>但是\n>\n>$$P\\_{s}(X) \\neq P\\_{t}(X)$$\n>\n大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。\n\n\n\n### ICS 会导致什么问题？\n\n简而言之，每个神经元的输入数据不再是 “独立同分布”。\n\n1. 上层参数需要不断适应新的输入数据分布，降低学习速度。\n\n2. 下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。    \n\n3. 每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。\n\n\n\n## Normalization 的通用框架与基本思想\n\n\n\n><font color=red> **BN的基本思想其实相当直观：**  </font>\n>\n>因为深层神经网络在做非线性变换前的激活输入值（就是那个x=WU+B，U是输入）随着网络深度加深或者在训练过程中，其分布逐渐**发生偏移或者变动**，之所以训练收敛慢，一般是**整体分布逐渐往非线性函数的取值区间的上下限两端靠近**（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这导致反向传播时**低层神经网络的梯度消失**，这是训练深层神经网络收敛越来越慢的**本质原因**。\n>\n>而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，其实就是把越来越偏的分布强制拉回比较标准的分布，这样**使得激活输入值落在非线性函数对输入比较敏感的区域**，这样输入的小变化就会导致损失函数较大的变化，意思是这样**让梯度变大，避免梯度消失问题产生**，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。\n\n  \n我们以神经网络中的一个普通神经元为例。神经元接收一组输入向量  \\\\(\\mathbf{x}=\\left(x\\_{1}, x\\_{2}, \\cdots, x\\_{d}\\right)\\\\)  通过某种运算后，输出一个标量值：  \\\\(y = f(\\mathbf{x})\\\\)  \n\n由于 ICS 问题的存在，  \\\\(\\mathbf{x}\\\\)  的分布可能相差很大。要解决独立同分布的问题，“理论正确”的方法就是对每一层的数据都进行白化操作。然而标准的白化操作代价高昂，特别是我们还希望白化操作是可微的，保证白化操作可以通过反向传播来更新梯度。\n\n\n\n因此，以 BN 为代表的 Normalization 方法退而求其次，进行了简化的白化操作。基本思想是：在将  \\\\(\\mathbf{x}\\\\) 送给神经元之前，先对其做平移和伸缩变换， 将 \\\\(\\mathbf{x}\\\\) 的分布规范化成在固定区间范围的标准分布。\n\n\n\n通用变换框架就如下所示：\n\n$$\nh=f\\left(\\mathbf{g} \\cdot \\frac{\\mathbf{x}-\\mu}{\\sigma}+\\mathbf{b}\\right)\n$$\n\n我们来看看这个公式中的各个参数。\n\n\n1. \\\\(\\mu\\\\)  是**平移参数**（shift parameter），  \\\\(\\sigma\\\\)  是**缩放参数**（scale parameter）。通过这两个参数进行 shift 和 scale 变换：  \\\\(\\hat{\\mathbf{x}}=\\frac{\\mathbf{x}-\\mu}{\\sigma}\\\\)  得到的数据符合均值为 0、方差为 1 的标准分布。\n\n2.  \\\\(\\mathbf{b}\\\\)  是**再平移参数**（re-shift parameter），\\\\(\\mathbf{g}\\\\)  是**再缩放参数**（re-scale parameter）。将 上一步得到的  \\\\(\\hat{\\mathbf{x}}\\\\)  进一步变换为： \n\n$$\n\\mathbf{y}=\\mathbf{g} \\cdot \\hat{\\mathbf{x}}+\\mathbf{b}\n$$\n\n最终得到的数据符合均值为 \\\\(\\mathbf{g}\\\\) 、方差为 \\\\(\\mathbf{g}^{2}\\\\) 的分布。\n\n\n奇不奇怪？奇不奇怪？    \n说好的处理 ICS，第一步都已经得到了标准分布，第二步怎么又给变走了？\n\n答案是 —— **为了保证模型的表达能力不因为规范化而下降**。\n\n我们可以看到，第一步的变换将输入数据限制到了一个全局统一的确定范围（均值为 0、方差为 1）。下层神经元可能很努力地在学习，但不论其如何变化，其输出的结果在交给上层神经元进行处理之前，将被粗暴地重新调整到这一固定范围。\n\n难道我们底层神经元人民就在做无用功吗？\n\n\n\n所以，为了尊重底层神经网络的学习结果，我们将规范化后的数据进行再平移和再缩放，使得每个神经元对应的输入范围是针对该神经元量身定制的一个确定范围（均值为   \\\\(\\mathbf{b}\\\\)  、方差为 \\\\(\\mathbf{g}^{2}\\\\)  ）。rescale 和 reshift 的参数都是可学习的，这就使得 Normalization 层可以学习如何去尊重底层的学习结果。\n\n\n\n**除了充分利用底层学习的能力，另一方面的重要意义在于保证获得非线性的表达能力。** Sigmoid 等激活函数在神经网络中有着重要作用，通过区分饱和区和非饱和区，使得神经网络的数据变换具有了非线性计算能力。*而第一步的规范化会将几乎所有数据映射到激活函数的非饱和区（线性区），仅利用到了线性变化能力，从而降低了神经网络的表达能力。*而进行再变换，*则可以将数据从线性区变换到非线性区，恢复模型的表达能力。*\n\n\n\n>\n>**经过这么的变回来再变过去，会不会跟没变一样？**\n>\n>\n>\n>不会。因为，再变换引入的两个新参数 g 和 b，可以表示旧参数作为输入的同一族函数，但是新参数有不同的学习动态。在旧参数中， \\\\(\\mathbf{X}\\\\)  的均值取决于下层神经网络的复杂关联；但在新参数中，  \\\\(\\mathbf{y}=\\mathbf{g} \\cdot \\hat{\\mathbf{x}}+\\mathbf{b}\\\\) 仅由 \\\\(\\mathbf{b}\\\\) 来确定，去除了与下层计算的密切耦合。新参数很容易通过梯度下降来学习，简化了神经网络的训练。\n>\n>\n>\n>**这样的 Normalization 离标准的白化还有多远？**\n>\n>标准白化操作的目的是 “独立同分布”。独立就不说了，暂不考虑。变换为均值为\\\\(\\mathbf{b}\\\\) 、方差为 \\\\(\\mathbf{g}^{2}\\\\)的分布，也并不是严格的同分布，只是映射到了一个确定的区间范围而已。（所以，这个坑还有得研究呢！）\n\n\n\n\n\n## 主流 Normalization 方法梳理\n\n### Batch Normalization 纵向规范化\n\n\n\n![](/img/2019-06-24-norm-1.jpg)\n\n\n\n$$\n\\mu\\_{i}=\\frac{1}{M} \\sum x\\_{i}, \\quad \\sigma\\_{i}=\\sqrt{\\frac{1}{M} \\sum\\left(x\\_{i}-\\mu\\_{i}\\right)^{2}+\\epsilon}\n$$\n\n其中  \\\\(\\mathbf{M}\\\\) 是 mini-batch 的大小。\n\n\n![](/img/2019-06-24-norm-2.png)\n\n\n![](/img/2019-06-24-norm-3.png)\n\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n\n","tags":["深度学习"],"categories":["深度学习"]},{"title":"R 语言入门","url":"/2019/06/21/R/","content":"\n# 基础操作\n\n查看数据类型：\n\n```R\n> class(spam)\r[1] \"data.frame\"\n```\n\n\n安装包：\n\n```R\ninstall.packages(\"ElemStatLearn\")\n```\n\n使用包：\n\n```R\nlibrary(ElemStatLearn)\n```\n\n\n# 数据结构\n\n## 1. Vectors\n\n\n```R\n# 创建向量\na <-c(1, 2, 3, 4, 5, 6)\nb <-c(\"one\", \"two\", \"three\")\nc <-c(TRUE, FALSE, TRUE, TRUE, FALSE)\n\n> 5:1\n[1] 5 4 3 2 1\n\n> 2*(1:5)\n[1]  2  4  6  8 10\n\n> rep(1,9)\n[1] 1 1 1 1 1 1 1 1 1\n\n> rep(c(1,0,4), each=3)\n[1] 1 1 1 0 0 0 4 4 4\n\n> seq(1,3, by=0.2)\n> seq(from=1, to=3, by=0.2)\n [1] 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4 2.6 2.8 3.0\n \n> seq(3,1, by=-0.2)\n [1] 3.0 2.8 2.6 2.4 2.2 2.0 1.8 1.6 1.4 1.2 1.0\n> seq(3,1, by=0.2)\nError in seq.default(3, 1, by = 0.2) : wrong sign in 'by' argument\n\n> seq(3,1.1)\n[1] 3 2 \n```\n\n```\n# 向量索引\na[2]  # 第二个元素    \n# [1] 2\na[-2] # 删除第二个元素，a 还是原来的 a\n# [1] 1 3 4 5 6\na[c(2:4)] # 取出第二到第四个元素\n# [1] 2 3 4\n```\n\n\n## 2. 矩阵\n\n**二维数组**\n\n```R\n# 创建矩阵\nmymat <- matrix(c(1:10), nrow=2, ncol=5, byrow=TRUE)\n```\n\n得到：\n\n```R\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    2    3    4    5\n[2,]    6    7    8    9   10\n```\n\n```\n#矩阵索引\nmymat[2,]   # 取第二行\nmymat[,2]   # 取第二列\nmymat[1,5]  # 第一行第五列的元素\n```\n\n```python\nnrow(mymat)  # 获取行数\nncol(mymat)  # 获取列数\n```\n\n\n## 3. 数组\n\n维度可以大于 2\n\n```R\n# 创建数组\nmyarr <- array(c(1:12),dim=c(2,3,2))\n```\n\n得到：\n\n```R\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    7    9   11\n[2,]    8   10   12\n```\n\n```R\ndim(myarr) # 取矩阵或数组的维度\n# [1] 2 3 2\nmyarr[1,2,1] # 取第一个矩阵的第一行第二列\n# [1] 3\n```\n\n## 4. Data Frame\n\n类似矩阵，每一列可以有不同的模式\n\n```R\n# 创建数据框\nkids <- c(\"Wang\", \"Li\")\nage <- c(\"18\", \"16\")\ndf <- data.frame(kids, age)\n```\n\n得到：\n\n```R\n  kids age\n1 Wang  18\n2   Li  16\n```\n\n```R\n#数据框索引\ndf[1,]   # 第一行\ndf[,2]   # 第二列\ndf[1:2,1:2]   # 前两行，前两列\ndf$age   # 根据列名称\n\n> df$age\n[1] 18 16\nLevels: 16 18\n\n#数据框常用函数\nstr(df)   # 数据框的结构\n\n> str(df)\n'data.frame':\t2 obs. of  2 variables:\n $ kids: Factor w/ 2 levels \"Li\",\"Wang\": 2 1\n $ age : Factor w/ 2 levels \"16\",\"18\": 2 1\n\nrownames(df)   # 行名称\ncolnames(df)   # 列名称\n\n> rownames(df)\n[1] \"1\" \"2\"\n> colnames(df)\n[1] \"kids\" \"age\" \n```\n\n更换列名：\n\n```R\ncolnames(m) <- c(\"Date\", \"Bill\", \"Dollar\", \"Gold\")\n```\n\n\n最小值：\n\n```python\nmin(m$Gold, na.rm=T)\n```\n\n其中 `na.rm=T` 是为了防止 na 值影响结果，会移除 na 数据\n\n### 合并\n\n根据某一共同列合并：\n\n```R\n# student\n\n  SID  Course Score\n1  11    Math    90\n2  11 English    80\n3  12    Math    80\n4  12 Chinese    95\n5  13    Math    96\n\n\nresult <- merge(student,score,by.x=\"ID\",by.y=\"SID\")\n\n\n# result\n\n ID   Name Gender  Birthdate Age  Course Score\n1 11  Devin      M 1984-12-29  31    Math    90\n2 11  Devin      M 1984-12-29  31 English    80\n3 12 Edward      M 1983-05-06  32    Math    80\n4 12 Edward      M 1983-05-06  32 Chinese    95\n5 13  Wenli      F 1986-08-08  29    Math    96\n```\n\n合并多个：\n\n```R\nm <- Reduce(function(x,y)merge(x, y, by=\"DATE\", all=TRUE), list(data1, data2, data3))\n```\n\n### 类型转换\n\n```R\nfor (i in 1:ncol(df)) {\n   df[,i] <- as.integer(df[,i])  #将每列类型变为integer型\n}\n```\n\n# 数据清洗\n\n## 排序\n\n单变量序列的排序常用到 `rank`、`sort` 和 `order` 函数。\n\n```R\n> a <- c(3, 1, 5)\n> rank(a)\n[1] 2 1 3\n> sort(a)\n[1] 1 3 5\n> order(a)\n[1] 2 1 3\n```\n\n- `rank` 用来计算序列中每个元素的秩，这里的“秩”可以理解为该元素在序列中由小到大排列的次序\n- `sort` 函数给出的是排序后的结果\n- `order` 函数给出的是排序后的序列中各元素在原始序列中的位置，序列 [3, 1, 5] 按升序规则排序后的结果是 [1, 3, 5] ，其中 [1, 3, 5] 在原始序列中的位置是 [2, 1, 3]\n\n\n## 筛选\n\n\n## 标准化\n\n中心化：`数据 - 均值`\n标准化：`(数据 - 均值）/ 标准差`\n\n\n数据中心化：  `scale(data,center=T,scale=F) `\n数据标准化：  `scale(data,center=T,scale=T)` 或默认参数 `scale(data)`\n\n\nscale 方法中的两个参数 center 和 scale 的解释：\n\n1. center 和 scale 默认为真,即 T 或者 TRUE\n2. center 为真表示数据中心化\n3. scale 为真表示数据标准化\n\n\n# 实践\n\n## 读取 CSV\n\n```python\nread.table(file, header = FALSE, sep = \"\", quote = \"\\\"'\",\n           dec = \".\", numerals = c(\"allow.loss\", \"warn.loss\", \"no.loss\"),\n           row.names, col.names, as.is = !stringsAsFactors,\n           na.strings = \"NA\", colClasses = NA, nrows = -1,\n           skip = 0, check.names = TRUE, fill = !blank.lines.skip,\n           strip.white = FALSE, blank.lines.skip = TRUE,\n           comment.char = \"#\",\n           allowEscapes = FALSE, flush = FALSE,\n           stringsAsFactors = default.stringsAsFactors(),\n           fileEncoding = \"\", encoding = \"unknown\", text, skipNul = FALSE)\n```\n\n![](/img/2019-06-21-R-1.jpg)\n\n\n```R\ndataset <- read.table(\"C:\\\\Datasets\\\\haberman.csv\", header=FALSE, sep=\",\")\n```\n\n- 如果第一行数据包含列的名字，则第二个参数应该为：`header = TRUE`\n- 分割符：\n\t- 空格： `sep = \" \"`\n\t- tabs： `sep = \"\\t\"`\n\t- 默认是 “white space” (one or more spaces, tabs, etc.)\n\n如果分割符号为逗号，那么也可以用 `read.csv`，并且不用写 `seq` 参数：\n\n```R\ndataset <- read.csv(\"C:\\\\Datasets\\\\haberman.csv\", header=FALSE)\n```\n\n\n# 机器学习算法\n\n## KNN\n\n引入 `class` 包：\n\n```R\nlibrary(class)\n```\n`knn()` 函数的语法和参数如下：\n\n```python\nknn(train, test, cl, k = 1, l = 0, prob = FALSE, use.all = TRUE)\n```\n\n- train：指定训练样本集\n- test ：指定测试样本集\n- cl ：指定训练样本集中的分类变量\n- k ：指定最邻近的k个已知分类样本点，默认为1\n- l ：指定待判样本点属于某类的最少已知分类样本数，默认为0\n- prob：设为TRUE时，可以得到待判样本点属于某类的概率，默认为FALSE\n- use.all：控制节点的处理办法，即如果有多个第K近的点与待判样本点的距离相等，默认情况下将这些点都纳入判别样本点，当该参数设为FALSE时，则随机挑选一个样本点作为第K近的判别点\n\n\n```R\n> # z-score数据标准化\n> iris_scale <- scale(iris[-5]) \n> train <- iris_scale[c(1:25,50:75,100:125),] #训练集\n> test <- iris_scale[c(26:49,76:99,126:150),] #测试集\n> train_lab <- iris[c(1:25,50:75,100:125),5]\n> test_lab <- iris[c(26:49,76:99,126:150),5]\n> pre <- knn(train=train,test=test,cl=train_lab,k=round(sqrt(dim(train)[1])),prob = F)  \n> table(pre,test_lab)\n            test_lab\npre          setosa versicolor virginica\n  setosa         24          0         0\n  versicolor      0         24         3\n  virginica       0          0        22\n```\n\n实例：\n\n```R\nlibrary(ElemStatLearn)\n\n####################################\n## Part 1. Load and Explore Data   #\n####################################\n\ndata(spam)\ncolnames(spam) <- c(paste(\"X\", 1:57, sep = \"\"), \"spam\")\nspam$spam <- factor(spam$spam, levels = c(\"spam\", \"email\"), labels = c(\"spam\", \"email\"))\n\n\n#######################\n# Normalize Variables #\n#######################\n\nspam_n <- as.data.frame(scale(spam[1:57]))\n\n\n################################################\n## Part 2. Creating training and test datasets #\n################################################\n\nindex <- sample(1:4601, 3000)\n\nspam_train <- spam_n[index, ]\nspam_test <- spam_n[-index, ]\nspam_train_label <- spam[index, 58]\nspam_tset_label <- spam[-index, 58]\n\n\n########################################\n## Part 3. Train the model on the data #\n########################################\n\n\nlibrary(class)\n\nspam_test_pred <- knn(train = spam_train, test = spam_test, cl = spam_train_label, k = 3)\nresult <- data.frame(spam_tset_label, spam_test_pred)\n\nmean(spam_test_pred == spam_tset_label)\n```\n\n\n## Naive Bayes\n\n\n![](/img/2019-06-21-R-2.png)\n\n\n实例：\n\n```R\n\n###########################################\n#    Classification with Naive Bayes      #  \n###########################################\n\nsetwd(\"D:/data/ML2019/\")\nlibrary(data.table)     # read text data\nlibrary(wordcloud)      # make word cloud\nlibrary(RColorBrewer)   # make word cloud\nlibrary(tm)             # construct text vector\nlibrary(magrittr)       # enable pipeline operator        \nlibrary(e1071)          # naive bayes \nlibrary(caret)          # split data into training and test sets\n\n\n####################################\n## Part 1. Load and Explore Data   #\n####################################\n\n\nsms_raw <- fread(\"SMSSpamCollection\", header = FALSE, encoding = \"Latin-1\",sep = \"\\t\", quote=\"\")\n\ncolnames(sms_raw)<-c(\"type\", \"text\")\n\nsms_raw$type <- factor(sms_raw$type)\ntable(sms_raw[, type])\n\n#  ham spam \r# 4827  747\n\ntable(sms_raw[, type]) %>% prop.table()\n\n#       ham      spam \r# 0.8659849 0.1340151\n\n##########################\n#     Word Cloud 词云     #\n##########################\n\n\npal <- brewer.pal(7, \"Dark2\")\nsms_raw[type == \"spam\", text] %>%\n    wordcloud(min.freq = 20,\n              random.order = FALSE, colors = pal\n    )\nsms_raw[type == \"ham\", text] %>%\n    wordcloud(min.freq = 70,\n              random.order = FALSE, colors = pal\n    )\n\n################################################\n## Part 2. Creating training and test datasets #\n################################################\n\n## p=75% of data are allocated to training \ntrain_index <- createDataPartition(sms_raw$type, p = 0.75, list = FALSE)\n\n# createDataPartition 来自 caret 包\n# 数据划分函数，对象是 spam$typ，\n# p=0.75表示训练数据所占的比例为75%\n# list是输出结果的格式，默认list=FALSE\n\nsms_raw_train <- sms_raw[train_index, ]\nsms_raw_test <- sms_raw[-train_index, ]\n\n#################\n# check results #\n#################\ndim(sms_raw_train)\n# [1] 4182    2\n\ndim(sms_raw_test)\n# [1] 1392    2\n\ndim(sms_raw_train)[1]/dim(sms_raw_test)[1]\n# [1] 3.00431\n\ntable(sms_raw_train[, type]) %>% prop.table()\n#       ham      spam \r# 0.8658537 0.1341463 \n\ntable(sms_raw_test[, type]) %>% prop.table()\n#       ham      spam \r# 0.8663793 0.1336207\n\n# 可以看到训练集和测试集两个分类的比例几乎相同\n\n################################\n## Part 3. Text processing    ##\n################################\n\n##############################\n# Text processing functions  #\n##############################\ncorpus <- function(x) VectorSource(x) %>% VCorpus(readerControl = list(reader = readPlain))\nclean <- function(x) {\n    x %>%\n        tm_map(content_transformer(tolower)) %>%\n        tm_map(content_transformer(removeNumbers)) %>%\n        tm_map(content_transformer(removeWords), stopwords()) %>%\n        tm_map(content_transformer(removePunctuation)) %>%\n        tm_map(content_transformer(stripWhitespace))\n}\n\nsms_corpus_train <- corpus(sms_raw_train[, text]) %>% clean\nsms_corpus_test <- corpus(sms_raw_test[, text]) %>% clean\n\n##################################\n# check text processing results  #\n##################################\n\nsms_raw_train[1, text]\ninspect(sms_corpus_train[[1]])\n\nsms_raw_test[1, text]\ninspect(sms_corpus_test[[1]])\n\n##################################\n#   construct document matrix    #\n##################################\n\nsms_dtm_train_all <- DocumentTermMatrix(sms_corpus_train)\n\n## Finding frequent words\nsms_dict <- findFreqTerms(sms_dtm_train_all, 5)\n\nsms_dtm_train <- DocumentTermMatrix(\n    sms_corpus_train, control = list(dictionary = sms_dict)\n)\n\nsms_dtm_test <- DocumentTermMatrix(\n    sms_corpus_test, control = list(dictionary = sms_dict)\n)\n\n\n## function to convert counts to Yes/No strings\nconvert_counts <- function(x) {\n              x <- ifelse(x > 0, \"Yes\", \"No\")\n}\n\nsms_train <- sms_dtm_train %>% \n    apply(MARGIN = 2, convert_counts)\n\nsms_test <- sms_dtm_test %>% \n    apply(MARGIN = 2, convert_counts)\n\n\n########################################\n## Part 3. Train the model on the data #\n########################################\n\nsms_classifier_1 <- naiveBayes(sms_train, sms_raw_train$type)\nsms_test_pred_1 <- predict(sms_classifier_1, sms_test)\n\n#########################################\n## Part 4. Evaluating model performance #\n#########################################\n\ninstall.packages(\"gmodels\")\nlibrary(gmodels)\n\nCrossTable(x = sms_raw_test$type, y = sms_test_pred_1, prop.chisq=FALSE)\nmean(sms_test_pred_1==sms_raw_test$type)\n\n\n#########################################\n## Part 5. Improving model performance  #\n#########################################\n\nsms_classifier_2 <- naiveBayes(sms_train, sms_raw_train$type, laplace = 1)\nsms_test_pred_2 <- predict(sms_classifier_2, sms_test)\nCrossTable(x = sms_raw_test$type, y = sms_test_pred_2, prop.chisq = FALSE)\nmean(sms_test_pred_2==sms_raw_test$type)\n\n```","tags":["R语言"],"categories":["编程语言"]},{"title":"聚类","url":"/2019/04/25/Clustering/","content":"\n## K-means\n\n我们的问题为：\n\n$$\n\\min \\_{C_{1}, \\ldots, C\\_{K}} \\sum\\_{k=1}^{K} W\\left(C\\_{k}\\right)\n$$\n\n\n$$\nW\\left(C\\_{k}\\right)=\\frac{1}{\\left|C\\_{k}\\right|} \\sum\\_{i, j \\in C\\_{k}}\\Vert x\\_{i}-x\\_{j}\\Vert \\_{2}^{2}\n$$\n\n将以上两个式子结合一下，可以得到最优化问题为：\n\n$$\n\\min \\_{C\\_{1}, \\ldots, C\\_{K}} \\sum\\_{k=1}^{K} \\frac{1}{\\left|C\\_{k}\\right|} \\sum\\_{i, j \\in C\\_{k}}\\Vert x\\_{i}-x\\_{j}\\Vert \\_{2}^{2}\n$$\n\n\n**Centroid**\n\n让 \\\\(\\mu\\_{k}=\\frac{1}{\\left|C\\_{k}\\right|} \\sum\\_{i \\in C\\_{k}} x\\_{i}\\\\) 为 \\\\( C\\_{k}\\\\) 的 mean/centroid。\n\n$$\n\\frac{1}{\\left|C\\_{k}\\right|} \\sum\\_{i, j \\in \\mathcal{C}\\_{k}}\\Vert x\\_{i}-x\\_{j}\\Vert \\_{2}^{2}=2 \\sum\\_{i \\in C\\_{k}}\\Vert x\\_{i}-\\mu\\_{k}\\\\Vert \\_{2}^{2}\n$$ \n\n代入回之前的式子，优化问题等同于：\n\n$$\n\\min \\_{C_{1}, \\ldots, C\\_{K}} \\sum\\_{k=1}^{K}\\left\\\\{\\sum\\_{i \\in C\\_{k}}\\Vert x\\_{i}-\\mu\\_{k}\\Vert \\_{2}^{2}\\right\\\\}\n$$\n\n###  迭代法\n\n \\\\(\\text { binary matrix } R=\\left[r\\_{n k}\\right] \\in R^{N \\times K}\n\\\\)    \n \\\\(\\text { 如果 } x\\_{n} \\text { is assigned to cluster } k, \\text { 那么 }r\\_{n k}=1 \\text { 并且 } r\\_{n j}=0, j \\neq k\\\\) \n\n</br>\n**目标：**\n\n找到  \\\\(\\left\\\\{\\mu\\_{k}\\right\\\\}\\\\)，并把每一个数据点分配到一类，使 objective function 最小化。\n\n$$\nJ\\left(R,\\left\\\\{\\mu\\_{k}\\right\\\\}\\right)=\\sum\\_{n=1}^{N}\\left[\\sum\\_{k=1}^{K} r\\_{n k}\\Vert x\\_{n}-\\mu\\_{k}\\Vert ^{2}\\right]\n$$ \n\n- \\\\(\\mu\\_{k}=\\frac{1}{\\left|C\\_{k}\\right|} \\sum\\_{i \\in C\\_{k}} x\\_{i}\\\\) 为 \\\\( C\\_{k}\\\\) 的 mean/centroid。\n- \\\\(R=\\left[r\\_{n k}\\right] \\in R^{N \\times K}\\\\)\n  \n**重复以下两步：**\n\n1. step 1：固定 \\\\(\\\\{\\mu\\_{k}\\\\}\\\\)，最优化 \\\\(R\\\\)  \n2. step 2：固定 \\\\(R\\\\)，最优化 \\\\(\\\\{\\mu\\_{k}\\\\}\\\\)\n\n**具体如下：**\n\n#### step 1\n\n对于某一个具体的 n，我们选择 \\\\(r\\_{nj}\\\\) 最小化 \n \n$$\\sum\\_{k=1}^{K} r\\_{n k}\\Vert x\\_{n}-\\mu\\_{k}\\Vert ^{2}$$ \n\n![](/img/2019-04-25-Clustering-1.png)\n\n**也就是说，将 \\\\(x\\_n\\\\) 分配给最接近的 centroid。**\n\n\n#### step 2\n\n对于固定的  \\\\(R\\\\),   \\\\(J\\left(R,\\left\\\\{\\mu\\_{k}\\right\\\\}\\right)\\\\) 是 convex，quadratic 的，因此，将关于  \\\\(u\\_{k}\\\\) 的梯度设为 0：\n\n$$\n2 \\sum\\_{n=1}^{N} r\\_{n k}\\left(\\mu\\_{k}-x\\_{n}\\right)=0 \\Rightarrow \\mu_{k}=\\frac{\\sum\\_{n=1}^{N} r\\_{n k} x\\_{n}}{\\sum\\_{n=1}^{N} r\\_{n k}}\n$$\n\n让 \\\\(u\\_{k}\\\\) 等于属于 cluster k 的所有数据点 \\\\(x\\_n\\\\) 的均值\n\n![](/img/2019-04-25-Clustering-2.png)\n\n>k-means 算法对于异常值十分敏感，因为具有极大值的对象可能会产生严重扭曲的数据分布  \n\n## k-medoids\n\nk-means 与 k-medoids 之间的差异就是可以理解为对于数据样本的平均值和中位数之间的差异：前者的取值范围可以是连续空间中的任意值，而后者的取值却只能是数据样本范围中的样本。这个原因就是 k-means 对于数据样本的要求太高了，要求所有数据样本处在一个欧式空间中，对于有很多噪声的数据就会造成极大的误差。同时对于非数值型数据样本，不能够计算平均值等实数型变量。\n\n与 K-means 算法类似，区别在于中心点的选取，K-means 中选取的中心点为当前类中所有点的重心，而 K-medoids 法选取的中心点为当前 cluster 中存在的一点，准则函数是当前 cluster 中所有其他点到该中心点的距离之和最小，这就在一定程度上削弱了异常值的影响，但缺点是计算较为复杂，耗费的计算机时间比 K-means 多。\n\n$$\n\\widehat{J}\\left(R,\\left\\\\{\\mu\\_{k}\\right\\\\}\\right)=\\sum_{n=1}^{N} \\sum\\_{k=1}^{K} r\\_{n k} V\\left(x\\_{n}, \\mu\\_{k}\\right)\n$$\n\n- 其中 \\\\(V\\left(x\\_{n}, \\mu\\_{k}\\right)\\\\) 表示 \\\\(x\\_n\\\\) 和 \\\\(\\mu\\_k\\\\) 的 dissimilarity\n\n## Hierarchical Clustering\n\nK-means 需要提前确定 k 的值，而 Hierarchical Clustering 不需要\n\n假设有N个待聚类的样本，对于层次聚类来说，其步骤为：\n\n1. 初始化：把每个样本各自归为一类（每个样本自成一类），计算每两个类之间的距离，在这里也就是样本与样本之间的相似度（本质还是计算类与类之间的距离）。\n2. 寻找各个类之间最近的两个类，把它们归为一类（这样，类的总数就减少了一个）\n3. 重新计算新生成的这个类与各个旧类之间的距离（相似度）\n4. 重复 2、3 步，直到所有的样本都归为一类，结束。\n\n2、详细描述：\n\n整个聚类过程其实是建立了一棵树，在建立过程中，可以通过第二步上设置一个阈值，当最近的两个类的距离大于这个阈值，则认为迭代终止。\n\n\n\n另外，关键的一步是第三步，如何判断两个类之间的相似度有不少种方法，下面介绍三种：\n\n1. **Single Linkage**：又叫做 nearest-neighbor，就是取两个类中最近的两个样本之间的距离作为两个集合的距离，即：最近的两个样本之间的距离越小，\n\n\t这两个类之间相似度越大，容易造成一种叫做 Chaining 的效果，两个类明明从“大局”上离的比较远，但由于其中个别点距离比较近就被合并了。\n\n\t这种合并之后 Chaining 效应会进一步扩大，最后得到比较松散的聚类 cluster。\n\n2. **Complete Linkage**：完全是 Single Linkage 的反面极端，取两个集合距离最远的两个点的距离作为两个集合的距离，其效果也刚好相反，限制非常大。\n\n\t两个聚类 cluster 即使已经很接近了，但是只要有不配合的带你存在，就顽固到底，老死不相合并，也是不太好的办法，这两种相似度定义方法共同问题就是：\n\n\t只考虑了某个特有的数据，而没有考虑类数据整体的特点。\n\n3. **Average Linkage**：这种方法就是把两个集合中的点两两距离全部放在一起求平均值，相应的能得到一点合适的结果。\n\n\tAverage Linkage 的一个变种就是取两两距离的中值，与取平均值相比更加能够解除个别偏离样本对结果的干扰。\n\t\n![](/img/2019-04-25-Clustering-5.png)\n![](/img/2019-04-25-Clustering-6.png)\n\n\n![](/img/2019-04-25-Clustering-7.png)\n<div align=center>\n<center><small><font color=gray>  Complete Linkage   </font></small></center>\n</div>\n\n\n![](/img/2019-04-25-Clustering-4.png)\n![](/img/2019-04-25-Clustering-3.png)\n\n对于 Hierarchical Clustering 的实际应用，有以下问题需要考虑：\n\n- 应该使用哪一种 dissimilarity measure？\n- 应该使用哪一种 linkage？\n- 什么时候应该切断树状结构？\n\n\n<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>\n\n  \n","tags":["机器学习"],"categories":["机器学习"]},{"title":"主成分分析","url":"/2019/04/22/PCA/","content":"\nPCA的思想就是将n维特征映射到k维上(k<n)，这k维是重新构造出来的全新维度特征，而不是简单的从n维特征去除n-k维特征，这k维就是主成分。\n\n在三维坐标系中，四种颜色的标记界限并不直观，但是在重新定义的二维坐标系中，四种颜色标记界限非常直观，这就是一个典型的PCA，在降维的同时最大程度的保留了数据的特征，为后续的分析提供更直观的支持。\n\n![](/img/2019-04-22-PCA-1.jpg)\n\n\n## 基础数学知识\n\n### 方差\n\n$$\n\\{Var}(X)=\\frac{1}{n} \\sum\\_{i=1}^{n} X\\_{i}^{2}\n$$\n\n### 协方差\n\n$$\n\\{Cov}(X, Y)=\\frac{1}{n} \\sum\\_{i=1}^{n} X\\_{i} Y\\_{i}=\\frac{1}{n} X \\cdot Y\n$$\n\n当 \\\\(Cov(a,b)=0\\\\)  时，表示两个字段完全独立，这也是我们的优化目标。\n\n### 特征值分解\n\n如果说一个向量 \\\\(v\\\\) 是方阵 \\\\(A\\\\) 的特征向量，将一定可以表示成下面的形式：\n\n$$\nA v=\\lambda v\n$$\n\n这时候 \\\\(λ\\\\) 就被称为特征向量 \\\\(v\\\\) 对应的特征值。  \n一个矩阵的一组**特征向量**是一组**正交向量**。\n\n**特征分解：**\n\n$$\nA=Q \\Sigma Q^{-1}\n$$\n\n其中 \\\\(Q\\\\) 是这个矩阵 \\\\(A\\\\) 的特征向量组成的矩阵，正交矩阵是可逆的。\n\n\\\\(\\Sigma=diag\\left(\\lambda\\_{1}, \\lambda\\_{2}, \\dots, \\lambda\\_{n}\\right)\\\\) 是一个对角阵，每一个对角线上的元素就是一个特征值。\n\n>一个矩阵其实就是一个线性变换，因为一个矩阵乘以一个向量后得到的向量，其实就相当于将这个向量进行了线性变换。    \n>\n>**特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么**\n\n### 奇异值分解\n\n特征值分解是一个提取矩阵特征很不错的方法，但是它只**适用于方阵**。而在现实的世界中，我们看到的大部分矩阵都不是方阵，而奇异值分解是一个能**适用于任意的矩阵**的一种分解的方法。\n\n奇异值分解实际上把矩阵的变换分为了三部分：\n\n- 旋转\n- 拉伸\n- 投影\n\n![](/img/2019-04-22-PCA-2.jpg)\n\n\n\n$$\nA=U \\Sigma V^{T}\n$$\n\n![](/img/2019-04-22-PCA-3.jpg)\n\n-  \\\\(A\\\\) 是一个 \\\\(M * N\\\\) 的矩阵\n-  \\\\(U\\\\) 是一个 \\\\(M * M\\\\) 的方阵（里面的向量是正交的，U里面的向量称为**左奇异向量**），\n-  \\\\(Σ\\\\) 是一个 \\\\(M * N\\\\) 的实数对角矩阵（对角线以外的元素都是 \\\\(0\\\\) ，对角线上的元素称为**奇异值**），\n-  \\\\(V^{T}\\\\) 是一个 \\\\(N * N\\\\) 的矩阵，里面的向量也是正交的， \\\\(V\\\\) 里面的向量称为**右奇异向量**）\n\n\n奇异值和特征值是怎么对应起来的呢？\n\n首先，我们将一个矩阵A的转置 \\\\(\\mathrm{A}^{\\top} \\* \\mathrm{A}\\\\)，将会得到 \\\\(\\mathrm{A}^{\\top} \\mathrm{A}\\\\) 是一个方阵，我们用这个方阵求特征值可以得到：\n\n$$\n\\left(A^{T} A\\right) v\\_{i}=\\lambda\\_{i} v\\_{i}\n$$\n并且有：\n$$\n\\sigma\\_{i}=\\sqrt{\\lambda\\_{i}}\n$$\n$$\nu\\_{i}=\\frac{1}{\\sigma_{i}} A v\\_{i}\n$$\n\n- \\\\(σ\\_i\\\\)  是就是奇异值， \n- \\\\(u\\_i\\\\) 是左奇异向量\n- \\\\(v\\\\) 是右奇异向量\n\n常见的做法是将奇异值由大而小排列。如此 \\\\(Σ\\\\) 便能由 \\\\(M\\\\) 唯一确定。\n\n\n\n$$\nA=U \\Sigma V^{T} \\Rightarrow A V=U \\Sigma V^{T} V \\Rightarrow A V=U \\Sigma \\Rightarrow A v\\_{i}=\\sigma\\_{i} u\\_{i} \\Rightarrow \\sigma\\_{i}=A v\\_{i} / u\\_{i}\n$$\n\n\n奇异值 \\\\(σ\\\\) 跟特征值类似，在矩阵 \\\\(Σ\\\\) 中也是从大到小排列，而且 \\\\(σ\\\\) 的减少特别的快，在很多情况下，前 10% 甚至 1% 的奇异值的和就占了全部的奇异值之和的 99% 以上了。\n\n也就是说，我们也可以用前 \\\\(r\\\\) 大的奇异值来近似描述矩阵\n\n**部分奇异值分解：**\n\n$$\nA\\_{m \\times n} \\approx U\\_{m \\times r} \\Sigma\\_{r \\times r} V\\_{r \\times n}^{T}\n$$\n\n \\\\(r\\\\) 是一个远小于 \\\\(m\\\\)、 \\\\(n\\\\) 的数，这样矩阵的乘法看起来像是下面的样子：\n \n![](/img/2019-04-22-PCA-4.jpg)\n\n右边的三个矩阵相乘的结果将会是一个接近于 \\\\(A\\\\) 的矩阵，在这儿， \\\\(r\\\\) 越接近于 \\\\(n\\\\) ，则相乘的结果越接近于 \\\\(A\\\\) 。而这三个矩阵的面积之和（在存储观点来说，矩阵面积越小，存储量就越小）要远远小于原始的矩阵 \\\\(A\\\\)。 \n\n如果想要压缩空间来表示原矩阵 \\\\(A\\\\)，只需存下三个矩阵：\\\\(U、Σ、V\\\\)。\n\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML\"></script>\n","tags":["机器学习"],"categories":["机器学习"]},{"title":"支持向量机","url":"/2019/04/10/SVM/","content":"\n>[从零推导支持向量机(SVM)](https://zhuanlan.zhihu.com/p/31652569)    \n>[Github PDF  地址](https://github.com/HaoMood/File/blob/master/%E4%BB%8E%E9%9B%B6%E6%8E%A8%E5%AF%BC%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA(SVM).pdf)\n\n\n## 1. 线性二分类模型\n\n![](/img/2019-04-10-SVM-1.jpg)\n\n\n## 2. 线性支持向量机\n\n![](/img/2019-04-10-SVM-2.jpg)\n\n### 2.1 间隔\n\n![](/img/2019-04-10-SVM-3.jpg)\n\n### 2.2 线性支持向量机基本型\n\n![](/img/2019-04-10-SVM-4.jpg)\n![](/img/2019-04-10-SVM-5.jpg)\n![](/img/2019-04-10-SVM-6.jpg)\n\n## 3. 核函数\n\n![](/img/2019-04-10-SVM-7.jpg)\n\n### 3.1 非线性可分问题\n\n![](/img/2019-04-10-SVM-8.jpg)\n\n![](/img/2019-04-10-SVM-9.jpg)\n\n## 4.  软间隔\n\n![](/img/2019-04-10-SVM-10.jpg)\n![](/img/2019-04-10-SVM-11.jpg)","tags":["机器学习"],"categories":["机器学习"]},{"title":"CART (Classification And Regression Tree)","url":"/2019/03/30/CART/","content":"\n## 概述\n\n求  \\\\(R\\_m\\\\)： \n\n1. 扩张树：用贪心，上至下递归分区方法    \n2. split function 选择最好的特征  \\\\(j\\*\\\\) 和该特征最好的值 \\\\(t\\*\\\\) \n\n\t![](/img/2019-03-30-CART-1.jpg)\n\nSplit s divide the current node into two children.    \n比如，对于 t，\\\\(\\text{Left-Child }= {(y\\_i, x\\_i) : x\\_{ij} ≤ t}\\ 而\\ \\text{Right-Child }= {(y\\_i, x\\_i) : x\\_{ij} > t}\\\\) \n\n\n\n**2D 的例子**\n\n![](/img/2019-03-30-CART-2.jpg)\n\n## Splitting 规则\n\n### 1. Regression\n\n![](/img/2019-03-30-CART-3.jpg)\n\n\n### 2. Classification\n\n![](/img/2019-03-30-CART-4.jpg)\n\n**不纯度函数(impurity function)**\n\n  1. 当所有样本都属于同一类时候  \\\\(I\\\\)  取最小值. 即 \\\\(I\\\\) 在点  \\\\((1,0,...,0),(0,1,...,0),...,(0,..,0,1)\\\\)  取最小值.\n\n  2. 当样本中每个类目下样本个数相同时I取最大值. 即 \\\\(I\\\\) 在点 \\\\((1/k,..,1/k)\\\\) 取最大值.\n\n\n## Prunning 剪枝\n\n代价复杂性剪枝 (cost complexity pruning)\n\n$$min \\ \\ \\frac {1}{N} \\sum^{\\vert T \\vert}\\_{m=1} \\sum\\_{x_i \\in {R\\_m}} L(y\\_i, w\\_m) + \\alpha \\vert T\\vert$$\n\n \\\\(\\vert T \\vert\\\\) 是 termainal nodes 的总数\n \n  \\\\(L(·, ·)\\\\) 是 loss function, 例如 \\\\(L(yi, f (x\\_i)) = L(x\\_i,w\\_m) = (y\\_i − w\\_m)^2\\\\) \n  \n  \\\\(w\\_m\\\\) 是与 \\\\(R\\_m\\\\) 对应的预测值  \\\\(\\rightarrow\\\\)  也就是 \\\\(R\\_m\\\\) 中训练集的平均值\n\n\n\n## 算法\n\n![](/img/2019-03-30-CART-5.jpg)\n\n![](/img/2019-03-30-CART-6.jpg)\n\n![](/img/2019-03-30-CART-7.jpg)\n\n\n\n\n\nMultiple trees: \n\n- bagging 袋装法\n- random forests 随机森林\n- boosting 提升法\n\n\n## boosting \n\n\n>**基本思想**：      \n>在分类问题中，通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类器的性能。\n\n**历史**：\n\n- PAC learning framework (1990)\n- AdaBoost methods (1996)\n- gradient boosting (2000)\n\n**weak learner**:    \nclassifiers whose error rate is slightly better than random guessing\n\nBoosting 改变训练样本的权重，产生一系列的分类器：     \n![](/img/2019-03-30-CART-8.jpg)\n\n最终的分类器可以表示为：     \n![](/img/2019-03-30-CART-9.jpg)\n\n \\\\(\\alpha\\_m\\\\)：分类系数（由 boosting 算法计算得出）\n\n\n\n### AdaBoost\n\n![](/img/2019-03-30-CART-10.jpg)\n\n![](/img/2019-03-30-CART-11.jpg)\n\n![](/img/2019-03-30-CART-12.jpg)\n\n\n## 随机森林\n\n\n你可能会问为什么不直接使用一个决策树？这种分类器堪称完美，因为根本不会犯任何错误！但要记住一个重点：决策树只是不会在训练数据上犯错。\n\n\n随机森林是由许多决策树构成的模型。这不仅仅是森林，而且是随机的，这涉及到两个概念：\n\n1. 随机采样数据点\n\n2. 基于特征的子集分割节点\n\n随机森林由LeoBreiman（2001）提出，它通过自助法（bootstrap）重采样技术，从原始训练样本集N中有放回地重复随机抽取k个样本生成新的训练样本集合，然后根据自助样本集生成k个分类树组成随机森林，新数据的分类结果按分类树投票多少形成的分数而定。其实质是对决策树算法的一种改进，将多个决策树合并在一起，每棵树的建立依赖于一个独立抽取的样品，森林中的每棵树具有相同的分布，分类误差取决于每一棵树的分类能力和它们之间的相关性。特征选择采用随机的方法去分裂每一个节点，然后比较不同情况下产生的误差。能够检测到的内在估计误差、分类能力和相关性决定选择特征的数目。单棵树的分类能力可能很小，但在随机产生大量的决策树后，一个测试样品可以通过每一棵树的分类结果经统计后选择最可能的分类。\n\n\n**随机森林的构建过程**\n\n决策树相当于一个大师，通过自己在数据集中学到的知识对于新的数据进行分类。但是俗话说得好，一个诸葛亮，玩不过三个臭皮匠。随机森林就是希望构建多个臭皮匠，希望最终的分类效果能够超过单个大师的一种算法。\n\n那随机森林具体如何构建呢？有两个方面：数据的随机性选取，以及待选特征的随机选取。\n\n**数据的随机选取**\n\n首先，从原始的数据集中采取有放回的抽样，构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。第二，利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。最后，如果有了新的数据需要通过随机森林得到分类结果，就可以通过对子决策树的判断结果的投票，得到随机森林的输出结果了。\n\n\n随机森林采用的是bagging的思想，bagging又称为bootstrap aggreagation，通过在训练样本集中进行有放回的采样得到多个采样集，基于每个采样集训练出一个基学习器，再将基学习器结合。随机森林在对决策树进行bagging的基础上，在决策树的训练过程中引入了随机属性选择。传统决策树在选择划分属性的时候是在当前节点属性集合中选择最优属性，而随机森林则是对结点先随机选择包含k个属性的子集，再选择最有属性，k作为一个参数控制了随机性的引入程度。\n\n另外，GBDT训练是基于Boosting思想，每一迭代中根据错误更新样本权重，因此是串行生成的序列化方法，而随机森林是bagging的思想，因此是并行化方法。\n\n尽管有剪枝等等方法，一棵树的生成肯定还是不如多棵树，因此就有了随机森林，解决决策树泛化能力弱的缺点。（可以理解成三个臭皮匠顶过诸葛亮）\n\n而同一批数据，用同样的算法只能产生一棵树，这时Bagging策略可以帮助我们产生不同的数据集。Bagging策略来源于bootstrap aggregation：从样本集（假设样本集N个数据点）中重采样选出Nb个样本（有放回的采样，样本数据点个数仍然不变为N），在所有样本上，对这n个样本建立分类器（ID3\\C4.5\\CART\\SVM\\LOGISTIC），重复以上两步m次，获得m个分类器，最后根据这m个分类器的投票结果，决定数据属于哪一类。\n\n随机森林在bagging的基础上更进一步：\n\n1.  样本的随机：从样本集中用Bootstrap随机选取n个样本\n\n2.  特征的随机：从所有属性中随机选取K个属性，选择最佳分割属性作为节点建立CART决策树（泛化的理解，这里面也可以是其他类型的分类器，比如SVM、Logistics）\n\n3.  重复以上两步m次，即建立了m棵CART决策树\n\n4.  这m个CART形成随机森林，通过投票表决结果，决定数据属于哪一类（投票机制有一票否决制、少数服从多数、加权多数）\n\n \n假设随机森林中有3棵子决策树，2棵子树的分类结果是A类，1棵子树的分类结果是B类，那么随机森林的分类结果就是A类。\n\n关于调参：\n\n1. 如何选取K，可以考虑有N个属性，取K=根号N\n2. 最大深度（不超过8层）\n3. 棵数\n4. 最小分裂样本树\n5. 类别比例\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>","tags":["机器学习"],"categories":["机器学习"]},{"title":"机器学习汇总","url":"/2019/03/25/ml/","content":"\n# 基础知识\n\n## 1. 参数法与非参数法\n\n机器学习上的方法分：\n\n- **参数方法**\n\n\t根据先验知识假定模型服从某种分布，然后利用训练集估计出模型参数，也就弄清楚了整个模型。   \n\t\n\t假设了一个在整个输入空间上有效的模型，将问题归结为在样本上估计少量参数，(如：线性模型估计w，高斯分布估计mu和sigma)，参数学习方法假定了一个模型,当模型假定不成立或样本不是一个分组可能导致很大的误差。\n\t\n\t例如感知器\n\t\n- **非参数方法**\n\n\t不需要知道数据的概率分布，只需要假设：相似的输入具有相似的输出。基于记忆训练集，然后根据训练集预测。\n\t\n\t非参数方法使用合适的距离度量相似性，对于输入样本，从训练集中找出它们的相似示例(输入样本的邻域)，并由相似的实例插值得到正确的输入。\n\t\n\t例如kNN\n\n\n## 2. 维度灾难\n\n>Curse of Dimensionality\n\n维度灾难是在数字图像处理中，对于已知样本数目，存在一个特征数目的最大值，当实际使用的特征数目超过这个最大值时，分类器的性能不是得到改善，而是退化。\n\n这种现象正是在识别模式中被称为“维度灾难”的一种表现形式。此外，提取特征向量的维度过\n高会增加计算的复杂度，给后续的分类问题造成负面影响。\n\n非参数方法在特征数 p 很大时表现不好。\n\n\n## 3. 分类器性能指标\n\n### 3.1 ROC 曲线\n\n>receiver operating characteristics 接收者操作特征\n\n\n\n![](/img/2019-03-25-ml-1.jpg)\n\n由上表可得出横，纵轴的计算公式：\n\n1. 真正类率 (True Postive Rate)： \\\\(TPR = \\frac{TP}{TP+FN}\\\\)， 代表分类器预测的正类中实际正实例占所有正实例的比例。Sensitivity\n\n2. 负正类率 (False Postive Rate) ： \\\\(FPR = \\frac{FP}{FP+TN}\\\\)，代表分类器预测的正类中实际负实例占所有负实例的比例。1-Specificity\n\n3. 真负类率 (True Negative Rate) ： \\\\(TNR = \\frac{TN}{FP+TN}\\\\)，代表分类器预测的负类中实际负实例占所有负实例的比例， \\\\(TNR=1-FPR\\\\) 。Specificity\n\n假设采用逻辑回归分类器，其给出针对每个实例为正类的概率，那么通过设定一个阈值如0.6，概率大于等于0.6的为正类，小于0.6的为负类。对应的就可以算出一组(FPR,TPR),在平面中得到对应坐标点。\n\n随着阈值的逐渐减小，越来越多的实例被划分为正类，但是这些正类中同样也掺杂着真正的负实例，即TPR和FPR会同时增大。\n\n*阈值最大时，对应坐标点为 \\\\((0,0)\\\\) ,阈值最小时，对应坐标点 \\\\((1,1)\\\\) 。*\n\n![](/img/2019-03-25-ml-2.jpg)\n\n**横轴**：负正类率 (false postive rate FPR) 特异度，*FPR越大，预测正类中实际负类越多。*    \n**纵轴**：真正类率 (true postive rate TPR )灵敏度，*TPR越大，预测正类中实际正类越多。*\n\n**理想目标**：     \n\n\\\\(TPR=1，FPR=0\\\\)，即图中 \\\\((0,1)\\\\) 点，故ROC曲线越靠拢 \\\\((0,1)\\\\) 点，越偏离 45 度对角线越好，Sensitivity、Specificity 越大效果越好。\n\n\n### 3.2 AUC\n\n>area under the curve\n\n常常用AUC来评估二分类模型的性能\n\nRoc 曲线下的面积，介于 0.1 和 1 之间。Auc 作为数值可以直观的评价分类器的好坏，值越大越好。\n\n首先 AUC 值是一个概率值，当你随机挑选一个正样本以及负样本，**当前的分类算法根据计算得到的Score 值将这个正样本排在负样本前面的概率就是 AUC 值，AUC 值越大，当前分类算法越有可能将正样本排在负样本前面，从而能够更好地分类。**\n\nAUC 可以看做随机从正负样本中选取一对正负样本，其中正样本的得分大于负样本的概率！\n\n**AUC 对正负样本比例不敏感**\n\n利用概率解释，还可以得到AUC另外一个性质，对正负样本比例不敏感。 在训练模型的时候，如果正负比例差异比较大，例如正负比例为1:1000，训练模型的时候通常要对负样本进行下采样。当一个模型训练完了之后，用负样本下采样后的测试集计算出来的AUC和未采样的测试集计算的AUC基本一致，或者说前者是后者的无偏估计！ \n\n# 方法\n\n## 1. 梯度法\n\n次梯度法（subgradient method）是传统梯度下降方法的拓展，用来处理不可微（non-differentiable ）的凸函数。它的优势是比传统方法处理问题范围大，但劣势是算法收敛速度慢。而传统的梯度下降方法只能处理可导函数。\n\n![](/img/2019-03-25-ml-3.jpg)\n\n\n\n>其实无论是梯度法还是次梯度法，本质上我们都在使用一阶泰勒展开式的原理去逼近在某点的原函数。正如泰勒展开式的思想所述，将目标函数在某点附近展开为泰勒(Taylor)多项式来逼近原函数。\n\n\n## 1. 朴素贝叶斯\n\n![](/img/2019-03-25-ml-4.jpg)\n\n <div align=center>\n ![](/img/2019-03-25-ml-101.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n**实例解析**\n\n![](/img/2019-03-25-ml-5.jpg)\n\n> 现在给我们的问题是，如果一对男女朋友，男生想女生求婚，男生的四个特点分别是不帅，性格不好，身高矮，不上进，请你判断一下女生是嫁还是不嫁？\n\n这是典型的二分类问题，按照朴素贝叶斯的求解，转换为 \n\n- \\\\(P(嫁 | 不帅、性格不好、矮、不上进)\\\\) \n- \\\\(P(不嫁 | 不帅、性格不好、矮、不上进)\\\\) \n\n的概率，最终选择嫁与不嫁的答案。\n\n这里我们根据贝特斯公式:\n\n![](/img/2019-03-25-ml-6.jpg)\n\n$$P(不帅、性格不好、矮、不上进)=P(嫁)P(不帅|嫁)P(性格不好|嫁)P(矮|嫁)P(不上进|嫁)$$\n$$+P(不嫁)P(不帅|不嫁)P(性格不好|不嫁)P(矮|不嫁)P(不上进|不嫁)$$  \n\n\n$$P(不帅、性格不好、矮、不上进|嫁)=P(不帅|嫁)P(性格不好|嫁)P(矮|嫁)P(不上进|嫁)$$\n\n<br/>\n将上面的公式整理一下可得:\n\n![](/img/2019-03-25-ml-7.jpg)\n\n \n \n但是由贝叶斯公式可得:对于目标求解为不同的类别，贝叶斯公式的分母总是相同的。所以，只求解分子即可：\n \n$$P(嫁)P(不帅|嫁)P(性格不好|嫁)P(矮|嫁)P(不上进|嫁)=1/2 \\* 1/2 \\* 1/6 \\* 1/6 \\* 1/6=1/864$$   \n \n对于类别“不嫁”的贝叶斯分子为:\n\n$$P(不嫁)P(不帅|不嫁)P(性格不好|不嫁)P(矮|不嫁)P(不上进|不嫁)=1/2 \\* 1/3 \\* 1/2 \\* 1\\* 2/3=1/18$$\n\n经代入贝叶斯公式可得：\n\n$$P(嫁|不帅、性格不好、矮、不上进)=(1/864) / (1/864+1/18)=1/49=2.04\\%$$\n\n$$P(不嫁|不帅、性格不好、矮、不上进)=(1/18) / (1/864+1/18)=48/49=97.96\\%$$\n\n则 \\\\(P(不嫁|不帅、性格不好、矮、不上进) > P(嫁|不帅、性格不好、矮、不上进)\\\\) ，则该女子选择不嫁！\n\n<br/>\n\n**优点**：  \n\n- 算法逻辑简单,易于实现（算法思路很简单，只要使用贝叶斯公式转化即可！）\n- 分类过程中时空开销小（假设特征相互独立，只会涉及到二维存储）\n\n\n**缺点**：\n\n- 朴素贝叶斯假设属性之间相互独立，这种假设在实际过程中往往是不成立的。在属性之间相关性越大，分类误差也就越大。\n\n\n> [带你彻彻底底搞懂朴素贝叶斯公式](https://blog.csdn.net/fisherming/article/details/79509025)    \n> [朴素贝叶斯算法原理小结](https://www.cnblogs.com/pinard/p/6069267.html)\n\n\n\n\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n","tags":["机器学习"],"categories":["机器学习"]},{"title":"c++ 基础汇总","url":"/2019/03/17/cpp/","content":"\n\n# 类\n## 1. 类成员函数\n\n类的成员函数是指那些**把定义和原型写在类定义内部的函数，就像类定义中的其他变量一样**。\n\n类成员函数是类的一个成员，它可以操作类的任意对象，可以访问对象中的所有成员。\n\n范围解析运算符 `::`\n\n\n在类里面不写是什么类型，默认是 private 的。\n\n## 2. 构造函数 constructor\n\n类的构造函数是类的一种特殊的成员函数，**它会在每次创建类的新对象时执行。**\n\n构造函数的名称与类的名称是完全相同的，并且不会返回任何类型，也不会返回 void。\n\n**构造函数可用于为某些成员变量设置初始值。**\n\n```cpp\n#include <iostream>\n \nusing namespace std;\n \nclass Line\n{\n   public:\n      void setLength( double len );\n      double getLength( void );\n      Line();  // 这是构造函数\n \n   private:\n      double length;\n};\n \n// 成员函数定义，包括构造函数\nLine::Line(void)\n{\n    cout << \"Object is being created\" << endl;\n}\n \n```\n\n### 带参数的构造函数\n\n默认的构造函数没有任何参数，但如果需要，构造函数也可以带有参数。这样在创建对象时就会给对象赋初始值，如下面的例子所示：\n\n```cpp\n#include <iostream>\n \nusing namespace std;\n \nclass Line\n{\n   public:\n      void setLength( double len );\n      double getLength( void );\n      Line(double len);  // 这是构造函数\n \n   private:\n      double length;\n};\n \n// 成员函数定义，包括构造函数\nLine::Line( double len)\n{\n    cout << \"Object is being created, length = \" << len << endl;\n    length = len;\n}\n \nvoid Line::setLength( double len )\n{\n    length = len;\n}\n \ndouble Line::getLength( void )\n{\n    return length;\n}\n// 程序的主函数\nint main( )\n{\n   Line line(10.0);\n \n   // 获取默认设置的长度\n   cout << \"Length of line : \" << line.getLength() <<endl;\n   // 再次设置长度\n   line.setLength(6.0); \n   cout << \"Length of line : \" << line.getLength() <<endl;\n \n   return 0;\n}\n```\n\n### 使用初始化列表来初始化字段\n\n\n\n使用初始化列表来初始化字段：\n\n```\nLine::Line( double len): length(len) \n{ \n\tcout << \"Object is being created, length = \" << len << endl; \n}\n```\n\n上面的语法等同于如下语法：\n\n```cpp\nLine::Line( double len) \n{ \n\tlength = len; \n\tcout << \"Object is being created, length = \" << len << endl; \n}\n```\n\n假设有一个类 C，具有多个字段 X、Y、Z 等需要进行初始化，同理地，您可以使用上面的语法，只需要在不同的字段使用逗号进行分隔，如下所示：\n\n```\nC::C( double a, double b, double c): X(a), Y(b), Z(c) \n{ \n\t.... \n}\n```\n\n## 3. 析构函数 destructor\n\n类的析构函数是类的一种特殊的成员函数，它会在每次删除所创建的对象时执行。\n\n析构函数的名称与类的名称是完全相同的，只是在前面加了个波浪号（~）作为前缀，它不会返回任何值，也不能带有任何参数。\n\n**析构函数有助于在跳出程序（比如关闭文件、释放内存等）前释放资源。**\n\n\n## 4. 友元函数\n\n类的友元函数是定义在类外部，但**有权访问类的所有私有（private）成员和保护（protected）成员**。\n\n尽管友元函数的原型有在类的定义中出现过，但是友元函数并不是成员函数。\n\n`friend class ClassTwo;`\n\n\n```python\n#include <iostream>\n \nusing namespace std;\n \nclass Box\n{\n   double width;\npublic:\n   friend void printWidth( Box box );\n   void setWidth( double wid );\n};\n\n// 成员函数定义\nvoid Box::setWidth( double wid )\n{\n    width = wid;\n}\n\n// 请注意：printWidth() 不是任何类的成员函数\nvoid printWidth( Box box )\n{\n   /* 因为 printWidth() 是 Box 的友元，它可以直接访问该类的任何成员 */\n   cout << \"Width of box : \" << box.width <<endl;\n}\n \n// 程序的主函数\nint main( )\n{\n   Box box;\n \n   // 使用成员函数设置宽度\n   box.setWidth(10.0);\n   \n   // 使用友元函数输出宽度\n   printWidth( box );\n \n   return 0;\n}\n```\n\n输出：  \n\n`Width of box : 10`\n\n## 5. 内联函数\n\nC++ 内联函数是通常与类一起使用。如果一个函数是内联的，那么在编译时，**编译器会把该函数的代码副本放置在每个调用该函数的地方。**\n\n对内联函数进行任何修改，都需要重新编译函数的所有客户端，因为编译器需要重新更换一次所有的代码，否则将会继续使用旧的函数。\n\n如果想把一个函数定义为内联函数，则需要在函数名前面放置关键字 inline，在调用函数之前需要对函数进行定义。如果已定义的函数多于一行，编译器会忽略 inline 限定符。\n\n在类定义中的定义的函数都是内联函数，即使没有使用 inline 说明符。\n\n下面是一个实例，使用内联函数来返回两个数中的最大值：\n\n```cpp\n#include <iostream>\n \nusing namespace std;\n\ninline int Max(int x, int y)\n{\n   return (x > y)? x : y;\n}\n\n// 程序的主函数\nint main( )\n{\n\n   cout << \"Max (20,10): \" << Max(20,10) << endl;\n   cout << \"Max (0,200): \" << Max(0,200) << endl;\n   cout << \"Max (100,1010): \" << Max(100,1010) << endl;\n   return 0;\n}\n```\n\n\n# stream\n\n\n在C++中，有一个stream这个类，所有的I/O都以这个“流”类为基础的，包括我们要认识的文件I/O.\n\n![](/img/2019-03-26-stream-1.jpg)\n\n![](/img/2019-03-26-stream-2.jpg)\n\nI/O 对象无拷贝或者赋值\n \nstream 这个类有两个重要的运算符：\n\n1. 插入器 `<<`\n\n　　向流输出数据。比如说系统有一个默认的标准输出流 `cout`，一般情况下就是指的显示器，所以，`cout<<\"Write Stdout\"<<'\\n'` 就表示把字符串\"Write Stdout\"和换行字符('\\n')输出到标准输出流。\n\n2. 析取器 `>>`\n\n　　从流中输入数据。比如说系统有一个默认的标准输入流(cin)，一般情况下就是指的键盘，所以，cin>>x;就表示从标准输入流中读取一个指定类型的数据。\n\n## `iostream`\n\n- `istream` 输入流 (`cin`)\n- `ostream` 输出流 (`cout`)\n\n\n\n\n## `fstream`\n\n　　在 C++ 中，对文件的操作是通过stream的子类fstream(file stream)来实现的，所以，要用这种方式操作文件，就必须加入头文件fstream.h。    \n　　\n　　  \nofstream是从内存到硬盘，ifstream是从硬盘到内存，其实所谓的流缓冲就是内存空间\n\n\n\n# 指针\n\n## 基础\n\n`int* pN = &N;`\n\n`pN` 和 `&N` 是 `int *` 类型\n\n`*pN` 是 `int` 类型\n\n\n\n## 数组的指针\n\n**数组名实际上就是数组数据所在内存区域的首地址**，表示数组在内存中的起始位置。\n\n```cpp\nint nArray[3] = {1, 2, 3}; // 定义一个数组\ncout << nArray;\n```\n\n输出为：`0x7ffeeeb00ffc`\n\n可以通过把首地址赋值给指针，然后对该指针进行加减运算，使指针发生偏转指向数组中的其他元素，从而遍历整个数组。例如：\n\n```cpp\nint nArray[3] = { 1, 2, 3 };   // 定义一个数组\nint* pIndex = nArray;          // 将数组的起始地址赋值给指针pIndex\ncout<<\"指针指向的地址是：\"<<pIndex<<endl;       // 输出指针指向的地址\ncout<<\"指针所指向的数据的值是：\"<<*pIndex<<endl; // 输出这个位置上的数据\n\npIndex++;   // 对指针进行加运算，使其指向数组中的下一个值\ncout<<\"指针指向的地址是：\"<<pIndex<<endl;        // 输出指针指向的地址\ncout<<\"指针所指向的数据的值是：\"<<*pIndex<<endl;  // 输出数据\n```\n输出为 \n\n```cpp\n指针指向的地址是：0016FA38\n\n指针所指向的数据的值是：1\n\n指针指向的地址是：0016FA3C\n\n指针所指向的数据的值是：2\n```\n\n从输出结果中可以看到，pIndex 指针初始指向的地址是 `0016FA38`，也就是 nArray 这个数组的首地址。换句话说，也就是 pIndex 指向的是数组中的第一个数据，所以输出 `*pIndex` 的值是 1。    \n\n而在对指针进行加 1 运算后，指针指向的地址变为 `0016FA3C`，它向地址增大的方向偏移了 **4** 个字节，指向了数组中的第二个数据，输出 `*pIndex`的值自然也就变成了 2。\n\n![](/img/2019-03-17-pointer-1.jpg)\n\n除了指针的加减算术运算之外，常用到的还有指针的关系运算。指针的关系运算通常用 `==` 或 `!=` 来判断两个相同类型的指针是否相等，也就是判断它们是否指向同一地址上的同一数据，以此作为条件或循环结构中的条件判断语句。例如：\n\n\n```cpp\nint nArray[3] = { 1, 2, 3 };    // 定义一个数组\nint* pIndex = nArray;           // 将数组的起始地址赋值给指针pIndex\nint* pEnd = nArray + 3;         // 计算数组的结束地址并赋值给pEnd\nwhile( pIndex != pEnd )         // 在while的条件语句中判断两个指针是否相等，\n                               // 也就是判断当前指针是否已经偏转到结束地址\n{\n    cout<<*pIndex<<endl;        // 输出当前指针指向的数据\n    // 对指针进行加1 运算，\n   // 使其偏移到下一个内存位置，指向数组中的下一个数据\n    ++pIndex;                  \n}\n```\n\n\n\n## -> \n\nc++ 中当定义类对象是指针对象时候，就需要用到 `->` 指向类中的成员；当定义一般对象时候时就需要用到 `.` 指向类中的成员。\n\n例如：\n\n```cpp\nclass A\n{\n　　public\n　　play();\n}\n```\n\n如果定义如下：\n\n`A *p` 则使用：`p->play();` 左边是结构指针。\n\n`A p` 则使用：`p.paly();` 左边是结构变量。\n\n总结：\n\n箭头（`->`）：左边必须为指针；\n\n点号（`.`）：左边必须为实体。\n\n\n## this 指针\n\n\n在 C++ 中，每一个对象都能通过 this 指针来访问自己的**地址**。this 指针是所有成员函数的隐含参数。因此，在成员函数内部，它可以用来指向调用对象。\n\n>友元函数没有 this 指针，因为友元不是类的成员。只有成员函数才有 this 指针。\n\n\n```cpp\n#include <iostream>\n \nusing namespace std;\n \nclass Box\n{\n   public:\n      // 构造函数定义\n      Box(double l=2.0, double b=2.0, double h=2.0)\n      {\n         cout <<\"Constructor called.\" << endl;\n         length = l;\n         breadth = b;\n         height = h;\n      }\n      double Volume()\n      {\n         return length * breadth * height;\n      }\n      int compare(Box box)\n      {\n         return this->Volume() > box.Volume();\n      }\n   private:\n      double length;     // Length of a box\n      double breadth;    // Breadth of a box\n      double height;     // Height of a box\n};\n \nint main(void)\n{\n   Box Box1(3.3, 1.2, 1.5);    // Declare box1\n   Box Box2(8.5, 6.0, 2.0);    // Declare box2\n \n   if(Box1.compare(Box2))\n   {\n      cout << \"Box2 is smaller than Box1\" <<endl;\n   }\n   else\n   {\n      cout << \"Box2 is equal to or larger than Box1\" <<endl;\n   }\n   return 0;\n}\n```\n\n# 引用\n\n引用变量是一个别名，也就是说，**它是某个已存在变量的另一个名字**。一旦把引用初始化为某个变量，就可以使用该引用名称或变量名称来指向变量。\n\n> C++ 引用 vs 指针\n>引用很容易与指针混淆，它们之间有以下不同：  \n> \n> - 不存在空引用。引用必须连接到一块合法的内存。\n> - 一旦引用被初始化为一个对象，就不能被指向到另一个对象。\n> - 指针可以在任何时候指向到另一个对象。\n> - 引用必须在创建时被初始化。指针可以在任何时间被初始化。","tags":["c++"],"categories":["编程语言"]},{"title":"make/cmake/qmake","url":"/2019/01/22/make/","content":"\n# make/cmake/qmake\n\n>Makefile是类unix环境下(比如Linux)的类似于批处理的\"脚本\"文件。\n\ngcc是GNU Compiler Collection（就是GNU编译器套件），也可以简单认为是编译器，它可以编译很多种编程语言（括C、C++、Objective-C、Fortran、Java等等\n\n1. make 是用来执行Makefile的  \n2. Makefile是类unix环境下(比如Linux)的类似于批处理的\"脚本\"文件。其基本语法是: **目标+依赖+命令**，只有在**目标**文件不存在，或**目标**比**依赖**的文件更旧，**命令**才会被执行。由此可见，Makefile和make可适用于任意工作，不限于编程。比如，可以用来管理latex。  \n3. Makefile+make可理解为类unix环境下的项目管理工具，但它太基础了，抽象程度不高，而且在windows下不太友好(针对visual studio用户)，于是就有了跨平台项目管理工具cmake  \n4. cmake是跨平台项目管理工具，它用更抽象的语法来组织项目。虽然，仍然是目标，依赖之类的东西，但更为抽象和友好，比如你可用math表示数学库，而不需要再具体指定到底是math.dll还是libmath.so，在windows下它会支持生成visual studio的工程，在linux下它会生成Makefile，甚至它还能生成eclipse工程文件。也就是说，从同一个抽象规则出发，它为各个编译器定制工程文件。  \n5. cmake是抽象层次更高的项目管理工具，cmake命令执行的CMakeLists.txt文件  \n6. qmake是Qt专用的项目管理工具，对应的工程文件是*.pro，在Linux下面它也会生成Makefile，当然，在命令行下才会需要手动执行qmake，完全可以在qtcreator这个专用的IDE下面打开*.pro文件，使用qmake命令的繁琐细节不用你管了。\n\n总结一下，make用来执行Makefile，cmake用来执行CMakeLists.txt，qmake用来处理*.pro工程文件。Makefile的抽象层次最低，cmake和qmake在Linux等环境下最后还是会生成一个Makefile。cmake和qmake支持跨平台，cmake的做法是生成指定编译器的工程文件，而qmake完全自成体系。\n\n具体使用时，Linux下，小工程可手动写Makefile，大工程用automake来帮你生成Makefile，要想跨平台，就用cmake。如果GUI用了Qt，也可以用qmake+*.pro来管理工程，这也是跨平台的。当然，cmake中也有针对Qt的一些规则，并代替qmake帮你将qt相关的命令整理好了。\n\n另外，需要指出的是，make和cmake主要命令只有一条，make用于处理Makefile，cmake用来转译CMakeLists.txt，而qmake是一个体系，用于支撑一个编程环境，它还包含除qmake之外的其它多条命令(比如uic，rcc,moc)。\n\n上个简图，其中cl表示visual studio的编译器，gcc表示linux下的编译器\n<div align=center>\n![](/img/2019-01-22-make-1.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n  \n\n1. gcc是GNU Compiler Collection（就是GNU编译器套件），也可以简单认为是编译器，它可以编译很多种编程语言（括C、C++、Objective-C、Fortran、Java等等）。\n\n2. 当你的程序只有一个源文件时，直接就可以用gcc命令编译它。\n\n3. 但是当你的程序包含很多个源文件时，用gcc命令逐个去编译时，你就很容易混乱而且工作量大\n\n4. 所以出现了make工具  \nmake工具可以看成是一个智能的批处理工具，它本身并没有编译和链接的功能，而是用类似于批处理的方式—通过调用makefile文件中用户指定的命令来进行编译和链接的。\n\n5. makefile是什么？简单的说就像一首歌的乐谱，make工具就像指挥家，指挥家根据乐谱指挥整个乐团怎么样演奏，make工具就根据makefile中的命令进行编译和链接的。\n\n6. makefile命令中就包含了调用gcc（也可以是别的编译器）去编译某个源文件的命令。\n\n7. makefile在一些简单的工程完全可以人工手下，但是当工程非常大的时候，手写makefile也是非常麻烦的，如果换了个平台makefile又要重新修改。\n\n8. 这时候就出现了Cmake这个工具，cmake就可以更加简单的生成makefile文件给上面那个make用。当然cmake还有其他功能，就是可以跨平台生成对应平台能用的makefile，你不用再自己去修改了。\n\n9. 可是cmake根据什么生成makefile呢？它又要根据一个叫CMakeLists.txt文件（学名：组态档）去生成makefile。\n\n10. 到最后CMakeLists.txt文件谁写啊？亲，是你自己手写的。\n\n11. 当然如果你用IDE，类似VS这些一般它都能帮你弄好了，你只需要按一下那个三角形\n\n12. 接着是qmake，qmake是什么，先说一下Qt这个东西。Qt是跨平台C++图形用户界面应用程序开发框架。它既可以开发GUI程序，也可用于开发非GUI程序，比如控制台工具和服务器。简单的说就是C++的第三方库，使用这个库你可以很容易生成windows，Linux，MAC os等等平台的图形界面。现在的Qt还包含了开发各种软件一般需要用到的功能模块（网络，数据库，XML，多线程啊等等），比你直接用C++（只带标准内裤那种）要方便和简单。\n\n13. 你可以用Qt简简单单就实现非常复杂的功能，是因为Qt对C++进行了扩展，你写一行代码，Qt在背后帮你写了几百上千行，而这些多出来的代码就是靠Qt专有的moc编译器（The Meta-Object Compiler）和uic编译器（User Interface Complier）来重新翻译你那一行代码。问题来了，你在进行程序编译前就必须先调用moc和uic对Qt源文件进行预处理，然后再调用编译器进行编译。上面说的那种普通makefile文件是不适用的，它没办法对qt源文件进行预处理。所以qmake就产生了。\n\n14. qmake工具就是Qt公司制造出来，用来生成Qt 专用makefile文件，这种makefile文件就能自动智能调用moc和uic对源程序进行预处理和编译。qmake当然必须也是跨平台的，跟cmake一样能对应各种平台生成对应makefile文件。\n\n15. qmake是根据Qt 工程文件（.pro）来生成对应的makefile的。工程文件（.pro）相对来说比较简单，一般工程你都可以自己手写，但是一般都是由Qt的开发环境 Qt Creator自动生成的，你还是只需要按下那个邪恶三角形就完事了。\n\n16. 还没有完，由于qmake很简单很好用又支持跨平台，而且是可以独立于它的IDE，所以你也可以用在非Qt工程上面，照样可以生成普通的makefile，只要在pro文件中加入CONFIG -= qt 就可以了。\n\n17. 这样qmake和cmake有什么区别？  \n不好意思，cmake也是同样支持Qt程序的，cmake也能生成针对qt 程序的那种特殊makefile，  \n只是cmake的CMakeLists.txt 写起来相对与qmake的pro文件复杂点。  \nqmake 是为 Qt 量身打造的，使用起来非常方便，但是cmake功能比qmake强大。  \n一般的Qt工程你就直接使用qmake就可以了，cmake的强大功能一般人是用不到的。  \n当你的工程非常大的时候，又有qt部分的子工程，又有其他语言的部分子工程，据说用cmake会 方便，我也没试过。\n    \n    \n     \n>[make makefile cmake qmake都是什么，有什么区别？](https://www.zhihu.com/question/27455963)"},{"title":"GAN","url":"/2018/09/19/gan/","content":"\n\n## 概述\n\n>论文地址：[Generative Adversarial Nets](https://arxiv.org/pdf/1406.2661v1.pdf)\n\n\nGAN是Ian Goodfellow提出的使用对抗过程来获得生成模型的新框架。\n\n生成对抗网络主要由两个部分组成，\n\n- 生成器 \\\\(G\\\\) (Generator)\n- 判别器 \\\\(D\\\\) (discriminator)\n\n生成器G的作用：    \n尽量去拟合 (cover) 真实数据分布，**生成以假乱真的图片**。它的输入参数是一个随机噪声 \\\\(z，G(z)\\\\) 代表其生成的一个样本 (fake data)。\n\n判别器 \\\\(D\\\\) 的作用：   \n判断一张图片是否是“真实的”，**即能判断出一张图片是真实数据(training data)还是生成器 \\\\(G\\\\) 生成的样本 (fake data)**。它的输入参数是 \\\\(x，x\\\\) 代表一张图片， \\\\(D(x)\\\\) 代表 \\\\(x\\\\) 是真实图片的概率。\n\n\n**具体过程：**\n\n1. 对于从训练数据中取样出的真实图片x，判别器D希望D(x)的输出值接近1，即判定训练数据为真实图片。\n\n2. 给定一个随机噪声z，判别器D希望 \\\\(D(G(z))\\\\) 的输出值接近 \\\\(0\\\\) ，即认定生成器G生成的图片是假的；而生成器G希望 \\\\(D(G(z))\\\\) 的输出值接近1，即G希望能够欺骗D，让D将生成器G生成的样本误判为真实图片。这样 G 和 D 就构成了博弈的状态。\n\n3. 在博弈的过程中，生成器G和判别器D都不断的提升自己的能力，最后达到一个平衡的状态。G可以生成足以“以假乱真”的图片G(z)。对于D来说，它难以判定G生成的图片究竟是不是真实的，因此 \\\\(D(G(z)) = 0.5\\\\) 。这样我们的目的就达成了：我们得到了一个生成式的模型G，它可以用来生成真实图片。\n\n>一个很有意思例子：生成器G可以被比作假币制造者团队，试图生产出无法检测出真伪的假币；判别器D可以被比作警察，试图区分出真币和假币。在比拼竞争的过程中，双方都不断提升自己的方法，最终导致假币与真品无法区分。说明我们得到了一个效果非常好的生成器G\n\n\n![](https://ws3.sinaimg.cn/large/006tNbRwgy1fvf1vsgjyrj30mt09uaaa.jpg)\n![](https://ws4.sinaimg.cn/large/006tNbRwgy1fvf33sz4l9j30hs0by74y.jpg)\n<center><small><font color=gray>  这里的G网络的输入是一个符合简单分布如高斯分布或者均匀分布的随机噪声  </font></small></center>\n\n\n![](https://ws3.sinaimg.cn/large/006tNbRwgy1fvf4nrc9mfj31980g6wh5.jpg)\n<center><small><font color=gray>  黑色的线表示数据x的实际分布，绿色的线表示数据的生成分布，蓝色的线表示生成的数据对应在判别器中的分布效果  </font></small></center>\n\n\n- 对于图a，D还刚开始训练，本身分类的能力还很有限，有波动，但是初步区分实际数据和生成数据还是可以的。     \n- 图b，D训练得比较好了，可以很明显的区分出生成数据。     \n- 图c：绿色的线与黑色的线的偏移，蓝色的线下降了，也就是生成数据的概率下降了。那么，由于绿色的线的目标是提升概率，因此就会往蓝色线高的方向移动。那么随着训练的持续，由于G网络的提升，G也反过来影响D的分布。假设固定G网络不动，训练D，那么训练到最优， \\\\(D^\\*\\_g(x) = p\\_{data}(x)/(p\\_{data}(x)+p\\_{g}(x))\\\\) 。     \n- 因此，随着 \\\\(p\\_g(x)\\\\) 趋近于 \\\\(p\\_{data}(x),D^\\*\\_g(x)\\\\) 会趋近于 \\\\(0.5\\\\) ，也就是到图d。而我们的目标就是希望绿色的线能够趋近于黑色的线，也就是让生成的数据分布与实际分布相同。图d符合我们最终想要的训练结果。到这里，G网络和D网络就处于纳什均衡状态，无法再进一步更新了。\n\n\n\n**GAN 模型的目标函数：**\n\n$$\\min\\_{G} \\max_D V(G,D)=E\\_{x\\sim P\\_{data}}[logD(x)] + E\\_{x\\sim P\\_G}[log(1-D(x))]$$\n\n或者我们可以直接理解：    \nG网络的loss是 \\\\(log(1-D(G(z))\\\\) ，    \n而D的loss是 \\\\(-(log(D(x)) + log(1-D(G(z)))\\\\) \n\n### 怎么训练\n    \n#### 交替训练。\n\n1. 固定G时，训练D；\n$$\\max {E\\_{x\\sim p(r)} log(D(x)) + E\\_{x\\sim p(g)} log(1-D(x)) } $$（公式1）   \n转化成最小形式：\n$$\\min -[{E\\_{x\\sim p(r)} log(D(x)) + E\\_{x\\sim p(g)} log(1-D(x)) } ]$$       \n2. 固定D时，训练G。   \n可以设置超参数k, 表示训练k次D，再训练一次G.\n$$min Loss_G =E\\_{x\\sim P\\_G}[log(1-D(x))] $$（公式2）\n\n\n![](https://ws1.sinaimg.cn/large/006tNbRwgy1fvf3xw32xtj31f20xuah2.jpg)\n\n全局最优  \\\\( p\\_g=p\\_{data}\\\\)\n\n\n <script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n\n","tags":["GAN"],"categories":["GAN"]},{"title":"强化学习（二）","url":"/2018/08/13/RL2/","content":"\n<br/>\n<div align=center>\n![](/img/2018-08-13-RL2-1.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n强化学习（RL，基于MDP）的求解policy的方式一般分为三种：\n\n- Value <—critic\n- Policy <—actor\n- Value + Policy <— Actor-critic\n\n\n## 策略梯度\n\n强化学习是一个通过奖惩来学习正确行为的机制. 家族中有很多种不一样的成员, 有学习奖惩值, 根据自己认为的高价值选行为, 比如 Q learning, Deep Q Network, 也有不通过分析奖励值, 直接输出行为的方法, 这就是今天要说的 Policy Gradients 了. 甚至我们可以为 Policy Gradients 加上一个神经网络来输出预测的动作. 对比起以值为基础的方法, Policy Gradients 直接输出动作的最大好处就是, 它能在一个连续区间内挑选动作, 而基于值的, 比如 Q-learning, 它如果在无穷多的动作中计算价值, 从而选择行为, 这, 它可吃不消.\n\n有了神经网络当然方便, 但是, 我们怎么进行神经网络的误差反向传递呢? Policy Gradients 的误差又是什么呢? 答案是! 哈哈, 没有误差! 但是他的确是在进行某一种的反向传递. 这种反向传递的目的是让这次被选中的行为更有可能在下次发生. 但是我们要怎么确定这个行为是不是应当被增加被选的概率呢? 这时候我们的老朋友, reward 奖惩正可以在这时候派上用场,\n\n![](/img/2018-08-13-RL2-2.jpg)\n<div align=center>\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n现在我们来演示一遍, 观测的信息通过神经网络分析, 选出了左边的行为, 我们直接进行反向传递, 使之下次被选的可能性增加, 但是奖惩信息却告诉我们, 这次的行为是不好的, 那我们的动作可能性增加的幅度 随之被减低. 这样就能靠奖励来左右我们的神经网络反向传递. 我们再来举个例子, 假如这次的观测信息让神经网络选择了右边的行为, 右边的行为随之想要进行反向传递, 使右边的行为下次被多选一点, 这时, 奖惩信息也来了, 告诉我们这是好行为, 那我们就在这次反向传递的时候加大力度, 让它下次被多选的幅度更猛烈! 这就是 Policy Gradients 的核心思想了. 很简单吧.\n\n\nPolicy gradient 是 RL 中另外一个大家族, 他不像 Value-based 方法 (Q learning, Sarsa), 但他也要接受环境信息 (observation), 不同的是他要输出不是 action 的 value, 而是具体的那一个 action, 这样 policy gradient 就跳过了 value 这个阶段. 而且个人认为 Policy gradient 最大的一个优势是: 输出的这个 action 可以是一个连续的值, 之前我们说到的 value-based 方法输出的都是不连续的值, 然后再选择值最大的 action. 而 policy gradient 可以在一个连续分布上选取 action.\n\n介绍的 policy gradient 的第一个算法是一种基于 整条回合数据 的更新, 也叫 REINFORCE 方法. 这种方法是 policy gradient 的最基本方法, 有了这个的基础, 我们再来做更高级的.\n\n![](/img/2018-08-13-RL2-3.jpg)\n<div align=center>\n\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n`log(Policy(s,a))*V` 中的 `log(Policy(s,a))` 表示在 状态 `s` 对所选动作 `a` 的吃惊度, 如果 `Policy(s,a)` 概率越小, 反向的 `log(Policy(s,a))` (即 `-log(P)`) 反而越大. 如果在 `Policy(s,a)` 很小的情况下, 拿到了一个 大的 `R`, 也就是 大的 `V`, 那 `-log(Policy(s, a))*V` 就更大, 表示更吃惊, (我选了一个不常选的动作, 却发现原来它能得到了一个好的 reward, 那我就得对我这次的参数进行一个大幅修改). 这就是 `log(Policy)*V` 的物理意义啦\n\n### 策略梯度 \n\n## Actor Critic\n\n我们有了像 Q-learning 这么伟大的算法, 为什么还要瞎折腾出一个 Actor-Critic? 原来 Actor-Critic 的 Actor 的前生是 Policy Gradients, 这能让它毫不费力地在连续动作中选取合适的动作, 而 Q-learning 做这件事会瘫痪. 那为什么不直接用 Policy Gradients 呢? 原来 Actor Critic 中的 Critic 的前生是 Q-learning 或者其他的 以值为基础的学习法 , 能进行单步更新, 而传统的 Policy Gradients 则是回合更新, 这降低了学习效率.\n\n结合了 Policy Gradient (Actor) 和 Function Approximation (Critic) 的方法. Actor 基于概率选行为, Critic 基于 Actor 的行为评判行为的得分, Actor 根据 Critic 的评分修改选行为的概率.\n\n![](/img/2018-08-13-RL2-101.jpg)\n<div align=center>\n\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n或者说详细点, 就是 Actor 在运用 Policy Gradient 的方法进行 Gradient ascent 的时候, 由 Critic 来告诉他, 这次的 Gradient ascent 是不是一次正确的 ascent, 如果这次的得分不好, 那么就不要 ascent 那么多.\n\n## DDPG\n\n![](/img/2018-08-13-RL2-102.jpg)\n<div align=center>\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n","tags":["强化学习"],"categories":["强化学习"]},{"title":"AlphaGO Zero 原理","url":"/2018/08/08/AlphaGO-Zero/","content":"\n## 1. 概述\n\n简单来说，AlphaGo Zero 的训练可以分为三个同时进行的阶段：\n\n- 自我对战\n- 再训练网络\n- 评估网络\n\n在自我对战阶段， AlphaGo Zero 创建一个训练集合，自我完成对战 25000 次。**棋局每变动一次，博弈、搜索可能性和胜出者的信息将被存储。**\n\n**训练网络阶段，是神经网络权值得到优化的过程。**在一次完整的训练循环中， AlphaGo Zero 将从 50 万局博弈中选取 2048 个移动位置作为样品，并对这些位置的神经网络进行训练。之后，通过损失函数，来对比神经网络预测与搜索可能性和实际胜出方的信息。每完成一千次这样的训练循环，就对神经网络进行一次评估。\n\n**在评估网络阶段，测试新的神经网络是否得到优化。**在这个过程中，博弈双方都通过各自的神经网络评估叶节点，并使用蒙特卡洛树搜索进行下一步棋路的选择。\n<div align=center>\n![](/img/2018-08-08-AlphaGO-Zero-1.jpg)\n<center><small><font color=gray>  AlphaGo Zero结构图  </font></small></center>\n</div>\n<br/>\n<div align=center>\n![](/img/2018-08-08-AlphaGO-Zero-2.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n**a 部分是利用初始化的神经网络和MCTS进行自博弈，收集到对弈的数据以及胜负关系**\n\n程序自我对弈完成一个棋局产生一个状态序列  \\\\(s\\_1,...,s\\_T\\\\)  ，在  \\\\(T\\\\)  时刻棋局结束，产生了获胜方，用  \\\\(z\\\\)  表示。在其中的每一个时刻  \\\\(T\\\\)  ，棋局状态用 \\\\( s\\_t\\\\)  表示，会在神经网络 \\\\( f\\_{\\theta} \\\\) 的引导下执行一次 MCTS 搜索  \\\\({\\alpha}\\_{\\theta}\\\\)  ，通过 MCTS 搜索计算得到的概率  \\\\(a\\_t \\sim {\\pi}\\_t\\\\)  确定如何行为（在何处落子）\n\n**b 部分是利用收集到的数据训练当前棋盘的价值和每一个走子的概率 (神经网络的训练过程)**      \n\n神经网络的输入是某时刻  \\\\(t\\\\)  的棋局状态  \\\\(s\\_t\\\\)  外加一些历史和额外信息，输出是一个行为概率向量 \\\\( p\\_t\\\\)  和一个标量  \\\\(v\\_t\\\\) \n\nAlpha Zero 算法主体思想就是在策略迭代过程中重复使用上述两个工具：神经网络的参数得以更新，这样可以使得神经网络的输出  \\\\((p, v) = f_{\\theta}(s) \\\\) ：移动概率和获胜奖励更接近与经过改善了的搜索得到的概率以及通过自我对弈得到的棋局结果，后者用 \\\\( (\\pi, z) \\\\) 表示。得到的新参数可以在下一次自我对弈的迭代过程中让搜索变得更加强大。\n\n- \\\\(p\\\\) (move probabilities) 在当前棋局状态下采取每种可能落子方式的概率\n- \\\\(v\\\\) 当前棋局状态 \\\\(s\\\\) 下棋手最终获胜还是落败（分别用 \\\\(1\\\\) 和 \\\\(-1\\\\) 表示）\n-  \\\\(\\pi\\\\) 表示经过神经网络改善了的蒙特卡洛树搜索输出的选择每一个 move 的概率\n-   \\\\(z\\\\) 表示通过自我对弈得到的棋局结果\n\n>The vector of move probabilities  \\\\(p\\\\)  represents the probability of selecting each move a (including pass),  \\\\(p\\_a = Pr(a|s)\\\\) . The value  \\\\(v\\\\)  is a scalar evaluation, estimating the probability of the current player winning from position  \\\\(s\\\\) . \n>       \n>The MCTS search outputs probabilities π of playing each move. These search probabilities usually select much stronger moves than the raw move probabilities p of the neural network  \\\\(f\\_θ(s)\\\\) ; MCTS may therefore be viewed as a powerful policy improvement operator. Self­play with search—using the improved MCTS­based policy to select each move, then using the game winner  \\\\(z\\\\)  as a sample of the value—may be viewed as a powerful policy evaluation operator. \n\n<br/>\n## 2. 传统蒙特卡洛树搜索 MCTS\n\n### 2.1 树搜索\n\n围棋第一手有 \\\\(361\\\\) 种下法，第二手有 \\\\(360\\\\) 种，第三手有 \\\\(359\\\\) ，依次类推，即一共有  \\\\(361!\\\\)  种下法。这个一个天文数字，比目前可观测宇宙的所有原子数还要多。要进行完全树搜索，是不可能的。因此我们必须进行剪枝，并限制思考的深度。\n<div align=center>\n![](/img/2018-08-08-AlphaGO-Zero-3.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n<br/>\n所谓**剪枝**，就是指没必要考虑每种下法，我们**只需考虑最有价值的几手下法**。所谓**限制思考的深度**，就是我们最多只思考5步，10步，20步。常见的算法是Alpha-beta剪枝算法。但是，剪枝算法也有它的缺陷，它很有可能**过早的剪掉了后期价值很大走法**。\n\n<br/>\n### 2.2 蒙特卡洛方法\n\n简而言之，蒙特卡洛方法(Monte Carlo method)，是一种“统计模拟方法”。\n\n假设我们要计算一个不规则形状的面积，我们只需在包含这个不规则形状的矩形内，随机的掷出一个点，每掷出一个点，则 \\\\(N+1\\\\) ，如果这个点在不规则图形内则 \\\\(W+1\\\\) 。落入不规则图形的概率即为 \\\\( W/N\\\\) 。当掷出足够多的点之后，我们可以认为： \\\\(不规则图形面积＝矩形面积＊W/N\\\\) 。\n\n要应用蒙特卡洛算法的问题，**首先要将问题转化为概率问题**，然后通过统计方法将其问题的解估计出来。\n\n<br/>\n### 2.3 蒙特卡洛树搜索 MCTS\n\n这种算法简而言之是用蒙特卡洛方法估算每一种走法的胜率。如果描述的再具体一些，通过不断的模拟每一种走法，直至终局，该走法的模拟总次数 \\\\(N\\\\) ，与胜局次数 \\\\(W\\\\) ，即可推算出该走法的胜率为  \\\\(W/N\\\\) 。\n<div align=center>\n![](/img/2018-08-08-AlphaGO-Zero-4.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n该算法的每个循环包含4个步骤：\n\n- **选择：** 从根节点往下走，每次都选一个“最值得看的子节点”（具体规则稍后说），直到来到一个“存在未扩展的子节点”的节点，如图中的 \\\\( 3/3 \\\\) 节点。什么叫做“存在未扩展的子节点”，其实就是指这个局面存在未走过的后续着法。\n- **扩展：** 我们给这个节点加上一个 \\\\( 0/0 \\\\) 子节点，对应之前所说的“未扩展的子节点”，就是还没有试过的一个着法。\n- **仿真：** 从上面这个没有试过的着法开始，用快速走子策略（Rollout policy）走到底，得到一个胜负结果。按照普遍的观点，快速走子策略适合选择一个棋力很弱但走子很快的策略。因为如果这个策略走得慢（比如用 AlphaGo 的策略网络走棋），虽然棋力会更强，结果会更准确，但由于耗时多了，在单位时间内的模拟次数就少了，所以不一定会棋力更强，有可能会更弱。这也是为什么我们一般只模拟一次，因为如果模拟多次，虽然更准确，但更慢。\n- **回溯：** 把模拟的结果加到它的所有父节点上。例如第三步模拟的结果是  \\\\(0/1\\\\) （代表黑棋失败），那么就把这个节点的所有父节点加上  \\\\(0/1\\\\) 。\n\n<br/>\n### 2.4 上限置信区间算法 UCT\n\n怎么选择节点？和从前一样：如果轮到黑棋走，就选对于黑棋有利的；如果轮到白棋走，就选对于黑棋最不利的。但不能太贪心，不能每次都只选择“最有利的/最不利的”，因为这会意味着搜索树的广度不够，容易忽略实际更好的选择。\n\n为了在最大胜率和新节点探索上保持平衡，**UCT（Upper Confidence Bound，上限置信区间算法）**被引入。所谓置信区间，就是概率计算结果的可信度。打个比方，如果掷了3次硬币，都是正面朝上，我们就认为掷硬币正面朝上概率是100%，那肯定是错误的，因为我们的样本太少了。所以UCT就是用来修正这个样本太少的问题。\n\n**具体公式如下：**\n\n$$\\text{score = }\\ \\frac{w\\_i}{n\\_i}+c\\sqrt{\\frac{\\ln N\\_i}{n\\_i}}$$\n\n- \\\\(w\\_i\\\\)  是 \\\\(i\\\\) 节点的胜利次数\n-  \\\\(n\\_i\\\\) 是i节点的模拟次数\n-   \\\\(N\\_i\\\\) 是所有模拟次数 \n-   \\\\(c\\\\) 是探索常数，理论值为 \\\\(\\sqrt{2}\\\\) ，可根据经验调整，\\\\(c\\\\) 越大就越偏向于广度搜索，\\\\(c\\\\) 越小就越偏向于深度搜索\n\n我们看例子说明这是什么意思，就看之前的图吧。\n<div align=center>\n![](/img/2018-08-08-AlphaGO-Zero-5.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n假设根节点是轮到黑棋走。那么我们首先需要在  \\\\(7/10、5/8、0/3\\\\)  之间选择 (即第二排)：\n\n- 其中 \\\\( 7/10 \\\\) 对应的分数为  \\\\(7/10 + C \\cdot \\sqrt{\\frac{\\log(21)}{10}}  \\approx 0.7 + 0.55 C\\\\) 。\n- 其中 \\\\( 5/8 \\\\) 对应的分数为  \\\\(5/8 + C \\cdot \\sqrt{\\frac{\\log(21)}{8}}  \\approx 0.625 + 0.62 C\\\\) 。\n- 其中 \\\\( 0/3 \\\\) 对应的分数为  \\\\(0/3 + C \\cdot \\sqrt{\\frac{\\log(21)}{3}}  \\approx 0 + 1.00 C\\\\) 。\n- 可以注意到， \\\\(C\\\\) 越大，就会越照顾访问次数相对较少的子节点。\n\n如果  \\\\(C\\\\)  比较小，我们将会选择  \\\\(7/10\\\\) ，接着就要在  \\\\(2/4\\\\)  和 \\\\( 5/6 \\\\) 间选择。注意，由于现在是白棋走，需要把胜率估计倒过来：\n\n- 其中  \\\\(2/4\\\\)  对应的分数为  \\\\((1-2/4) + C \\cdot \\sqrt{\\frac{\\log(10)}{4}}  \\approx 0.5 + 0.76 C\\\\) 。\n- 其中  \\\\(5/6\\\\)  对应的分数为  \\\\((1-5/6) + C \\cdot \\sqrt{\\frac{\\log(10)}{6}}  \\approx 0.17 + 0.62 C\\\\) 。\n\n那么我们下一步肯定应该选 2/4。所以说这张图是错误的，因为制图的人并没有注意到要把胜率倒过来。\n\n\n<br/>\n## 3. 深度强化学习\n<br/>\n### 3.1 神经网络与MCTS的结合\n\n\n常见的MCTS分为4个步骤：选择，扩展，模拟和反向传播。\n\n神经网络用来指导MCTS进行判断，主要目的是用神经网络的输出代替四个步骤中的扩展和模拟这两步。\n\n神经网络的**输出是落子概率和局面评估**。从根节点开始，选择到叶节点，然后判断是否代表这结束，如果没有结束，则根据神经网络输出的评分进行更新，同时根据神经网络给出的落子策略进行扩展。如果结束，则根据玩家的胜负进行更新。\n\n**但是对于传统的MCTS，我们没有好的策略，所以只能大规模的搜索。**在到达叶节点所代表的局面的时候，我们需要使用**随机策略进行多次模拟，一直模拟到对局结束**才能得到局面的评估。这需要消耗大量的计算资源和时间。所以引入神经网络来代替模拟步骤。\n\n所以总的来说，落子的选择整体是根据MCTS来的。神经网络的作用是帮助缩短MCTS所需要的时间。\n\n>没有 MCTS 相当于职业棋手只凭棋感不做计算走快棋。**神经网络提供几个候选的走法，MCTS 再算一算到底哪个点更好。**\n\n<br/>\n### 3.2 神经网络架构\n\n由残差模块构成的 CNN，输入为 \\\\(19\\times 19\\times 17\\\\)\n\n17 是 17 个特征，使用了既往 8 个回合的 16 个特征以及一个当前玩家信息特征  \\\\((8 \\times 2 + 1 = 17)\\\\) : \n\n$$s\\_t = [X\\_t , Y\\_t , X\\_{t−1}, Y\\_{t−1}, ..., X\\_{t−7}, Y\\_{t−7} , C]$$\n\n**其中  \\\\(X\\_t\\\\)  内包含的是当前棋手的数据：**\n\n加入当前棋手执黑棋，那么此时棋盘上所有黑棋对应的位置取值1，白棋和空白位置取值0。类似的 \\\\( Y\\_t \\\\) 反映的就是白棋信息，当前棋盘上所有白棋的位置取值1，黑棋和空白处取值0。\n\n**\\\\(C\\\\) 内的所有 \\\\(（19\\times 19）\\\\) 个数据要么都是1，要么都是0，**如果此时是黑棋要落子则取1，白棋落子则取0。\n\n网络的共同部分多数是用 \\\\(3\\times 3\\\\) 的卷积核（stride = 1)，256个特征数，后接 BatchNormalization 和 Relu 单元。每一个残差单元包括 (参见下图)：\n\n- **策略端**：输出特征数为 \\\\(19\\times 19+1\\\\)，分别代表在棋盘上所有可能位置落子的可能性以及 Pass 的可能性。\n- **价值端**：全连接一个256个特征的隐藏层，最后以tanh的激活方式输出 \\\\([-1,1]\\\\) 之间的值。 \n\n>网络的前20层左右，是常见的神经网络结构。然后跟着是两个“头”，一个头取走了前20层的输出，然后产生了下一步的落子概率，另一个头基于同样的数据，输出当前局面的获胜概率。\n\n<br/>\n<div align=center>\n![](/img/2018-08-08-AlphaGO-Zero-101.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n<div><br/></div>\n\n**训练数据：**自我对弈产生大量的  \\\\(( s,\\pi,z )\\\\) 数据对，通过 Mini-batch 采样。\n\n**损失函数：**\n\n$$ l=(z-v)^{2}-\\pi^{T}log(p)+c||\\theta||^{2} $$\n\n\n\n- 第一项：通过最小二乘最小化获胜概率误差      \n- 第二项：通过交叉熵最大化先验走子概率与提升后走子概率一致性     \n- 第三项：L2范数权值衰减防止过拟合。\n\n<div> </div>\n\n<br/>\n### 3.3 过程细节\n\n为了在 self play 每一步得到，MCTS 需要完成1600次搜索。搜索树中每一节点 \\\\(s\\\\) 针对合法操作保存以下数据结构 \n\n$$\\\\{N(s,a),W(s,a),Q(s,a),P(s,a)  \\\\}$$\n\n\n-  \\\\(s\\ \\\\)  树的每一个节点代表了一种棋盘布局\n-  \\\\(a\\ \\\\)  每一个边代表了在一种布局 \\\\(s\\\\) 下的一种落子方式\n- \\\\(N(s,a)\\ \\\\) 记录边的访问次数  \n- \\\\(W(s,a)\\ \\\\)  合计行动价值  \n- \\\\(Q(s,a)\\ \\\\) 平均行动价值 \n-  \\\\(P(s,a)\\ \\\\)  选择该条边的先验概率\n\n多次模拟过程会在独立线程并行运行。搜索算法在 \\\\(a，b，c\\\\) 三步迭代多次后，根据搜索结果选择落子 \\\\(d\\\\) 。\n\n\n<br/>\n<div align=center>\n![](/img/2018-08-08-AlphaGO-Zero-6.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n- **a** 每次模拟选择的分支，有最大 \\\\(Q+U\\\\) , 其中 \\\\(Q\\\\) 是动作价值， \\\\(U\\\\) 是上限置信， \\\\(U\\\\) 依赖于一个存储在分支上的优先概率 \\\\(P\\\\) 和该分支的访问次数 \\\\(N\\\\) （每访问一次 \\\\(N+1\\\\)）\n- **b** 扩展叶节点，神经网络 \\\\((P(s, .), V(s)) = f\\_θ(s)\\\\) 评估 \\\\(s\\\\) ; 将向量 \\\\(P\\\\) 的值被存储在 \\\\(s\\\\) 的扩展边上\n- **c** 根据 \\\\(V\\\\) 更新动作价值（action-value) \\\\(Q\\\\)，反映所有该动作的子树的平均值\n- **d** 一旦搜索结束，搜索概率 \\\\(\\pi\\\\) 被返回，与  \\\\(Ν^{(1/τ)}\\\\)  成正比， \\\\(N\\\\) 是每个分支的访问次数，而 \\\\(τ\\\\) 是一个参数控制着温度（temperature）\n\n\n\n\n\n这里先知晓有这样的神经网络结构  \\\\(（p,v）=f\\_{\\theta}（s）\\\\)  （初始状态参数  \\\\(\\theta\\\\)  随机赋值）\n\n在自我对弈的每一步，根据深度神经网络计算出落子概率（先验概率  \\\\(p\\\\)  ），如对状态  \\\\(s\\_{1}\\\\)  得到  \\\\((p\\_{1},v\\_{1}) \\\\) ；然后通过 MCTS（蒙特卡罗搜索树算法）进行 policy\nimprovement，MCTS 搜索的输出是当前状态  \\\\(i\\\\)  下不同位置落子的概率 \\\\( \\pi\\_{i} \\\\) ，该落子概率会优于该状态下先验概率 \\\\(p\\_{i}\\\\)  ，然后基于 \\\\( \\pi\\_{i} \\\\) 完成当前步骤落子，之后每步均如此过程直到完成当前对局得到最终结果 \\\\(z（ z\\in[-1,1] ）\\\\) 。\n<div align=center>\n![](/img/2018-08-08-AlphaGO-Zero-7.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n\n神经网络通过使用 MCTS 搜索的自我对弈强化学习来进行训练。一开始神经网络的参数被随机设置称  \\\\(\\theta\\_0 \\\\) ，在随后的每一次迭代中 \\\\( i\\geq1 \\\\) ,会通过自我对弈产生许多完整的棋局，在其中每一个完整棋局的一个时间步 \\\\( T \\\\) 时，都会利用上一个神经网络的参数来产生搜索策略 \\\\( \\pi\\_t = \\alpha\\_{i-1}(s\\_t) \\\\) ，并且用这个策略的采样产生实际自我对弈时的行为。\n\n发生下列任意情况之一，游戏终止于时间 \\\\( T \\\\) ：\n\n- 双方都 Pass\n- 搜索 value 降低至一个被 resignation（割舍？）的阈值\n- 游戏对弈达到设定的最大步数\n\n\n\n游戏结束后，会给出一个最终奖励  \\\\(r\\_T \\in \\\\{ -1, +1 \\\\}\\\\) , 每一个时间步 T 的数据以 \\\\( (s\\_t, \\pi\\_t, z\\_t) \\\\) 的形式保存，其中  \\\\(z\\_t = \\pm r\\_T\\\\)  是从  \\\\(T\\\\)  时刻玩家的立场得到的胜利者的奖励（是不是可以理解成：  \\\\(T \\\\) 时刻不管是白方还是黑方，只要最终赢得棋局， \\\\( z\\_t = 1 \\\\) 即成立？）。\n\n>不过需要在一个完整的对局结束后才能确定这一局中每一个 \\\\( (s, \\pi, z) \\\\) 中的  \\\\(z\\\\)  ，如果最后的胜者是 \\\\( s \\\\) 局面下的当前 player，则 \\\\( z=1 \\\\) ,如果最后的败者是 \\\\( s\\\\)  局面下的当前 player，则  \\\\(z=-1\\\\)  ,如果最后打平，则  \\\\(z=0\\\\) \n\n自我对弈过程中的最后几次迭代过程中产生的数据 \\\\( (s,\\pi,z) \\\\) 将会以均等的概率被选中来训练神经网络。\n\nAlphaGo Zero 里面的神经网络实际上是把 AlphaGo 里面的 Policy Network 和 Value Network 糅合在一起了，所以这个神经网络也有两个目标，神经网络的训练目标就是要尽可能的缩小两方面的差距：\n\n- 让网络输出的落子概率向量 \\\\(p\\\\) 和 MCTS 搜索输出  \\\\(\\pi\\\\)  越接近越好\n- 让网络预测的当前棋手的最终结果 \\\\(v\\\\) 和最终游戏赢家 \\\\(z\\\\) 越接近越好\n\n神经网络的损失函数由下式确定：  \n\n<font size=\"+1\">  $$l = (z-v)^{2} - {\\pi}^\\top logP + c ||\\theta||^2 $$\n  </font>\n \n- \\\\(c \\\\) 是控制参数 L2 正则项的一个系数。\n\n\n网络训练得到的新参数会被用来知道下一轮迭代中自我对弈时的 MCTS 搜索。\n\nAlphaGo Zero 每1000步会将一个神经网络存档，并且把这个存档和历史最优版本比较，如果胜率超过历史最优的55%，这个版本将会被更新为历史最优。并且在生成数据时，只使用历史最优的神经网络的 self－play 数据作为深度网络的训练数据。这样可以增加算法的优化速度。\n\n<br/>\n\n## 4. 搜索阶段算法\n\n<br/>\n<div align=center>\n![](/img/2018-08-08-AlphaGO-Zero-8.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n<br/>\n### a 选择 Select\n\n每一次模拟的第一个阶段起自搜索树的根节点  \\\\(s\\_0\\\\)  ，在第 L 个时间步结束于搜索树的叶节点  \\\\(s\\_L\\\\)  。对于其中的任意时间  \\\\(t<L\\\\)  ，根据搜索树内的统计数据来决定选择哪一个模拟行为  \n\n$$a\\_t = \\underset{a}{argmax}(Q(s\\_t, a) + U(s\\_t, a))$$\n\n其中： \n$$U(s,a) = c\\_{puct}P(s,a)\\frac{\\sqrt{\\sum\\_{b}{N(s,b)}}}{1+N(s,a)}$$ \n\n- \\\\(c\\_{puct}\\\\) 是决定探索程度的一个系数\n\n>this search control strategy initially prefers actions with high prior probability and low visit count, but asympotically prefers actions with high action value.\n\n<br/>\n### b 扩展和评估 Expand & Evaluate\n\n\n叶节点  \\\\(s\\_L\\\\)  将会等待来自神经网络的评估  \\\\((d\\_i (p), v) = f\\_\\theta (d\\_i (s\\_L )) \\\\)，其中  \\\\(d\\_i \\\\) 是一个 dihedral reflection 或 rotation，  \\\\(i\\in[1,2,3,...,8] \\\\) 。\n\n其中通过一个1至8的随机数来表示双方向镜面和旋转（因为围棋在棋盘旋转和镜像之后的胜率估算情况是一样的，如下图所示）\n<div align=center>\n![](/img/2018-08-08-AlphaGO-Zero-9.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n这些等待评估的状态会被送入一个队列，在神经网络评估队列里的状态时（使用 mini\\_batch\\_size=8），搜索将被暂时锁定。当神经网络得到结果后，该叶节点会被展开，同时每一条可能的边  \\\\((s\\_L,a)\\\\)  会以下面的数据进行初始化：  \n\n$$\\\\{ N(s\\_L,a)=0,W(s\\_L,a)=0,Q(s\\_L,a)=0,P(s\\_L,a)=P\\_a \\\\}$$ \n\n 同时来自神经网络的对该叶节点的价值估计也会影响路径中每一个节点的统计数据  \\\\(W\\\\)（见下），随后进行回溯过程。\n\n>AlphaGo Zero 会根据前面的落子规则不断的落子，这就相当于棋手在脑海中进行推演。但是围棋的搜索空间即使对于计算机来说也是太大，AlphaGo zero 只会推演（仿真）到一定步数就停止了。假设最后的布局是 \\\\(s'\\\\) , 那么 AlphaGo Zero 会调用深度神经网络来预测这局推演结束时自己的胜率 \\\\(v(s') \\\\) 。这样的推演过程会进行多次。\n\n<br/>\n### c 回溯 Backup\n\n等一次推演结束后，AlphaGo zero 会根据推演的结果更新自己的知识，也就是值函数 \\\\(Q(s,u)\\\\) \n\n对于 \\\\( t \\leq L \\\\) ,每一个边的统计结果将被更新。\n\n $$ N(s\\_t,a\\_t) = N(s\\_t,a\\_t)+1$$  \n \n $$ W(s\\_t,a\\_t) = W(s\\_t,a\\_t)+v $$      \n \n $$Q(s\\_t,a\\_t) = \\frac{W(s\\_t,a\\_t)}{N(s\\_t,a\\_t)}$$\n\n<br/>\n\n$$N(s,a) :\\  记录边的访问次数  $$\n\n$$W(s,a) :\\   合计行动价值  $$\n\n$$Q(s,a) :\\ 平均行动价值 $$\n\n<br/>\n### d 产生实际行为 Play\n\n路径中所有节点统计数据得到更新后（搜索结束后）， AlphaGo Zero 在根节点  \\\\(s\\\\)  处选择 \\\\(a\\\\) 操作进行落子，根据最新的统计数据来产生一个实际的行为 \\\\(\\pi\\_{a}\\\\) ，与访问次数成幂指数比例：  \n$$\\pi(a|s)=\\frac{N(s,a)^{1/\\tau}}{\\sum\\_{b}^{}{N(s,b)^{1/\\tau}}}$$  \n\n\\\\(\\pi\\_{a}\\ (\\pi(a|s))\\\\)  是落子到位置 \\\\(a\\\\) 的概率\n\n\\\\(\\tau \\\\) 为温度参数，控制探索的程度， \\\\( \\tau\\\\)  越大，不同走法间差异变小，探索比例增大，反之，则更多选择当前最优操作。\n\n在随后的时间步 (time\\_steps) 中，这个搜索树将会继续使用，对应于实际所采取的行为的子节点将变成根节点，该子节点下的子树的统计数据将会被保留，而这颗树的其余部分将会丢弃 (discarded)。 \n\n另外，如果该子树的根节点和最佳价值子节点的价值低于某一个阈值 \\\\( v\\_{resign}\\\\)，AlphaZero 将放弃搜索某子树。\n\n<br/>\n## 参考\n\n1. [AlphaGo Zero去掉蒙特卡洛搜索树会怎么样?](https://www.zhihu.com/question/263583260)\n2. [蒙特卡洛树搜索中神经网络是如何指导蒙特卡洛树搜索进行判断的](https://www.zhihu.com/question/289626696)\n3. [AlphaGo Zero 简明工作原理](https://zhuanlan.zhihu.com/p/32952677)\n4. [初步认识AlphaGo Zero原理](https://zhuanlan.zhihu.com/p/35103060)\n5. [AlphaGo Zero论文笔记](https://zhuanlan.zhihu.com/p/30707897)\n6. [AlphaGo Zero解读](https://zhuanlan.zhihu.com/p/30352003)\n7. [Code:AlphaZero_Gomoku](https://github.com/junxiaosong/AlphaZero_Gomoku)\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n","tags":["强化学习"],"categories":["强化学习"]},{"title":"TensorFlow 并行计算","url":"/2018/08/07/tensorflow-MultiGPU/","content":"\n## 多 GPU\n### 简述\n\n如果有两块卡，但是代码里不设置的话，默认把变量都放到 `device('/gpu:0')`，所以只有 gpu 0 在计算。\n\n\ntensorflow 默认是占满显存的，然后等到程序需要用的时候直接拿来用，这个是 tensorflow 设计的一个机制，对于这一机制大家褒贬不一\n\n\n### 限制 GPU 资源\n\n#### 动态申请显存\n\n```python\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = tf.Session(config=config)\n```\n\n#### 限制GPU使用率\n\n```python\ngpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\nconfig=tf.ConfigProto(gpu_options=gpu_options)\nsession = tf.Session(config=config)\n```\n\n其中0.333是你自己设置的想用百分之多少的显存。\n\n### 例一\n```python\n# Multi GPU computing\n# GPU:0 computes A^n\nwith tf.device('/gpu:0'):\n    #compute A^n and store result in c2\n    a = tf.constant(A)\n    c2.append(matpow(a, n))\n\n#GPU:1 computes B^n\nwith tf.device('/gpu:1'):\n    #compute B^n and store result in c2\n    b = tf.constant(B)\n    c2.append(matpow(b, n))\n\nwith tf.device('/cpu:0'):\n  sum = tf.add_n(c2) #Addition of all elements in c2, i.e. A^n + B^n\n\nt1_2 = datetime.datetime.now()\nwith tf.Session(config=tf.ConfigProto(log_device_placement=log_device_placement)) as sess:\n    # Runs the op.\n    sess.run(sum)\nt2_2 = datetime.datetime.now()\n```\n\n### 例二\n\n```python\n# Place all ops on CPU by default\nwith tf.device('/cpu:0'):\n    tower_grads = []\n    reuse_vars = False\n\n    # tf Graph input\n    X = tf.placeholder(tf.float32, [None, num_input])\n    Y = tf.placeholder(tf.float32, [None, num_classes])\n\n    # Loop over all GPUs and construct their own computation graph\n    for i in range(num_gpus):\n        with tf.device(assign_to_device('/gpu:{}'.format(i), ps_device='/cpu:0')):\n\n            # Split data between GPUs\n            _x = X[i * batch_size: (i+1) * batch_size]\n            _y = Y[i * batch_size: (i+1) * batch_size]\n\n            # Because Dropout have different behavior at training and prediction time, we\n            # need to create 2 distinct computation graphs that share the same weights.\n\n            # Create a graph for training\n            logits_train = conv_net(_x, num_classes, dropout,\n                                    reuse=reuse_vars, is_training=True)\n            # Create another graph for testing that reuse the same weights\n            logits_test = conv_net(_x, num_classes, dropout,\n                                   reuse=True, is_training=False)\n\n            # Define loss and optimizer (with train logits, for dropout to take effect)\n            loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n                logits=logits_train, labels=_y))\n            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n            grads = optimizer.compute_gradients(loss_op)\n\n            # Only first GPU compute accuracy\n            if i == 0:\n                # Evaluate model (with test logits, for dropout to be disabled)\n                correct_pred = tf.equal(tf.argmax(logits_test, 1), tf.argmax(_y, 1))\n                accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n            reuse_vars = True\n            tower_grads.append(grads)\n\n    tower_grads = average_gradients(tower_grads)\n    train_op = optimizer.apply_gradients(tower_grads)\n\n    # Initializing the variables\n    init = tf.global_variables_initializer()\n\n    # Launch the graph\n    with tf.Session() as sess:\n        sess.run(init)\n        step = 1\n        # Keep training until reach max iterations\n        for step in range(1, num_steps + 1):\n            # Get a batch for each GPU\n            batch_x, batch_y = mnist.train.next_batch(batch_size * num_gpus)\n            # Run optimization op (backprop)\n            ts = time.time()\n            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n            te = time.time() - ts\n            if step % display_step == 0 or step == 1:\n                # Calculate batch loss and accuracy\n                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n                                                                     Y: batch_y})\n                print(\"Step \" + str(step) + \": Minibatch Loss= \" + \\\n                      \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n                      \"{:.3f}\".format(acc) + \", %i Examples/sec\" % int(len(batch_x)/te))\n            step += 1\n        print(\"Optimization Finished!\")\n\n        # Calculate accuracy for 1000 mnist test images\n        print(\"Testing Accuracy:\", \\\n            np.mean([sess.run(accuracy, feed_dict={X: mnist.test.images[i:i+batch_size],\n            Y: mnist.test.labels[i:i+batch_size]}) for i in range(0, len(mnist.test.images), batch_size)]))\n```\n\n\n\n1. [multigpu_basics (code)](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/6_MultiGPU/multigpu_basics.ipynb)     \n2. [Multi-GPU Training Example (code)](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/6_MultiGPU/multigpu_cnn.ipynb)\n3. [tensorflow 发现两块卡的显存都占用，但是实际上只有一块卡在运算](https://www.zhihu.com/question/56291568)\n\n\n\n## 多线程\n\n在进行 `tf.ConfigProto()` 初始化时，我们也可以通过设置 `intra_op_parallelism_threads` 参数和 `inter_op_parallelism_threads` 参数，来控制每个操作符op并行计算的线程个数。\n\n**二者的区别在于:**\n\n `intra_op_parallelism_threads` 控制运算符op内部的并行   \n \n当运算符op为单一运算符，并且内部可以实现并行时，如矩阵乘法，`reduce_sum`之类的操作，可以通过设置 `intra_op_parallelism_threads` 参数来并行, intra 代表内部。   \n\n   \n`inter_op_parallelism_threads` 控制多个运算符op之间的并行计算\n\n当有多个运算符 op，并且他们之间比较独立，运算符和运算符之间没有直接的路径Path相连。Tensorflow 会尝试并行地计算他们，使用由 `inter_op_parallelism_threads` 参数来控制数量的一个线程池。\n\n以上两个参数如果设置为0代表让系统设置合适的数值\n\n```python\nconfig = tf.ConfigProto(device_count={\"CPU\": 4}, # limit to num_cpu_core CPU usage\n                inter_op_parallelism_threads = 1, \n                intra_op_parallelism_threads = 4,\n                log_device_placement=True)\nwith tf.Session(config = config) as sess:\n  # To Do\n```\n\n实例比较，线程数为2和4，平均每个batch的运行时间：\n\n当参数为intra\\_op\\_parallelism\\_threads = 2时, 每个step的平均运行时间从610ms降低到380ms。\n当参数为intra\\_op\\_parallelism\\_threads = 4时, 每个step的平均运行时间从610ms降低到230ms。\n\n总结，在固定CPUcore的资源限制下，通过合理设置线程thread个数可以明显提升tensorflow程序运行速度。\n","tags":["TensorFlow"],"categories":["TensorFlow"]},{"title":"强化学习（一）Deep Q-Network","url":"/2018/08/05/RL/","content":"\n## \t1. 前言\n\n>虽然将深度学习和增强学习结合的想法在几年前就有人尝试，但真正成功的开端就是DeepMind在NIPS 2013上发表的 **[Playing Atari with Deep Reinforcement Learning](https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1312.5602)** 一文，在该文中第一次提出Deep Reinforcement Learning 这个名称，并且提出DQN（Deep Q-Network）算法，实现从纯图像输入完全通过学习来玩Atari游戏的成果。之后DeepMind在Nature上发表了改进版的DQN文章**Human-level Control through Deep Reinforcement Learning**，引起了广泛的关注，Deep Reinfocement Learning 从此成为深度学习领域的前沿研究方向。\n\n\n**智能体 Agent** 来表示一个具备行为能力的物体，比如机器人，无人车，人等等。\n\n那么增强学习考虑的问题就是智能体Agent和 **环境 Environment** 之间交互的任务。\n\n>比如一个机械臂要拿起一个手机，那么机械臂周围的物体包括手机就是环境，机械臂通过外部的比如摄像头来感知环境，然后机械臂需要输出动作来实现拿起手机这个任务。再举玩游戏的例子，比如我们玩极品飞车游戏，我们只看到屏幕，这就是环境，然后我们输出动作（键盘操作）来控制车的运动。\n\n那么，不管是什么样的任务，   \n都包含了一系列的：\n\n- **动作 Action**           \n- **观察 Observation**     \n- **反馈值 Reward**   \n   \n所谓的Reward就是Agent执行了动作与环境进行交互后，环境会发生变化，变化的好与坏就用Reward来表示。\n\n>接下来这里用了Observation观察一词而不是环境那是因为Agent不一定能得到环境的所有信息，比如机械臂上的摄像头就只能得到某个特定角度的画面。因此，只能用Observation来表示Agent获取的感知信息。\n\n\n只能用 Observation 来表示 Agent 获取的感知信息\n\n每个时间片，Agent 都是根据当前的观察来确定下一步的动作。观察 Observation 的集合就作为Agent的所处的 **状态 State**，因此，**状态 State** 和 **动作 Action** 存在映射关系，也就是一个 state 可以对应一个 action，或者对应不同动作的概率（常常用概率来表示，概率最高的就是最值得执行的动作）。状态与动作的关系其实就是输入与输出的关系，而状态 State 到动作 Action 的过程就称之为一个**策略 Policy**，一般用 \\\\(\\pi\\\\) 表示，也就是需要找到以下关系：  \n  \n$$a=\\pi(s)$$\n\n或者\n\n$$\\pi(a|s)$$\n\n其中 **a 是 action，s 是 state**。     \n第一种是一一对应的表示，第二种是概率的表示。\n\n>增强学习的任务就是找到一个最优的策略Policy从而使Reward最多\n\n我们一开始并不知道最优的策略是什么，因此往往从随机的策略开始，使用随机的策略进行试验，就可以得到一系列的状态,动作和反馈：\n\n$$\\\\{s\\_1,a\\_1,r\\_1,s\\_2,a\\_2,r\\_2,...s\\_t,a\\_t,r\\_t\\\\}$$\n\n这就是一系列的 **样本 Sample**。增强学习的算法就是需要根据这些样本来改进 Policy，从而使得得到的样本中的 Reward 更好。由于这种让 Reward 越来越好的特性，所以这种算法就叫做增强学习Reinforcement Learning。\n\n<br/>\n## 2. 马尔科夫决策过程\n\n>MDP只需要用一句话就可以说明白，就是 **“未来只取决于当前”**，专业点说就是下一步的状态只取决于当前的状态，与过去的状态没有关系。\n\n**一个状态 \\\\(S\\_t\\\\) 是Markov当且仅当：**\n\n<font size=\"+1\">   $$P(s\\_{t+1}|s\\_t)=P(s\\_{t+1}|s\\_t,s\\_{t-1},...s\\_1,s\\_0)$$  </font>\n\n\n\nP为概率。简单的说就是下一个状态仅取决于当前的状态和当前的动作。\n增强学习的问题都可以模型化为MDP的问题\n\n**因此 MDP 可以表示为一个元组** \\\\((S, A, P\\_{sa}, R)\\\\) ：\n\n-  \\\\(S\\\\) ：所有可能状态的集合， \\\\(s \\in S\\\\)，\\\\(s\\\\) 表示某个特定状态\n-  \\\\(A\\\\) ：针对每个状态，我们都要做出动作，这些动作的集合就是 \\\\(A\\\\)； \\\\(a \\in A\\\\)，有限动作 action 集合， \\\\(a\\\\) 表示某个特定动作\n-  \\\\(P\\_{sa}\\\\) ：状态转换分布（statetransition distribution），如果我们在状态 \\\\(s\\\\) 中采取了动作 \\\\(a\\\\) ，系统会转移到一个新的状态，状态转换分布描述了转移到哪个状态的概率分布。\n-  \\\\(R\\\\) ：回馈函数（rewardfunction），增强学习的核心概念，描述了动作能够产生的回报。比如 \\\\(R\\_π(s,a)\\\\) 描述了在状态 \\\\(s\\\\) 下采用策略 \\\\(\\pi\\\\) 所对应的动作 \\\\(a\\\\) 的回报，也叫做立即回报，回馈函数可以有不同的表达形式。\n-   \\\\(\\pi(s)\\rightarrow a\\\\)： 策略 policy，根据当前 state 来产生 action，可表现为  \\\\(a=\\pi(s) \\\\) 或 \\\\( \\pi(a|s) = P[a|s]\\\\)，后者表示某种状态下执行某个动作的概率\n\n>一个基本的 MDP 可以用 \\\\((S,A,P)\\\\) 来表示， \\\\(S \\\\) 表示状态， \\\\(A\\\\)  表示动作， \\\\(P \\\\) 表示状态转移概率，也就是根据当前的状态 \\\\(s\\_t\\\\) 和 \\\\(a\\_t\\\\) 转移到 \\\\(s\\_{t+1}\\\\) 的概率。\n\n如果我们知道了转移概率 P，也就是称为我们获得了 **模型 Model**，有了模型，未来就可以求解，那么获取最优的动作也就有可能，这种通过模型来获取最优动作的方法也就称为 Model-based 的方法。但是现实情况下，很多问题是很难得到准确的模型的，因此就有 Model-free 的方法来寻找最优的动作。\n\n<br/>\n## 3. 价值函数\n\n\n>既然一个状态对应一个动作，或者动作的概率，而有了动作，下一个状态也就确定了。这就意味着每个状态可以用一个确定的值来进行描述。可以由此判断一个状态是好的状态还是不好的状态。\n\n但是在选取最优策略的过程中，我们只看立即回报并不能判定哪种策略更优，我们希望的是在采取了策略 \\\\(\\pi\\\\) 以后，可以使得整个状态序列的折扣回馈最大。\n\n状态的好坏其实等价于对未来回报的期望，**回报 Return** 来表示某个时刻 t 的状态将具备的回报：\n\n<font size=\"+1\">    $$G\\_t = R\\_{t+1} + \\lambda R\\_{t+2} + ... = \\sum\\_{k=0}^\\infty\\lambda^kR\\_{t+k+1}$$ </font>\n\n\n- R 是 Reward 反馈\n- \\\\(λ\\\\) 是 discount factor 折扣因子，一般小于 1，就是说一般当下的反馈是比较重要的，时间越久，影响越小。\n\n>其中 \\\\(λ\\\\) 被称为折扣因子，在经济学上的解释叫做无风险折现率（risk-freeinterest rate），意思是马上得到的钱（回馈）比未来得到钱更有价值。\n\n**以上概念就组成了增强学习的完整描述：**找到一种策略，使得我们能够根据状态 \\\\(s\\_0, s\\_1, s\\_2, …\\\\) 采取策略中对应的动作 \\\\(a\\_0, a1, a2…，\\\\) 并使 \\\\(G\\_t\\\\) 的期望值最大化\n\n\n引出价值函数，对于获取最优的策略Policy这个目标，我们就会有两种方法：\n\n- 直接优化策略 \\\\(\\pi(a|s)\\\\) 或者 \\\\(a = \\pi(s)\\\\) 使得回报更高\n- 通过估计 value function 来间接获得优化的策略。道理很简单，既然我知道每一种状态的优劣，那么我就知道我应该怎么选择了，而这种选择就是我们想要的策略。\n\n>但是现在为了理解DQN，我们将只关注第二种做法，就是估计value function的做法，因为DQN就是基于value function的算法。\n\n1. **状态价值函数** \\\\(V\\\\) ：从状态 \\\\(x\\_0\\\\)  开始, 所有的动作 \\\\(a\\\\) ，都是执行某个策略 \\\\(π\\\\) 的结果，最后求每个动作带来累积奖赏\n2. **动作价值函数** \\\\(Q\\\\) ：从状态 \\\\(x\\_0\\\\)  开始，先执行动作 \\\\(a\\_0\\\\) ， 然后再执行某个策略 \\\\(π\\\\) ，再求相应的积累奖赏\n\n<br/>\n### 3.1 State-Value function 状态价值函数\n\n那么实际上除非整个过程结束，否则显然我们无法获取所有的 reward 来计算出每个状态的Return，因此，再引入一个 **概念价值函数 Value Function**，用 **value function**  \\\\(v(s)\\\\)  来表示一个状态未来的潜在价值。\n\n从定义上看，**value function** 就是回报的期望：\n\n$$v(s)  = \\mathbb E[G\\_t|S\\_t = s]$$\n\n\n$$V^{\\pi}(s) =\\mathbb E\\_{\\pi}[ R(s\\_0, a\\_0) + γR(s\\_1, a\\_1)+ γ^2R(s\\_2, a\\_2) + … | s\\_0= s ]$$\n\n这个函数也被称为**状态价值函数(statevalue function)**，记为 \\\\(V\\_{\\pi}(s)\\\\)。 因为初始状态 \\\\(s\\\\) 和策略 \\\\(\\pi\\\\) 是我们给定的，动作 \\\\(a = \\pi(s)\\\\) 。\n\n\n<br/>\n### 3.2 Action-Value function 动作价值函数\n\n**我们更关心在某个状态下的不同动作的价值**。显然。如果知道了每个动作的价值，那么就可以选择价值最大的一个动作去执行了。\n\n这就是 **Action-Value function**  \\\\(Q^\\pi(s,a)\\\\) 。那么同样的道理，也是使用 reward 来表示，只是这里的 reward 和之前的 reward 不一样：\n\n**这里是执行完动作 action 之后得到的 reward，之前 state 对应的 reward 则是多种动作对应的 reward 的期望值**。显然，动作之后的 reward 更容易理解。\n\n那么，有了上面的定义，动作价值函数就为如下表示：\n\n$$ Q^{\\pi}(s,a) = \\mathbb E[R\\_{t+1}+\\lambda R\\_{t+2} + \\lambda ^2R\\_{t+3} + ...|S\\_t = s,A\\_t=a]$$\n\n$$\n\\begin{align}\nQ^\\pi(s,a) & =  \\mathbb E[r\\_{t+1} + \\lambda r\\_{t+2} + \\lambda^2r\\_{t+3} + ... |s,a] \\\\\\\\\n& = \\mathbb E\\_{s^\\prime}[r+\\lambda Q^\\pi(s^\\prime,a^\\prime)|s,a]\n\\end{align}\n$$\n\n这里要说明的是动作价值函数的定义，加了 \\\\(\\pi\\\\) ,也就是说是**在策略下的动作价值**。因为对于每一个动作而已，都需要由策略根据当前的状态生成，因此必须有策略的支撑。而前面的价值函数则不一定依赖于策略。当然，如果定义 \\\\(v^\\pi(s)\\\\) 则表示在策略 \\\\(\\pi\\\\) 下的价值。\n\n那么事实上我们会更多的使用动作价值函数而不是价值函数，因为动作价值函数更直观，更方便应用于算法当中。\n\n<br/>\n## 4. Bellman 方程\n\n>在上文我们介绍了 Value Function 价值函数，所以为了解决增强学习的问题，一个显而易见的做法就是我们需要估算 Value Function。是的，只要我们能够计算出价值函数，那么最优决策也就得到了。因此，问题就变成了如何计算 Value Function？\n\n$$P^a\\_{ss\\prime} = P(S\\_{t+1}=s\\prime|S\\_t =s, A\\_t =a)$$\n\n\n还记得回报 Result 的基本定义吗？就是所有 Reward 的累加（带衰减系数 discount factor）\n\n$$G\\_t = R\\_{t+1} + \\lambda R\\_{t+2} + ... = \\sum\\_{k=0}^\\infty\\lambda^kR\\_{t+k+1}$$\n\n那么 Value Function 该如何定义？也很简单，就是期望的回报啊！期望的回报越高，价值显然也就越大，也就越值得去选择。用数学来定义就是如下：\n\n$$v(s) = \\mathbb E[G\\_t|S\\_t = s]$$\n\n$$v\\_{\\pi}=\\sum\\_{a\\in A}P(a|s)\\left(R^a\\_s+\\lambda\\sum\\_{s\\prime \\in S}P^a\\_{ss\\prime}v\\_{\\pi}(s\\prime)\\right)$$\n\n接下来，我们把上式展开如下：\n\n$$\n\\begin{align}\n v(s) & = \\mathbb E[G\\_t|S\\_t = s] \\\\\\\\\n      & = \\mathbb E[R\\_{t+1}+\\lambda R\\_{t+2} + \\lambda ^2R\\_{t+3} + ...|S\\_t = s] \\\\\\\\ \n      & = \\mathbb E[R\\_{t+1}+\\lambda (R\\_{t+2} + \\lambda R\\_{t+3} + ...)|S\\_t = s] \\\\\\\\\n      & = \\mathbb E[R\\_{t+1} + \\lambda G\\_{t+1}|S\\_t = s] \\\\\\\\ \n      & = \\mathbb E[R\\_{t+1} + \\lambda v(S\\_{t+1})|S\\_t = s]\n\\end{align}\n$$\n\n因此，\n\n<font size=\"+1\">   $$v(s) = \\mathbb E[R\\_{t+1} + \\lambda v(S\\_{t+1})|S\\_t = s]$$  </font>\n\n上面这个公式就是Bellman方程的基本形态。从公式上看，**当前状态的价值和 下一步的价值以及当前的反馈Reward有关。**\n\n它表明Value Function是可以**通过迭代来进行计算的!!!**\n\n总结一下：\n\n$$v\\_{\\pi}(s) = \\mathbb E[R\\_{t+1} + \\lambda v\\_{\\pi}(S\\_{t+1})|S\\_t = s]$$\n\n$$ q\\_{\\pi}(s,a) = \\mathbb E\\_{\\pi}[R\\_{t+1} +\\lambda q\\_\\pi(S\\_{t+1},A\\_{t+1})|S\\_t =s,A\\_t = a]$$\n\n\n<br/>\n## 5. 最优化\n\n### 动态规划\n\n先简单介绍一下动态规划，因为严格来说，值迭代与策略迭代是用来解决动态规划问题的两种规划方法。而强化学习又有另外一个昵称——就是**拟动态规划**。说白了强化学习就是模拟动态规划算法。\n\n用一句话来总结动态规划就是，对一个复杂问题给出一个一般性的解决办法。它主要由两个性质:\n\n- **最优子结构**：最优解法能被分解到多个子问题中\n- **重叠子问题**：子问题能重复多次且解法能被重复利用、\n\n马尔科夫决策过程（MDP）满足以上两个性质，所以任何 MDP 都可以用动态规划来解。动态规划与强化学习的区别就是**动态规划假设 MDP 模型是全知的（即参数可知） 而 强化学习可以使 MDP 未知**。\n\nMDP需要解决的问题有两种：\n\n- 第一种是 prediction，它已知MDP的 \\\\(S,A,P,R,γ\\\\) 以及 policy，目标是算出在每个状态下的 value function(值函数其实就是问题的目标，一般都是跟 reward 有关的函数，例如 Atari 小游戏，一般值函数就是累计的得分的期望。目标一般就是最大化这个值函数。\n- 而第二种是control，它已知 MDP 的 \\\\(S,A,P,R,γ\\\\) 但是 policy 未知（即动作  \\\\(a\\_t\\\\)  未知），因此它的目标不仅是计算出最优的 value function 而且要给出最优的 Policy。\n\n###  5.1 Optimal value function 最优价值函数\n\n>能计算动作价值函数是不够的，因为我们需要的是最优策略，现在求解最优策略等价于求解最优的 value function，找到了最优的 value function，自然而然策略也就是找到。（当然，这只是求解最优策略的一种方法，也就是 value-based approach，由于 DQN 就是 value-based，因此这里只讲这部分，以后我们会看到还有 policy-based 和 model-based 方法。一个就是直接计算策略函数，一个是估计模型，也就是计算出状态转移函数，从而整个MDP过程得解）\n\n首先是最优动作价值函数和一般的动作价值函数的关系：\n\n$$V^\\*(s,a) = \\max\\_\\pi V^\\pi(s,a)$$\n\n$$Q^\\*(s,a) = \\max\\_\\pi Q^\\pi(s,a)$$\n\n也就是最优的动作价值函数就是所有策略下的动作价值函数的最大值。通过这样的定义就可以使最优的动作价值的唯一性，从而可以求解整个MDP。\n\n那么套用上一节得到的 value function，可以得到\n\n$$Q^\\*(s,a) = \\mathbb E\\_{s^\\prime}[r+\\lambda \\max \\_{a^\\prime}Q^\\*(s^\\prime,a^\\prime)|s,a]$$\n\n因为最优的Q值必然为最大值，所以，等式右侧的Q值必然为使 \\\\(a′\\\\) 取最大的Q值。\n\n下面介绍基于Bellman方程的两个最基本的算法，策略迭代和值迭代。\n<div align=center>\n![](/img/2018-08-05-RL-1.jpg)\n![](/img/2018-08-05-RL-2.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n### 5.2 Policy Iteration 策略迭代\n\n**策略迭代就是在policy未知的情况下，根据每次的reward学到最优policy。**\n\n对一个具体的 MDP 问题，每次先初始化一个策略，根据这个策略计算值函数 \\\\(v(s)\\\\) , 通过这个re值函数来根据贪心策略更新策略，不断迭代最终得到最优策略与最优值函数。总结下来就两个阶段。\n\n- **Policy evaluation** ：根据每一次的给出策略估计 \\\\(v\\_π\\\\) \n- **Policy improvement**：根据 Greedy poilcy 和之前得到的 \\\\(v\\_π\\\\) 获得当前策略 \\\\(π′\\\\) \n\nPolicy Iteration的目的是通过迭代计算value function 价值函数的方式来使policy收敛到最优。\n\n![](/img/2018-08-05-RL-15.jpg)\n![](/img/2018-08-05-RL-16.jpg)\n\n\n**给一个例子：**\n\n下图是一个叫 Small Gridworld 的例子，左上角和右下角是终点， \\\\(γ=1\\\\) ，移动一步 reward 减少1，起始的 random policy 是朝每个能走的方向概率相同，先单独看左边一列，它表示在第 \\\\(k\\\\) 次迭代每个 state上value function 的值，这一列始终采用了 random policy，这里的 value function 就是通过 Bellman Expectation Equation 得到的，考虑 \\\\(k=2\\\\) 的情况， \\\\(-1.7 = -1.0 + 2\\times (1/3.0)(-1)\\\\)，\\\\(-2.0 = -1.0 + 4(1/4.0)\\times (-1)\\\\) 。而右边一列就是在当前的 value function 情况下通过 greedy 算法找到当前朝哪个方向走更好。\n\n![](/imf/2018-08-05-RL-17.jpg)\n![](/img/2018-08-05-RL-18.jpg)\n\n\n\nPolicy Iteration 本质上就是直接使用 Bellman 方程而得到的：\n<div align=center>\n![](/img/2018-08-05-RL-3.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n### 5.3 Value Iteration 价值迭代\n\n\n\nValue Iteration 则是使用 Bellman 最优方程得到：\n<div align=center>\n![](/img/2018-08-05-RL-4.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n然后改变成迭代形式：\n<div align=center>\n![](/img/2018-08-05-RL-5.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n值迭代就是在已知 policy 和 MDP 模型的情况下，根据策略获得最优值函数和最优策略。 \n只不过这是确定策略，在值函数 \\\\(v\\_π\\\\) 取得最大值的 \\\\(a\\_t\\\\) (策略) \n通过每次迭代bellman方程获得 \\\\(v\\_i\\\\) , 知道值函数收敛。图解如下：    \n<br/>\n<div align=center>\n![](/img/2018-08-05-RL-6.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n<br/>\n## 6. Q-Value (Quality-Value)\n\nQ Learning的思想完全根据value iteration得到。但要明确一点是value iteration每次都对所有的Q值更新一遍，也就是所有的状态和动作。但事实上在实际情况下我们没办法遍历所有的状态，还有所有的动作，我们只能得到有限的系列样本。因此，只能使用有限的样本进行操作。那么，怎么处理？Q Learning提出了一种更新Q值的办法：\n\n<font size=\"+1\">  $$Q(S\\_{t},A\\_{t}) \\leftarrow Q(S\\_{t},A\\_{t})+\\alpha({R\\_{t+1}+\\lambda \\max \\_aQ(S\\_{t+1},a)} - Q(S\\_t,A\\_t))$$   </font>\n\n虽然根据value iteration计算出target Q值，但是这里并没有直接将这个Q值（是估计值）直接赋予新的Q，而是采用渐进的方式类似梯度下降，朝target迈近一小步，取决于α,这就能够减少估计误差造成的影响。类似随机梯度下降，最后可以收敛到最优的Q值。\n\n**具体的算法如下：**\n<div align=center>\n![](/img/2018-08-05-RL-7.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n**大致代码流程如下：**\n\n```python\ndef update():\n    # 学习 100 回合\n    for episode in range(100):\n        # 初始化 state 的观测值\n        observation = env.reset()\n\n        while True:\n            # 更新可视化环境\n            env.render()\n\n            # RL 大脑根据 state 的观测值挑选 action\n            action = RL.choose_action(str(observation))\n\n            # 探索者在环境中实施这个 action, 并得到环境返回的下一个 state 观测值, reward 和 done (是否是掉下地狱或者升上天堂)\n            observation_, reward, done = env.step(action)\n\n            # RL 从这个序列 (state, action, reward, state_) 中学习\n            RL.learn(str(observation), action, reward, str(observation_))\n\n            # 将下一个 state 的值传到下一次循环\n            observation = observation_\n\n            # 如果掉下地狱或者升上天堂, 这回合就结束了\n            if done:\n                break\n\n    # 结束游戏并关闭窗口\n    print('game over')\n    env.destroy()\n\nif __name__ == \"__main__\":\n    # 定义环境 env 和 RL 方式\n    env = Maze()\n    RL = QLearningTable(actions=list(range(env.n_actions)))\n\n    # 开始可视化环境 env\n    env.after(100, update)\n    env.mainloop()\n```\n\n**注意：**\n\n每一组 `(state, action, reward, state_)` 为一次序列\n\n<br/>\n以我们回到之前的流程, 根据 Q 表的估计, 因为在 s1 中, a2 的值比较大, 通过之前的决策方法, 我们在 s1 采取了 a2, 并到达 s2, 这时我们开始更新用于决策的 Q 表, 接着我们并没有在实际中采取任何行为, 而是再想象自己在 s2 上采取了每种行为, 分别看看两种行为哪一个的 Q 值大, 比如说 Q(s2, a2) 的值比 Q(s2, a1) 的大, 所以我们把大的 Q(s2, a2) 乘上一个衰减值 gamma (比如是0.9) 并加上到达s2时所获取的奖励 R (这里还没有获取到我们的棒棒糖, 所以奖励为 0), 因为会获取实实在在的奖励 R , 我们将这个作为我现实中 Q(s1, a2) 的值, 但是我们之前是根据 Q 表估计 Q(s1, a2) 的值. 所以有了现实和估计值, 我们就能更新Q(s1, a2) , 根据 估计与现实的差距, 将这个差距乘以一个学习效率 alpha 累加上老的 Q(s1, a2) 的值 变成新的值. \n\n**但时刻记住, 我们虽然用 maxQ(s2) 估算了一下 s2 状态, 但还没有在 s2 做出任何的行为, s2 的行为决策要等到更新完了以后再重新另外做. 这就是 off-policy 的 Q learning 是如何决策和学习优化决策的过程.**\n<div align=center>\n![](/img/2018-08-05-RL-8.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n$$\\text{update = learing\\_rate * (q\\_target - q\\_predict)}$$\n\n$$\\text{学习率 * (真实值 - 预测值)}$$\n\n\n>我们想象 Qlearning 的机器人天生近视眼,  \\\\(\\gamma= 1\\\\) 时, 机器人有了一副合适的眼镜, 在 s1 看到的 Q 是未来没有任何衰变的奖励, 也就是机器人能清清楚楚地看到之后所有步的全部价值, 但是当  \\\\(\\gamma= 0\\\\) , 近视机器人没了眼镜, 只能摸到眼前的 reward, 同样也就只在乎最近的大奖励, 如果  \\\\(\\gamma\\\\)  从 0 变到 1, 眼镜的度数由浅变深, 对远处的价值看得越清楚, 所以机器人渐渐变得有远见, 不仅仅只看眼前的利益, 也为自己的未来着想.\n\n<br/>\n## 7. Exploration and Exploitation 探索与利用\n\n在上面的算法中，我们可以看到需要使用某一个policy来生成动作，也就是说这个policy不是优化的那个policy，所以Q-Learning算法叫做Off-policy的算法。另一方面，因为Q-Learning完全不考虑model模型也就是环境的具体情况，只考虑看到的环境及reward，因此是model-free的方法。 \n\n回到policy的问题，那么要选择怎样的 policy 来生成 action 呢？有两种做法：\n\n- 随机的生成一个动作\n- 根据当前的Q值计算出一个最优的动作，这个 policy \\\\(\\pi\\\\) 称之为 **greedy policy 贪婪策略**。也就是 \\\\(\\pi(S\\_{t+1}) = arg\\max \\_aQ(S\\_{t+1},a)\\\\) \n\n**使用随机的动作就是 exploration**，也就是探索未知的动作会产生的效果，有利于更新Q值，获得更好的policy。\n\n而使用 **greedy policy 也就是 target policy 则是 exploitation**，利用policy，这个相对来说就不好更新出更好的Q值，但可以得到更好的测试效果用于判断算法是否有效。\n\n将两者结合起来就是所谓的 \\\\(\\epsilon\\ greedy\\\\) 策略， \\\\(\\epsilon\\\\) 一般是一个很小的值，作为**选取随机动作的概率值**。可以更改 \\\\(\\epsilon\\\\) 的值从而得到不同的 exploration 和 exploitation 的比例。例如 \\\\(\\epsilon = 0.1\\\\) 表示 90% 的时间是选择最优策略, 10% 的时间来探索.\n\n>要注意一点就是 egreedy 的 \\\\(\\epsilon\\\\) 是不断变小的，也就是随机性不断变小。怎么理解呢？就是一开始需要更多的探索，所以动作偏随机，慢慢的我们需要动作能够有效，因此减少随机。也就是越来越贪婪。   \n>**例如：**     \n>INITIAL\\_EPSILON = 0.5  # starting value of epsilon     \n>FINAL\\_EPSILON = 0.01  # final value of epsilon\n\n这里需要说明的一点是使用 \\\\(\\epsilon-greedy\\\\) 策略是一种极其简单粗暴的方法，对于一些复杂的任务采用这种方法来探索未知空间是不可取的。因此，最近有越来越多的方法来改进这种探索机制。\n\n<br/>\n## 8. 详解Q-Learning\n\n\n### 8.1 Value Function Approximation 价值函数近似\n\n在简单分析中，我们使用表格来表示Q(s,a)，但是这个在现实的很多问题上是几乎不可行的，因为状态实在是太多。使用表格的方式根本存不下。\n\n我们有必要对状态的维度进行压缩，解决办法就是 **价值函数近似 Value Function Approximation**\n\n就是**用一个函数来表示Q(s,a)**，即：\n\n$$Q(s,a) = f(s,a)$$\n\nf可以是任意类型的函数，比如线性函数：\n\n$$Q(s,a) = w\\_1s + w\\_2a + b$$ \n其中 \\\\(w\\_1,w\\_2,b\\\\) 是函数 \\\\(f\\\\) 的参数。\n\n通过函数表示，我们就可以无所谓 \\\\(s\\\\) 到底是多大的维度，反正最后都通过矩阵运算降维输出为单值的 \\\\(Q\\\\) 。这就是价值函数近似的基本思路。\n\n如果我们就用 \\\\(w\\\\) 来统一表示函数f的参数，那么就有\n\n$$Q(s,a) = f(s,a,w)$$\n\n为什么叫近似，因为我们并不知道 \\\\(Q\\\\) 值的实际分布情况，本质上就是用一个函数来近似 \\\\(Q\\\\) 值的分布，所以，也可以说是\n\n$$Q(s,a)\\approx f(s,a,w)$$\n\n\n<br/>\n### 8.2 Q值神经网络化！\n\n>用一个深度神经网络来表示这个函数 \\\\(f\\\\)，即我们可以将状态和动作当成神经网络的输入, 然后经过神经网络分析后得到动作的 Q 值, 这样我们就没必要在表格中记录 Q 值, 而是直接使用神经网络生成 Q 值.还有一种形式的是这样, 我们也能只输入状态值, 输出所有的动作值, 然后按照 Q learning 的原则, 直接选择拥有**最大值的动作**当做下一步要做的动作。一般使用第二种形式。\n\n以DQN为例，输入是经过处理的4个连续的84x84图像，然后经过两个卷积层，两个全连接层，最后输出包含每一个动作Q值的向量。\n\n用神经网络来表示Q值非常简单，Q值也就是变成用Q网络（Q-Network）来表示。接下来就到了很多人都会困惑的问题，那就是怎么训练Q网络？？？\n\n我们知道，神经网络的训练是一个最优化问题，最优化一个损失函数loss function，也就是标签和网络输出的偏差，目标是让损失函数最小化。为此，我们需要有样本，巨量的有标签数据，然后通过反向传播使用梯度下降的方法来更新神经网络的参数。\n\n所以，要训练Q网络，我们要能够为Q网络提供有标签的样本。\n\n所以，问题变成：\n\n**如何为 Q 网络提供有标签的样本？   \n答案就是利用 Q-Learning 算法。**\n\n回想一下 Q-Learning 算法，    \n$$Q(S\\_{t},A\\_{t}) \\leftarrow Q(S\\_{t},A\\_{t})+\\alpha({R\\_{t+1}+\\lambda \\max \\_aQ(S\\_{t+1},a)} - Q(S\\_t,A\\_t))$$\n\nQ值的更新依靠什么？依靠的是利用 Reward 和 Q 计算出来的目标Q值：\n\n$$\\text{Target-Q :  }\\ \\ R\\_{t+1}+\\lambda \\max \\_aQ(S\\_{t+1},a)$$\n\n因此，我们把目标Q值作为标签不就完了？我们的目标不就是**让Q值趋近于目标Q值**吗？    \n因此，Q网络训练的损失函数就是：\n<div align=center>\n![](/img/2018-08-05-RL-9.jpg)\n<center><small><font color=gray>  \\\\(s^\\`,a^`\\\\) 即下一个状态和动作   </font></small></center>\n</div>\n\n既然确定了损失函数，也就是cost，确定了获取样本的方式。那么DQN的整个算法也就成型了！接下来就是具体如何训练的问题了！\n\n<br/>\n**前边提到**，每一组 `(state, action, reward, state_)` 为一次序列：\n\n- `state (observation)` 为目前状态，传递给 q-eval net 得到预计值 (即输入状态值, 输出所有的动作值)；     \n- `state_` 为下一步状态，传递给 q-target net 得到目标值，之后可以得到 \\\\(\\max \\_aQ(S\\_{t+1},a)\\\\) ，在之后得到 \\\\(R\\_{t+1}+\\lambda \\max \\_aQ(S\\_{t+1},a)\\\\) \n\n**最终的  \\\\(loss\\\\)  为：**\n\n```python\nself.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))\n```\n**再次注意，q-eval net 和 q-target net 网络结构完全一样，只不过参数更新不同！(即 Fixed Q-targets 方法)**\n<div align=center>\n![](/img/2018-08-05-RL-10.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n<br/>\n<div align=center>\n![](/img/2018-08-05-RL-11.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n\n最基本的DQN，也就是NIPS 13版本的DQN：\n<div align=center>\n![](/img/2018-08-05-RL-12.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n**那么上面的算法看起来那么长，其实就是反复试验，然后存储数据。接下来数据存到一定程度，就每次随机采用数据，进行梯度下降！**\n\n也就是在DQN中增强学习 Q-Learning 算法和深度学习的 SGD 训练是同步进行的！\n通过 Q-Learning 获取无限量的训练样本，然后对神经网络进行训练。\n\n**整体代码结构大致如下：**\n\n```python\ndef run_maze():\n    step = 0    # 用来控制什么时候学习\n    for episode in range(300):\n        # 初始化环境\n        observation = env.reset()\n\n        while True:\n            # 刷新环境\n            env.render()\n\n            # DQN 根据观测值选择行为\n            action = RL.choose_action(observation)\n\n            # 环境根据行为给出下一个 state, reward, 是否终止\n            observation_, reward, done = env.step(action)\n\n            # DQN 存储记忆\n            RL.store_transition(observation, action, reward, observation_)\n\n            # 控制学习起始时间和频率 (先累积一些记忆再开始学习)\n            if (step > 200) and (step % 5 == 0):\n                RL.learn()\n\n            # 将下一个 state_ 变为 下次循环的 state\n            observation = observation_\n\n            # 如果终止, 就跳出循环\n            if done:\n                break\n            step += 1   # 总步数\n\n    # end of game\n    print('game over')\n    env.destroy()\n\n\nif __name__ == \"__main__\":\n    env = Maze()\n    RL = DeepQNetwork(env.n_actions, env.n_features,\n                      learning_rate=0.01,\n                      reward_decay=0.9,\n                      e_greedy=0.9,\n                      replace_target_iter=200,  # 每 200 步替换一次 target_net 的参数\n                      memory_size=2000, # 记忆上限\n                      # output_graph=True   # 是否输出 tensorboard 文件\n                      )\n    env.after(100, run_maze)\n    env.mainloop()\n    RL.plot_cost()  # 观看神经网络的误差曲线\n```\n\n<br/>\n### 8.3 Experience Replay 经验回放\n\n>Q learning 是一种 off-policy 离线学习法, 它能学习当前经历着的, 也能学习过去经历过的, 甚至是学习别人的经历. 所以每次 DQN 更新的时候, 我们都可以随机抽取一些之前的经历进行学习. 随机抽取这种做法打乱了经历之间的相关性, 也使得神经网络更新更有效率。\n\n其将系统探索环境得到的数据储存起来，然后随机采样样本更新深度神经网络的参数（有了一个记忆库之后再开始学习）。\n<div align=center>\n![](/img/2018-08-05-RL-13.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\nExperience Replay 的**动机**是：\n\n- 深度神经网络作为有监督学习模型，要求数据满足独立同分布，\n- 但 Q Learning 算法得到的样本前后是有关系的。为了打破数据之间的关联性，Experience Replay 方法通过存储-采样的方法将这个关联性打破了。\n\n之所以加入 experience replay 是因为样本是从游戏中的连续帧获得的，这与简单的 reinforcement learning 问题（比如maze）相比，样本的关联性大了很多，如果没有 experience replay，算法在连续一段时间内基本朝着同一个方向做 gradient descent，那么同样的步长下这样直接计算 gradient 就有可能不收敛。**因此 experience replay 是从一个 memory pool 中随机选取了一些 experience，然后再求梯度**，从而避免了这个问题。\n\n>原文的实验中指出mini batch是32，而replay memory存了最近的1000000帧。\n\n<br/>\n### 8.4 Fixed Q-targets\n\nFixed Q-targets 也是一种打乱相关性的机理, 如果使用 fixed Q-targets, 我们就会在 DQN 中使用到**两个结构完全相同但参数不同的神经网络** (有时差), 预测 Q 估计 的神经网络 **(evaluate net)** 具备**最新的参数**, 而预测 Q 现实 的神经网络 **(target net)** 使用的**参数则是很久以前的**。 \n\n例如一开始有两个完全一样的网络，一个进行训练，另一个不训练，到了训练10000次后，把训练过的网络参数完全复制给冻结的网络，之后仍是一个训练，持续更新参数，一个冻结，每10000次才更新一次。\n\n`target_net` 用于预测 `q_target` 目标值, 他不会及时更新参数. \n\n`eval_net` 用于预测 `q_eval` 估计值, 这个神经网络拥有最新的神经网络参数\n<div align=center>\n![](/img/2018-08-05-RL-14.jpg)\n\n\n\n![](/img/2018-08-05-RL-19.jpg)\n\n\n### 8.5 总结\n\n在 Q-Learning 算法中，计算经验得分的公式如下：\n\n$$\\text{Q(state, action) = Q(state, action) + }\\alpha\\text{ (R(state, action) + }\\gamma \\text{ Max[Q(next state, all actions)] - Q(state, action))}$$\n\n当  \\\\(\\alpha\\\\)  的值是  \\\\(1\\\\)  时，公式如下：\n\n$$\\text{Q(state, action) = R(state, action) +} \\gamma\\text{ Max[Q(next state, all actions)]}$$\n\n \n\n- `state`： 表示 Agent 当前状态。\n- `action`： 表示 Agent 在当前状态下要做的行为。\n- `next state`： 表示 Agent 在 state 状态下执行了 action 行为后达到的新的状态。\n- `Q(state, action)`： 表示 Agent 在 state 状态下执行了 action 行为后学习到的经验，也就是经验分数。\n- `R(state, action)`： 表示 Agent 在 state 状态下做 action 动作后得到的即时奖励分数。\n- `Max[Q(next state, all actions)]`： 表示 Agent 在 next state 状态下，自我的经验中，最有价值的行为的经验分数。\n- `Gamma`：  \\\\(\\gamma\\\\) ，表示折损率，也就是未来的经验对当前状态执行 action 的重要程度。\n\n<br/>\n**算法流程:**\n\nAgent 通过经验去学习。Agent将会从一个状态到另一个状态这样去探索，直到它到达目标状态。我们称每一次这样的探索为一个场景（episode）。\n  \n每个场景就是 Agent 从起始状态到达目标状态的过程。每次 Agent 到达了目标状态，程序就会进入到下一个场景中。\n\n1. 初始化 Q 矩阵，并将初始值设置成 0\n2. 设置好参数 γ 和得分矩阵 R\n3. 循环遍历场景（episode）：\n\n\t1. 随机初始化一个状态 s\n\t2. 如果未达到目标状态，则循环执行以下几步：\n\t\n\t\t1. 在当前状态 s 下，随机选择一个行为 a\n\t\t2. 执行行为 a 得到下一个状态 s`\n\t\t3. 使用  \\\\(\\text{Q(state, action) = R(state, action) +} \\gamma\\text{ Max[Q(next state, all actions)]}\\\\) 公式计算  \\\\(\\text{Q(state, action)}\\\\) \n\t\t4. 将当前状态 s 更新为 s`\n\n\n## 参考\n\n1. [DQN 从入门到放弃](https://zhuanlan.zhihu.com/p/21262246)\n2. [经验回放（Experience replay）](https://blog.csdn.net/suoyan1539/article/details/79571010)\n3. [强化学习系列之九:Deep Q Network (DQN)](http://www.algorithmdog.com/drl#i)\n4. [莫烦 PYTHON 强化学习 Reinforcement Learning](https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/)\n\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n","tags":["强化学习"],"categories":["强化学习"]},{"title":"MATLAB 入门","url":"/2018/07/17/matlab/","content":"\n\n## 1 基础\n\n### 注释\n\n`%我是注释`\n\n### 帮助\n\n`help 函数名或命令名`\n\n### 清理窗口信息的命令\n\n- `clc  `：清除命令窗口内的内容，即只清扫屏幕，但不清除内存中已存在的变量\n- `clf  `：擦除 Matlab 的当前图形窗口中的图形\n- `clear  `：清除内存中的指定变量或函数\n- `clear all  `：清除内存中所有的变量和函数\n\n### 已预定义的常量\n\n-  ` eps` ：计算机的最小整数\n-  ` pi ` ：圆周率 \\\\(pi\\\\) \n-  ` inf`  或 ` Inf `：无穷大 \\\\(∞\\\\) \n-  ` NaN` ：不定量\n-  ` i`  或  ` j` ：虚数单位，但可以重新被定义为别的变量(如果没有赋值就直接使用，则默认为是虚数单位)\n\n<br/>\n## 2 数组\n\n### 一维数组\n\n- `X = A ：step ：B`\n\n当没有指定step时，系统默认 `step=1`。\n\n\n- 生成4个数，起始为1，末尾为2的等差数列\n\n```\n>> linspace(1,2,4)\n\nans =\n\n    1.0000    1.3333    1.6667    2.0000\n```\n\n- `x=logspace(a, b, n)`\n\n功能：logspace(a, b, n)生成一个(1xn)数组，数据的第一个元素值为a，最后一个元素为b，n是总采样点数。需要注意的是，此时产生的数组元素在10^a 到10^b上并不是均匀分布的，而形成一个对数曲线。\n\n### 二维数组\n\n```\n>> [1 2 3; 4 5 6; 7 8 9]\n\nans =\n\n     1     2     3\n     4     5     6\n     7     8     9\n```\n\n### 三维数组\n\nxx\n\n### 多维数组操作\n\n```\n>> a = [1 2 3];\n>> b = [4 5 6];\n\n>> [a,b]\n>> \nans =\n\n     1     2     3     4     5     6\n\n>> [a;b]\n\nans =\n\n     1     2     3\n     4     5     6\n```\n\n```\n>> A = [1 2; 3 4];\n>> B = [5 6; 7 8];\n>> [A, B]\n\nans =\n\n     1     2     5     6\n     3     4     7     8\n\n>> [A;B]\n\nans =\n\n     1     2\n     3     4\n     5     6\n     7     8\n\n```\n\n#### cat() \n\n用来联结数组\n\n`C = cat(dim, A, B)`       按dim来联结A和B两个数组。\n\n`C = cat(dim, A1, A2, A3, ...)`    按dim联结所有输入的数组。\n\n`a=cat(3,A,B)` 左括号后的3表示构造出的矩阵维数；在新的矩阵中第1、2维就是A和B这两个矩阵的行数和列数，第3维是A和B这两个矩阵的矩阵个数，即为2\n\n`cat(2, A, B)` 相当于 [A, B];\n\n`at(1, A, B)` 相当于 [A; B].\n\n#### size()\n\n按照行-列-页的顺序，返回数组A每一维上的大小\n\n```\n>> a\n\na =\n\n     1     2     3\n     \n>> size(a)\n\nans =\n\n     1     3\n```\n\n#### ndims()\n\n```\n>> a\n\na =\n\n     1     2     3\n\n>> ndims(a)\n\nans =\n\n     2\n```\n返回数组A具有的维度值\n\n#### whos\n\n返回当前工作区的各个变量的详细信息\n\n```\n>> whos\n  Name      Size            Bytes  Class     Attributes\n\n  A         2x2                32  double              \n  B         2x2                32  double              \n  a         1x3                24  double              \n  ans       1x1                 8  double              \n  b         1x3                24  double              \n```\n\n<br/>\n## 3 线性代数\n\n### 常量矩阵命令\n\n`zeros(m,n)`：产生一个  \\\\(m\\times n\\\\)  零矩阵    \n`zeros(n)`：产生一个  \\\\(n\\\\)  阶零方阵      \n`ones(m,n)`：产生一个所有元素为  \\\\(1\\\\)  的  \\\\(m\\times n\\\\)  矩阵      \n`ones(n)`：产生一个所有元素为  \\\\(1\\\\)  的  \\\\(n\\\\)  阶方阵     \n`eye(n)`：产生一个  \\\\(n\\\\)  阶单位阵\n\n\n### 运算符\n\nMatlab 最擅长于线性代数中关于矩阵的各种运算，常用的运算符有：\n\n1.  `+` ：两矩阵和的运算\n2.  `-` ：两矩阵减的运算\n3.  `*` ：两矩阵乘积的运算\n4.  `.*` ：两矩阵各相应位置元素乘积的运算\n5.  `./` ：两矩阵各相应位置元素相除的运算\n6.  `A'` ：矩阵 A 的转置\n7.  `inv(A)` ：矩阵 A 的逆阵\n8.  `eig(A)` ：矩阵 A 的特征值\n9. `[V,D]=eig(A)`：给出由矩阵 A 的特征向量组成的矩阵 V(以列向量排列)和由对应的特征值组成的对角阵 D(特征值为对角线元素)\n10. `sum(A,1)`：对矩阵 A 的每列元素求和，给出求和值的行向量\n11. `sum(A,2)`：对矩阵 A 的每行元素求和，给出求和值的列向量\n12. `sum(X)`：对向量 X=(x1,x2,…….,xn)的分量求和\n13. `cumsum(X)`：给出向量 X 的累加和\n14. `length(X)`：给出向量 X 的维数，即其分量的个数\n15. `norm(X)`：给出向量 X 的范数，即向量的长度\n\n\n<br/>\n## 4 结构\n\n### if \n\n```matlab\n格式1：\n    if 条件\n        语句组\n    end\n\n格式2：\n    if 条件\n        语句组1\n    else\n        语句组2\n    end\n\n格式3：\n    if 条件1\n        语句组1\n    elseif 条件2\n        语句组2\n    .....\n    elseif 条件m\n        语句组m\n    else\n        语句组m+1\n    end\n```\n\n### switch\n\n```matlab\nswitch 表达式\n    case 值1\n        语句组1\n    case 值2\n        语句组2\n    ....\n    case 值m\n        语句组m\n    otherwise %其他值\n        语句组m+1\nend   \n```\n\n### for\n\n表达式1 的值是循环变量的初值，表达式2 的值是循环变量的增量，表达式3 的值为循环变量的终值。如果增量为1 时，表达式2 可以省略不写\n\n```matlab\nfor 循环变量=表达式1：表达式2：表达式3\n    循环体语句\nend\n```\n\n### while\n\n```matlab\nwhile 条件\n    循环体语句\nend\n```\n\n<br/>\n## 5 画图\n\n### 二维\n\n`plot(x, y)`\n\n**例：在区间 [0,2π] 内绘制正弦曲线 y=sinx 的语句**\n\n```matlab\nx=0:pi/100:2*pi;\ny=sin(x);\nplot(x,y)\n```\n\n指定线型与颜色\n\n`plot(x,y1,'cs1',x,y2,'cs2',....)`\n\n它以公共向量 x 为 X 轴，分别以 y1,y2，… 为 Y 轴在同一幅图内绘制出多条曲线，同时可以指定它们的不同颜色与不同线性。\n\n每条曲线的颜色和线型用字符串 ‘cs’ 来指定，其中 c 表示颜色，而 s 表示线型，线型可以是线或者标记，线和标记可同时使用。它们的位置次序可随意，如缺省的话，则默认颜色为蓝色、线型为实型。它们的符号如下所示。\n\n\n\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n","tags":["MATLAB"],"categories":["编程语言"]},{"title":"SSD 目标检测（七）","url":"/2018/07/07/SSD/","content":"\n\n\n>论文地址：[SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325)\n\n## 1 设计理念\n\n### 1.1 采用卷积进行检测\n\n与Yolo最后采用全连接层不同，SSD直接采用卷积对不同的特征图来进行提取检测结果。对于形状为  \\\\(m\\times n \\times p\\\\)  的特征图，只需要采用  \\\\(3\\times 3 \\times p\\\\)  这样比较小的卷积核得到检测值。\n\n<br/>\n\n### 1.2 设置先验框\n\n\n\n\n而SSD借鉴了Faster R-CNN中anchor的理念，每个单元设置尺度或者长宽比不同的先验框，预测的边界框（bounding boxes）是以这些先验框为基准的，在一定程度上减少训练难度。一般情况下，每个单元会设置多个先验框，其尺度和长宽比存在差异，如图5所示，可以看到每个单元使用了4个不同的先验框，图片中猫和狗分别采用最适合它们形状的先验框来进行训练，后面会详细讲解训练过程中的先验框匹配原则。\n\n<div align=center>\n![](/img/2018-07-07-SSD-1.jpg)\n<center><small><font color=gray>  SSD的先验框  </font></small></center>\n</div>\n\n对于每个单元的每个先验框，都有两个不同的检验值。\n\n第一部分是**各个类别的置信度或者评分**，在预测过程中，置信度最高的那个类别就是边界框所属的类别，特别地，当第一个置信度值最高时，表示边界框中并不包含目标。\n\n>值得注意的是 SSD 将背景也当做了一个特殊的类别，如果检测目标共有  \\\\(c\\\\)  个类别，SSD 其实需要预测  \\\\(c+1\\\\)  个置信度值，其中第一个置信度指的是不含目标或者属于背景的评分。后面当我们说  \\\\(c\\\\)  个类别置信度时，请记住里面包含背景那个特殊的类别，即真实的检测类别只有  \\\\(c-1\\\\)  个。\n\n第二部分就是**边界框的 location**，包含4个值  \\\\((cx, cy, w, h)\\\\)  ，分别表示边界框的中心坐标以及宽高。\n\n但是真实预测值其实只是边界框相对于先验框的转换值。先验框位置用  \\\\(d=(d^{cx}, d^{cy}, d^w, d^h) \\\\) 表示，其对应边界框用  \\\\(b=(b^{cx}, b^{cy}, b^w, b^h)\\\\)  表示，那么边界框的预测值  \\\\(l\\\\)  其实是  \\\\(b\\\\)  相对于  \\\\(d\\\\)  的转换值：\n\n$$l^{cx} = (b^{cx} - d^{cx})/d^w, \\space l^{cy} = (b^{cy} - d^{cy})/d^h$$\n\n$$l^{w} = \\log(b^{w}/d^w), \\space l^{h} = \\log(b^{h}/d^h)$$\n\n习惯上，我们称上面这个过程为边界框的编码（encode），预测时，你需要反向这个过程，即进行解码（decode），从预测值 l 中得到边界框的真实位置 b ：\n\n$$b^{cx}=d^w l^{cx} + d^{cx}, \\space b^{cy}=d^y l^{cy} + d^{cy}$$\n\n$$b^{w}=d^w \\exp(l^{w}), \\space b^{h}=d^h \\exp(l^{h})$$\n\n<br/>\n\n**综上所述**，对于一个大小  \\\\(m\\times n\\\\)  的特征图，共有  \\\\(mn\\\\)  个单元，每个单元设置的先验框数目记为  \\\\(k\\\\)  ，那么每个单元共需要  \\\\((c+4)k\\\\)  个预测值，所有的单元共需要  \\\\((c+4)kmn\\\\)  个预测值，由于 SSD 采用卷积做检测，所以就需要  \\\\((c+4)k\\\\)  个卷积核完成这个特征图的检测过程\n\n<br/>\n\n### 1.3 网络结构\n\n<div align=center>\n![](/img/2018-07-07-SSD-2.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n>**SSD利用了多尺度的特征图做检测**\n>\n>对每次卷积后输出的 \\\\(38×38,19×19\\\\) 等大小的特征图都保存下来，并都进行进一步的操作\n\nSSD采用VGG16作为基础模型，然后在 VGG-16 的基础上新增了卷积层来获得更多的特征图以用于检测。分别将VGG16的全连接层 fc6 和 fc7 转换成  \\\\(3\\times3\\\\)  卷积层 conv6 和  \\\\(1\\times1 \\\\) 卷积层 conv7，为了配合这种变化，采用了一种 `Atrous Algorithm`，其实就是 conv6 采用**扩展卷积或带孔卷积**（Dilation Conv），其在不增加参数与模型复杂度的条件下指数级扩大卷积的视野，其使用扩张率(dilation rate)参数，来表示扩张的大小，\n\n<div align=center>\n![](/img/2018-07-07-SSD-3.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n- (a)是普通的 \\\\( 3\\times3 \\\\) 卷积，其视野就是 \\\\( 3\\times3 \\\\) \n- (b)是扩张率为1，此时视野变成  \\\\(7\\times7\\\\)  \n- (c)扩张率为3时，视野扩大为  \\\\(15\\times15\\\\)  ，但是视野的特征更稀疏了。\n\nConv6 采用 \\\\( 3\\times3 \\\\) 大小但  \\\\(\\text{dilation rate=6}\\\\)  的扩展卷积。\n\n之后移除 dropout 层和 fc8 层，并新增一系列卷积层，在检测数据集上做 finetuing。其中 VGG16 中的 Conv4\\_3 层将作为用于检测的第一个特征图。\n\n>conv4\\_3层特征图大小是 38\\times38 ，但是该层比较靠前，其norm较大，所以在其后面增加了一个 **L2 Normalization** 层，以保证和后面的检测层差异不是很大，这个和 Batch Normalization 层不太一样，其仅仅是对每个像素点在 channle 维度做归一化，而 Batch Normalization 层是在  \\\\(\\text{[batch\\_size, width, height]}\\\\)  三个维度上做归一化。归一化后一般设置一个可训练的放缩变量 gamma，使用TF可以这样简单实现\n\n```python\n# l2norm (not bacth norm, spatial normalization)\ndef l2norm(x, scale, trainable=True, scope=\"L2Normalization\"):\n    n_channels = x.get_shape().as_list()[-1]\n    l2_norm = tf.nn.l2_normalize(x, [3], epsilon=1e-12)\n    with tf.variable_scope(scope):\n        gamma = tf.get_variable(\"gamma\", shape=[n_channels, ], dtype=tf.float32,\n                                initializer=tf.constant_initializer(scale),\n                                trainable=trainable)\n        return l2_norm * gamma\n```\n\n## 2 训练过程\n\n\n### 先验框匹配 \n\n在训练过程中，首先要确定训练图片中的ground truth（真实目标）与哪个先验框来进行匹配，与之匹配的先验框所对应的边界框将负责预测它。\n\n在Yolo中，ground truth的中心落在哪个单元格，该单元格中与其IOU最大的边界框负责预测它。但是在SSD中却完全不一样，SSD的先验框与ground truth的匹配原则主要有两点。\n\n**第一个原则是：**对于图片中每个ground truth，找到与其IOU最大的先验框，该先验框与其匹配，这样，可以保证每个ground truth一定与某个先验框匹配。\n\n>通常称与ground truth匹配的先验框为正样本（其实应该是先验框对应的预测box，不过由于是一一对应的就这样称呼了），反之，若一个先验框没有与任何ground truth进行匹配，那么该先验框只能与背景匹配，就是负样本。\n>\n>一个图片中ground truth是非常少的， 而先验框却很多，如果仅按第一个原则匹配，很多先验框会是负样本，正负样本极其不平衡，所以需要第二个原则。\n\n**第二个原则是：**对于剩余的未匹配先验框，若某个 ground truth 的  \\\\(\\text{IOU}\\\\)  大于某个阈值（一般是0.5），那么该先验框也与这个 ground truth 进行匹配。\n\n>这意味着某个 ground truth 可能与多个先验框匹配，这是可以的。但是反过来却不可以，**因为一个先验框只能匹配一个 ground truth**，如果多个 ground truth 与某个先验框 \\\\( \\text{IOU} \\\\) 大于阈值，那么先验框只与 \\\\( \\text{IOU} \\\\) 最大的那个先验框进行匹配。\n>\n>第二个原则一定在第一个原则之后进行\n\n<div align=center>\n![](/img/2018-07-07-SSD-4.jpg)\n<center><small><font color=gray> 绿色的GT是ground truth，红色为先验框，FP表示负样本，TP表示正样本   </font></small></center>\n</div>\n\n尽管一个 ground truth 可以与多个先验框匹配，但是 ground truth 相对先验框还是太少了，所以负样本相对正样本会很多。为了保证正负样本尽量平衡，SSD 采用了 `hard negative mining`，就是**对负样本进行抽样**，抽样时按照置信度误差（预测背景的置信度越小，误差越大）进行降序排列，选取误差的较大的top-k作为训练的负样本，以保证正负样本比例接近 \\\\(1:3\\\\) 。\n\n### 损失函数 \n\n损失函数定义为**位置误差**（locatization loss， loc）与**置信度误差**（confidence loss, conf）的加权和：\n\n<font size=\"+1\">  $$L(x, c, l, g) = \\frac{1}{N}(L\\_{conf}(x,c) + \\alpha L\\_{loc}(x,l,g))$$   </font>\n\n- \\\\(N\\\\)  是先验框的正样本数量\n- \\\\(x^p\\_{ij}\\in \\{ 1,0 \\} \\\\) 为一个指示参数，当  \\\\(x^p\\_{ij}= 1\\\\)  时表示第  \\\\(i\\\\)  个先验框与第  \\\\(j\\\\)  个 ground truth 匹配，并且 ground truth 的类别为  \\\\(p\\\\)  \n-  \\\\(c\\\\)  为类别置信度预测值 \n-  \\\\(l\\\\)  为先验框的所对应边界框的位置预测值\n-  \\\\(g\\\\)  是 ground truth 的位置参数\n\n对于**位置误差**，其采用 Smooth L1 loss，定义如下：\n\n<div align=center>\n![](/img/2018-07-07-SSD-5.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n<font size=\"+1\">  \n$$\nsmooth\\_{L\\_{1}}(x) =\n\\begin{cases}\n0.5x^2, & \\text{if |x| < 1}\\\\\\\\\n|x|-0.5, & \\text{otherwise}\n\\end{cases}\n$$  \n</font>\n\n对于**置信度误差**，其采用 softmax loss:\n\n<div align=center>\n![](/img/2018-07-07-SSD-6.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n### 数据扩增\n\n<br />\n\n## 参考\n\n1. [目标检测|SSD原理与实现](https://zhuanlan.zhihu.com/p/33544892)\n\n <script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n","tags":["目标检测"],"categories":["深度学习"]},{"title":"L0 L1 L2 正则化","url":"/2018/06/29/regularization/","content":"\n\n\n## 范数\n\n\n<font size=\"+1\">  $$\\Vert x \\Vert\\_p:=\\left(\\sum^n\\_{i=1}\\vert x\\_i \\vert^p\\right)^{\\frac {1}{p}}$$\n   </font>\n\n-  \\\\(L1\\ 范数：当\\ p=1\\ 时，表示某个向量中所有元素绝对值之和\\\\) \n-  \\\\(L2\\ 范数：当\\ p=2\\ 时，表示某个向量中所有元素平方和再开根， 也就是欧几里得距离公式\\\\)    \n\n\n对于线性回归模型，使用 L1 正则化的模型建叫做 **Lasso 回归**，使用 L2 正则化的模型叫做 **Ridge 回归（岭回归）**。\n\n## L1\n\n$$ \\mathop{argmin} \\limits_{w}{\\frac{1}{2n\\_{samples}}} \\Vert X\\_w - y\\Vert^2\\_2+\\alpha\\Vert w \\Vert\\_1$$\n\n\n**作用**\n\nL1 正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择，一定程度上，L1也可以防止过拟合。\n\n通常越大的 \\\\(\\lambda\\\\) 可以让代价函数在参数为0时取到最小值\n\n>稀疏矩阵指的是很多元素为 0，只有少数元素是非零值的矩阵，即得到的线性回归模型的大部分系数都是 0。\n>\n>  通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。   \n> 在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，或者贡献微小（因为它们前面的系数是0或者是很小的值，即使去掉对模型也没有什么影响），此时我们就可以只关注系数是非零值的特征。这就是稀疏模型与特征选择的关系。\n\n\n\n## L2\n\n**岭回归计算公式**\n\n$$ \\mathop{argmin} \\limits_{w}{\\frac{1}{2n\\_{samples}}} \\Vert X\\_w - y\\Vert^2\\_2+\\alpha\\Vert w \\Vert\\_2^2$$\n\n$$J(\\theta) = {\\frac{1}{2m}}\\left[\\sum^m\\_{i=1}(h\\_\\theta(x^{(i)})-y^{(i)})^2+\\lambda\\sum^{n}\\_{j=1}\\theta^2\\_j\\right]$$\n\n如果发生过拟合， 参数 \\\\(\\theta\\\\) 一般是比较大的值， 加入惩罚项后， 只要控制 \\\\(\\lambda\\\\) 的大小，当 \\\\(\\lambda\\\\) 很大时， \\\\(\\theta\\_1\\\\) 到 \\\\(\\theta\\_n\\\\) 就会很小，即达到了约束数量庞大的特征的目的。\n\n\n\n**作用**\n\nL2正则化可以防止模型过拟合（overfitting）\n\n>拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。\n>\n>因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』。\n\n\n## L0 \n\n**L0范数是指向量中非0的元素的个数。**\n\n如果我们用 L0 范数来规则化一个参数矩阵 W 的话，就是希望 W 的大部分元素都是 0，换句话说，就是让参数W是稀疏的。\n\n>通常使参数稀疏都是用 L1 范数实现，L1 范数也有个美称叫“稀疏规则算子”（Lasso regularization）。既然 L0 可以实现稀疏，为什么不用 L0，而要用 L1 呢？个人理解一是因为 L0 范数很难优化求解（NP难问题），二是 L1 范数是 L0 范数的最优凸近似，而且它比L0范数要容易优化求解。\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n","tags":["深度学习"],"categories":["深度学习"]},{"title":"深度学习模型压缩 Model Compression","url":"/2018/06/26/ModelCompression/","content":"\n## 1 前言\n\n目前深度学习模型压缩方法的研究主要可以分为以下几个方向： \n\n- **更精细模型的设计**   \n \n\t目前的很多网络都具有模块化的设计，在深度和宽度上都很大，这也造成了参数的冗余很多，因此有很多关于模型设计的研究，如 `SqueezeNet`、`MobileNet` 等，使用更加细致、高效的模型设计，能够很大程度的减少模型尺寸，并且也具有不错的性能。 \n\t\n- **参数修剪和共享（parameter pruning and sharing）**\n\n\t结构复杂的网络具有非常好的性能，其参数也存在冗余，因此对于已训练好的模型网络，可以寻找一种有效的评判手段，将不重要的 connection 或者 filter 进行裁剪来去除冗余和不重要的项。 \n\t\n\n- **核的稀疏化**\t\n\n\t在训练过程中，对权重的更新进行诱导，使其更加稀疏，对于稀疏矩阵，可以使用更加紧致的存储方式，如 CSC，但是使用稀疏矩阵操作在硬件平台上运算效率不高，容易受到带宽的影响，因此加速并不明显。\n\t\n>稀疏矩阵是指矩阵中的元素大部分是0的矩阵，事实上，实际问题中大规模矩阵基本上都是稀疏矩阵，很多稀疏度在90%甚至99%以上。因此我们需要有高效的稀疏矩阵存储格式。\t\n\n- **权值共享**\n\n\t就是让一些边共用一个权值，达到缩减参数个数的目的。假设相邻两层之间是全连接，每层有 \\\\(1000\\\\) 个节点，那么这两层之间就有 \\\\(1000×1000=100万\\\\) 个权重参数。可以将这一百万个权值做聚类，利用每一类的均值代替这一类中的每个权值大小，这样同属于一类的很多边共享相同的权值，假设把一百万个权值聚成一千类，则可以把参数个数从一百万降到一千个。       \n\t可以直接使用简单的 K-means，对每一层都做一个weight的聚类，属于同一个 cluster 的就共享同一个权值大小。 注意的一点：跨层的weight不进行共享权值；\n\n\n\n- **量化**\n\n\t一般而言，神经网络模型的参数都是用的32bit长度的浮点型数表示，实际上不需要保留那么高的精度，可以通过量化，比如用0~255表示原来32个bit所表示的精度，通过牺牲精度来降低每一个权值所需要占用的空间。\n\t如果我们存储的参数为256个，那么只需要8-bit整数就可以索引，相比于所有位置都存32bit的浮点数，模型的存储量可以下降到原来的1/4。\n\t \n除此之外，**Low-rank分解**、**迁移学习**等方法也有很多研究，并在模型压缩中起到了非常好的效果。\n\n\n## 2 网络剪枝\n\n### 方式 一\n\n>论文地址：[Sparsifying Neural Network Connections for Face Recognition](http://arxiv.org/abs/1512.01891)\n\n#### **算法流程**\n\n从网络的最后一层开始，根据一定规则对该层进行剪枝，然后retrain网络，循环上述过程。\n\n**剪枝的实现方法**\n\n就是为权重施加一个相同大小的Mask, Mask中只有激活的地方才是1，其余全0.\n\n<div align=center>\n![](/img/2018-06-26-ModelCompression-1.jpg)\n<center><small><font color=gray> 算法流程图   </font></small></center>\n</div>\n\n#### **全连接层剪枝**\n\n对于如全连接和局部连接这些没有权值共享的层，我们可以很简单的计算神经元之间的相关性：\n\n假设  \\\\(a\\_i\\\\)  是当前层的一个神经元， 上一层有  \\\\(K\\\\)  个神经元，则此时 \\\\(a\\_i\\\\) 与上一层之间应该有 \\\\(K\\\\) 个连接，即 \\\\(K\\\\) 个权重参数：  \\\\(b\\_{i1},b\\_{i2} … b\\_{iK}\\\\)  。 于是我们可以用下式计算 \\\\(a\\_i\\\\) 与每一个  \\\\(b\\_{ik}\\\\)  的相关系数 ：\n\n$$r\\_{ik} = \\frac{E[a\\_i-\\mu\\_{ai}][\\_i-\\mu\\_{bik}]}{\\sigma\\_{ai}\\sigma\\_{bik}}$$\n\n其中  \\\\(μ\\\\)  和  \\\\(\\sigma\\\\) 分别是在验证集上计算得到的均值与方差。\n\n**正相关和负相关同样重要，而且实验发现保留一些相关性较小的权重也会提高实验效果。**\n\n于是，作者首先将所有正相关的  \\\\(r\\_{ik}\\\\)  降序排列，然后均分为两部分，在前一部分随机采样  \\\\(λSK+\\\\)  个，在后面一部分随机采样  \\\\((1−λ)SK+\\\\)  个, 其中  \\\\(S\\\\)  为事先确定的稀疏度，  \\\\(λ\\\\) 文中设定为 \\\\(0.75\\\\)  。对负相关采取同样操作。据此，我们可以创建出表示剪枝的掩膜矩阵。\n\n#### **卷积层剪枝**\n\n卷积层剪枝稍微复杂一点，因为存在权值共享。\n\n设  \\\\(a\\_{im}\\\\)  是当前层第  \\\\(i\\\\)  个 feature map 中的第  \\\\(m\\\\)  神经元，该 feature map 中的共有  \\\\(M\\\\)  个神经元（ \\\\(m=1,2,...,M\\\\) ）。显然，根据卷积规则，这  \\\\(M\\\\)  个神经元都只与一个卷积核有关，即  \\\\(K\\\\)  个权值有关 (  \\\\(K\\\\)  为 filter size，例如  \\\\(3 × 3\\\\)，再乘以输入 channel 的数量)。\\\\(b\\_{mk}\\\\)  应该是上一层 feature map 中卷积的部分，该部分的位置与  \\\\(m\\\\)  有关，且包含  \\\\(K\\\\)  个元素（ \\\\(k=1,2,...,K\\\\) ）。\n\n最后，相关系数通过平均的方式计算：\n\n$$r\\_{ik}=\\sum\\_{m=1}^M \\left\\vert \\frac{E[a\\_{im}−μ\\_{aim}][b\\_{mk}−μ\\-{bmk}]}{σ\\_{aim}σ\\_{bmk}} \\right\\vert$$\n\n#### 实验结果\n\n下面是在LFW人脸验证的实验（整个实验都没有去碰卷积层，因为对于作者所用的VGG来说，全连接占据了90%的参数量）：\n\n<div align=center>\n![](/img/2018-06-26-ModelCompression-2.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n<div align=center>\n![](/img/2018-06-26-ModelCompression-3.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n-  \\\\(1/256\\\\)  表示稀疏度\n-  默认为主要选择相关性高的，小部分为相关性小的\n-   \\\\(r\\\\) 表示全部随机选择\n-    \\\\(h\\\\) 表示只选择相关性高的\n\n可以看出：**权重的幅值并不能很好地指示权重的重要性。**\n\n### 方式 二\n\n>论文地址：[Pruning Filters for Efficient ConvNets](https://arxiv.org/abs/1608.08710) \n\n\n\n这篇论文中，**作者提出对卷积层进行完全的剪枝。**对第 \\\\(k\\\\) 个卷积输出层进行剪枝，不仅影响当前的卷积层输出，也会影响接下来的网络层，也就是对于之后的网络层没有了原始输入中的第 \\\\(k\\\\) 个。\n\n<div align=center>\n![](/img/2018-06-26-ModelCompression-4.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n这篇论文对卷积窗口的贡献度排序的方法很简单，所采用的的排序指标为**卷积窗口经 L1 正则化的权重参数**。\n\n\n<div align=center>\n![](/img/2018-06-26-ModelCompression-5.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n对卷积窗口剪枝的迭代过程中，每一轮迭代会将全部的卷积窗口进行排序（排序指标为卷积核中 L1 正则化的权重参数），舍弃排序后指标最低的 m 个卷积窗口以达到剪枝的目的，然后用剪枝后的卷积窗口进行模型训练，再不断地重复这个过程。\n\n\n#### 剪枝的敏感度（Sensitivity）\n\n敏感度指每一卷积层进行单独剪枝，查看在 validation set 上准确度的变化\n\n对于 VGG-16, 一些卷积层的 filter 数量是一样的，所以对于差不多  Sensitivity 的卷积层，使用相同的比例进行剪枝，而对于 Sensitivity  比较大的，选择最小的比例进行剪枝或者不进行剪枝\n\n<div align=center>\n![](/img/2018-06-26-ModelCompression-6.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n#### 多层剪枝的策略\n\n之前的一些剪枝策略是逐层剪枝，然后进行retraining，但是这样是非常耗时的\n\n**两种策略：** \n\n- 独立剪枝：就是每一层是独立的，然后进行剪枝\n- 贪心剪枝：就是考虑到上一层被剪掉的情况\n\n如下图，第一种方法就是不考虑已经前面已经移除的filters（蓝色的），黄色的kernel仍然参与计算；而对于贪心剪枝就不用计算黄色的kernel。\n\n<div align=center>\n![](/img/2018-06-26-ModelCompression-7.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n### 方式 三\n\n>**对接近0的权重进行剪枝**\n\n1. 首先对正常模型进行训练\n2. 然后对接近 0 的权值设置 mask，使其相当于0\n3. 最后进行小型的 retrain\n\n项目地址：\n\n1. [pruning_with_tensorflow](https://github.com/ex4sperans/pruning_with_tensorflow)\n2. [impl-pruning-TF](https://github.com/garion9013/impl-pruning-TF)\n\n\n<br/>\n## 3 模型蒸馏\n\n>论文地址：[Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)\n\n模型蒸馏直接设计了一个简单结构的小网络，那小网络的准确率怎么和大网络比呢？    \n\n>模型蒸的主要思想是用预训练好的网络(通常结构较复杂，准确率较高)，来指导小网络的训练，并使小网络达到与复杂网络相近的准确率。\n\n大网络类比于老师，小网络类比于学生，老师经过漫长时间的“训练”摸索出一套适用于某个任务的方法，于是将方法提炼成“知识”传授给学生，帮助学生更快地学会处理相似的任务。\n\n\n\n整个思想中最大的难题在于如何有效地表达“知识”，并有效地指导小网络的训练。其整体结构如下图所示：\n\n<div align=center>\n![](/img/2018-06-26-ModelCompression-8.jpg)\n<center><small><font color=gray> 模型蒸馏结构  </font></small></center>\n</div>\n\n整个网络的损失函数包括原本任务的损失函数，和大网络对小网络的指导损失函数，其中指导损失函数为每个网络块输出特征图的均方误差，如下式所示：\n\n$$L\\_{TS}={\\frac{1}{2}}\\Vert{u\\_{Teacher}-r\\_{student}}\\Vert ^2$$\n\n-  \\\\(L\\_{TS}(Block)\\\\) 表示指导损失函数\n-  \\\\(u\\_{Teacher}\\\\) 表示大网络输出特征图\n-  \\\\(r\\_{student}\\\\) 表示小网络的输出特征图\n\n**整体网络的损失函数如下式所示：**\n\n$$L\\_{total} =\\lambda L\\_{orig}+（1-\\lambda） L\\_{TS}$$\n\n- \\\\(L\\_{orig}\\\\) 为直接训练网络的损失函数\n- \\\\(\\lambda\\\\) 为提前设定的超参数，表示大网络对小网络指导损失函数的重要性\n\n\n**对于 \\\\(\\lambda\\\\)  的取值：**\n\n- 当 \\\\(\\lambda\\\\) 过小时，总损失函数与原损失函数几乎相同\n- 当 \\\\(\\lambda\\\\) 过大时，总损失函数与指导损失函数几乎相同，每次迭代的参数更新值几乎全部取决于指导损失函数，这种训练将完全陷入模仿训练误区。此时，小网络学习重点偏向于模仿大网络而忽略了任务本身，导致实际训练效果下降甚至发生错误。\n- 推荐 \\\\(0.1至0.5\\\\) \n\n>如果先单独对指导损失函数进行训练，然后再加入任务损失函数联合训练，得到的模型效果**可能**将会比直接联合训练得到的模型好很多。\n\n\n\n<div align=center>\n![](/img/2018-06-26-ModelCompression-9.jpg)\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n**细节部分**， \\\\(softmax\\\\) 层的公式如下：\n \n $$q\\_{i}=\\frac{exp(\\frac {z\\_i}{T})}{\\sum\\_j exp(\\frac{z\\_j}{T})}$$\n\n- \\\\(z\\_i : \\mathrm {the\\ logit,\\ i.e.\\ the\\ input\\ to\\ the\\ softmax\\ layer}\\\\)\n- \\\\(q\\_i : \\mathrm {the\\ class\\ probability\\ computed\\ by\\ the\\ softmax}\\\\) \n- \\\\(T : \\mathrm {a\\ temperture\\ that\\ is\\ normally\\ set\\ to\\ 1}\\\\) \n\n \n\\\\(T\\\\) 就是调节参数，一般设为 1。 \\\\(T\\\\) 越大，分类的概率分布越“软” \n\t\n**“蒸馏”最简单的形式就是：**以从复杂模型得到的“软目标”为目标（这时T比较大），用“转化”训练集训练小模型。训练小模型时T不变仍然较大，训练完之后T改为1。\t\n\n**[代码实现](https://github.com/chengshengchan/model_compression/blob/master/teacher-student.py)**\n\n```python\nteacher=nin()\nstudent=lenet()\n\none_hot = tf.one_hot(y, n_classes,1.0,0.0)\nteacher_tau = tf.scalar_mul(1.0/args.tau, teacher)\nstudent_tau = tf.scalar_mul(1.0/args.tau, student)\nobjective1 = tf.nn.sigmoid_cross_entropy_with_logits(student_tau, one_hot)\nobjective2 = tf.scalar_mul(0.5, tf.square(student_tau-teacher_tau))\ntf_loss = (args.lamda*tf.reduce_sum(objective1) + (1-args.lamda)*tf.reduce_sum(objective2))/batch_size\n```\n\n**项目地址：**\n\n1. [Distilling-the-knowledge-in-neural-network](https://github.com/a7b23/Distilling-the-knowledge-in-neural-network)\n2. [model_compression](https://github.com/chengshengchan/model_compression)\n\n\n<br/>\n\n## 参考\n\n- [DEEP COMPRESSION](https://arxiv.org/pdf/1510.00149.pdf)\n\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n","tags":["深度学习"],"categories":["深度学习"]},{"title":"python 进度条包 tqdm 的使用","url":"/2018/06/25/tqdm/","content":"\n\n### 方式 1\n\n```python\nfrom tqdm import tqdm\nfrom time import sleep\n\nfor i in tqdm(range(1000)):\n    sleep(0.01)\n```\n\n### 方式 2\n\n```python\ntext = \"\"\nfor char in tqdm([\"a\", \"b\", \"c\", \"d\"]):\n    text = text + char\n```\n\n\n### 方式 3\n\n```python\nfor i in trange(100): \n   print(i)\n```\n\n### 方式 4 \n\n```python\npbar = tqdm([\"a\", \"b\", \"c\", \"d\"])\nfor char in pbar:\n    pbar.set_description(\"Processing %s\" % char)\n```\n\n### 应用\n\n\n```python\nprint('epoch', epoch)\ntraining_steps = tqdm(\n    range(1, FLAGS.steps_per_epoch),\n    initial=1, total=FLAGS.steps_per_epoch\n)\n\n# main training loop\nfor step in training_steps:\n\n    _, batch_loss, batch_accuracy = sess.run([\n        ops['optimize'], ops['log_loss'], ops['accuracy']\n    ])\n    running_loss += batch_loss\n    running_accuracy += batch_accuracy\n\n```","tags":["python 模块"],"categories":["python"]},{"title":"CNN 优化网络总结","url":"/2018/06/24/youhua/","content":"\n## 新技术\n\n### 深度可分离卷积\n\n>**将标准卷积分解成一个深度卷积和一个点卷积（1 × 1卷积核）**\n\n\n\n  ![](/img/2018-06-24-youhua-101.jpg)\n<div align=center>\n\n<center><small><font color=gray>    </font></small></center>\n</div>\n \n\n直观上来看，这种分解在效果上确实是等价的。   \n \n比如，把上图的代号化为实际的数字，输入图片维度是 \\\\(11 × 11 × 3\\\\) ，标准卷积为 \\\\(3 × 3 × 3 ×16\\\\) （假设stride为 2，padding为 1），那么可以得到输出为 \\\\(6 × 6 × 16\\\\) 的输出结果。   \n  \n现在输入图片不变，先通过一个维度是 \\\\(3 × 3 × 1 × 3\\\\) 的深度卷积（输入是3通道，这里有3个卷积核，对应着进行计算，理解成for循环），得到 \\\\(6 × 6 × 3\\\\) 的中间输出，然后再通过一个维度是 \\\\(1 × 1 × 3 ×16\\\\) 的 \\\\(1 ×1\\\\) 卷积，同样得到输出为 \\\\(6 × 6 × 16\\\\) 。\n\n**运算量：**\n\n标准卷积：\n\n$$D\\_K · D\\_K · M · N · D\\_F · D\\_F$$\n\n深度卷积：\n\n$$D\\_K·D\\_K·M·D\\_F·D\\_F$$\n\n点卷积：\n\n$$M·N·D\\_F·D\\_F$$\n\n深度可分离卷积：\n\n$$D\\_K·D\\_K·M·D\\_F·D\\_F+M·N·D\\_F·D\\_F$$\n\n深度可分离卷积与标准卷积之比：\n\n$$\\frac{D\\_K·D\\_K·M·D\\_F·D\\_F+M·N·D\\_F·D\\_F}{D\\_K · D\\_K · M · N · D\\_F · D\\_F} = {\\frac{1}{N}}+{\\frac{1}{D\\_K^2}}$$\n\n使用大量的 \\\\(3 × 3\\\\) 的卷积核，极大地减少了计算量（原来的 1/ 8到 1/9 之间）\n\n**举例：**\n\n给定输入图像的为3通道的 \\\\(224×224\\\\) 的图像，VGG16网络的第3个卷积层conv2\\_1输入的是尺寸为112的特征图，通道数为64，卷积核尺寸为3，卷积核个数为128\n\n传统卷积运算量就是：\n\n$$3×3×128×64×112×112=924844032$$\n\ndeep-wise 结合  \\\\(1×1\\\\)  方式的卷积的运算量：\n\n$$112×112×3×3×64 + 128×64×112×112 = 109985792$$\n\n两者比例为：\n\n$$\\frac{109985792}{924844032} = 0.1189$$\n\n**优点：**\n\n可以看出运用深度可分离卷积比普通卷积减少了所需要的参数。重要的是深度可分离卷积将以往普通卷积操作同时考虑通道和区域改变成，_**卷积先只考虑区域，然后再考虑通道。实现了通道和区域的分离。**_\n\n<br/>\n\n### 群卷积\n\n我们假设上一层的输出 feature map 有 \\\\(N\\\\) 个，即通道数 \\\\(channel=N\\\\) ，也就是说上一层有 \\\\(N\\\\) 个卷积核。再假设群卷积的群数目 \\\\(M\\\\) 。那么该群卷积层的操作就是，先将 channel 分成 \\\\(M\\\\) 份。每一个 group 对应 \\\\(N/M\\\\) 个 channel，与之独立连接。然后各个 group 卷积完成后将输出叠在一起（concatenate），作为这一层的输出 channel。\n\n\n![](/img/2018-06-24-youhua-1.jpg)\n\n<div align=center>\n\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n下图则是一个群卷积的 CNN 结构。filters 被分成了两个 group。每一个 group 都只有原来一半的 feature map\n\n![](/img/2018-06-24-youhua-2.jpg)\n<div align=center>\n\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n\n可以看到，图中将输入数据分成了 2 组（组数为 \\\\(g\\\\) ），需要注意的是，**这种分组只是在深度上进行划分，即某几个通道编为一组，**这个具体的数量由 \\\\(C\\_1/g\\\\) 决定。 \n    \n因为输出数据的改变，相应的，卷积核也需要做出同样的改变。即每组中卷积核的深度也就变成了 \\\\(C\\_1/g\\\\) ，而卷积核的大小是不需要改变的，此时每组的卷积核的个数就变成了 \\\\(C\\_2/g\\\\) 个，而不是原来的 \\\\(C\\_2\\\\) 了。然后用每组的卷积核同它们对应组内的输入数据卷积，得到了输出数据以后，再用 concatenate 的方式组合起来，最终的输出数据的通道仍旧是 \\\\(C\\_2\\\\) 。     \n\n也就是说，分组数 \\\\(g\\\\) 决定以后，那么我们将并行的运算 \\\\(g\\\\) 个相同的卷积过程，每个过程里（每组），输入数据为 \\\\(H\\_1×W\\_1×C\\_1/g\\\\) ，卷积核大小为 \\\\(h\\_1×w\\_1×C\\_1/g\\\\) ，一共有 \\\\(C\\_2/g\\\\) 个，输出数据为 \\\\(H\\_2×W\\_2×C\\_2/g\\\\) \n\n<br/>\n\n## 1. MobileNet\n\n>**论文地址：**[MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)    \n>**发布时间**：2017 年 4 月     \n>**官方代码：**[tensorflow/model/sresearch/slim/nets/mobilenet/](https://github.com/tensorflow/models/tree/5bb9e6f349e22270420dd637f3fa89260ab5b441/research/slim/nets/mobilenet)     \n>**民间实现：**[caffe](https://github.com/shicai/MobileNet-Caffe) | [Tensorflow](https://github.com/Zehaos/MobileNet) \n\n\nMobileNets结构建立在上述深度可分解卷积中（只有第一层是标准卷积）。该网络允许我们探索网络拓扑，找到一个适合的良好网络。其具体架构在表1说明。除了最后的全连接层，所有层后面跟了batchnorm和ReLU，最终输入到softmax进行分类。二者都附带了BN和ReLU层。\n\n按照作者的计算方法，MobileNets 总共 28 层 \\\\(（1 + 2 × 13 + 1 = 28）\\\\) 。\n\n![](/img/2018-06-24-youhua-3.jpg)\n<div align=center>\n<center><small><font color=gray>  对比标准卷积和分解卷积的结构, 表格中含有dw的就表示这一层采用了 deep-wise 结合 1x1 的方式  </font></small></center>\n</div>\n\n\n![](/img/2018-06-24-youhua-102.jpg)\n<div align=center>\n<center><small><font color=gray>  传统卷积（左）与深度可分离卷积（右）在 MobileNet 中的区别   </font></small></center>\n</div>\n\n\n>deep-wise 的卷积和后面的  \\\\(1×1\\\\)  卷积被当成了两个独立的模块，都在输出结果的部分加入了 Batch Normalization 和非线性激活单元\n\n\n下图为 MobileNet 对于不同结构单元在计算量和参数数量方面的统计:\n ![](/img/2018-06-24-youhua-103.jpg)\n<div align=center>\n\n<center><small><font color=gray>  在MobileNet中，有95%的计算量和75%的参数属于 \\\\(1×1\\\\) 卷积    </font></small></center>\n</div>\n\n\n\n\n<br/>\n### 训练细节\n\n作者基于 TensorFlow 训练 MobileNet，使用 RMSprop 算法优化网络参数。考虑到较小的网络不会有严重的过拟合问题，因此没有做大量的数据增强工作。在训练过程中也没有采用训练大网络时的一些常用手段，例如：辅助损失函数，随机图像裁剪输入等。\n\ndeep-wise 卷积核含有的参数较少，作者发现这部分最好使用较小的 weight decay或者不使用 weightdecay。\n\n<br/>\n### 宽度因子和分辨率因子\n\n>尽管标准的MobileNet在计算量和模型尺寸方面具备了很明显的优势，但是，在一些对运行速度或内存有极端要求的场合，还需要更小更快的模型，如何能够在不重新设计模型的情况下，以最小的改动就可以获得更小更快的模型呢？\n\n>本文中提出的**宽度因子（width multiplier）**和**分辨率因子（resolution multiplier）**就是解决这些问题的配置参数。\n\n**宽度因子**  \\\\(\\alpha \\\\) 是一个属于 \\\\((0,1]\\\\) 之间的数，附加于网络的通道数。简单来说就是新网络中每一个模块要使用的卷积核数量相较于标准的 MobileNet 比例。\n\n输入通道数  \\\\(M\\\\)  变为  \\\\(\\alpha M\\\\) ;     \n输出通道数 \\\\(N\\\\) 变为  \\\\(\\alpha N\\\\) \n\n对于 deep-wise 结合  \\\\(1x1\\\\)  方式的卷积核，计算量为：\n\n$$D\\_K·D\\_K·\\alpha M·D\\_F·D\\_F+\\alpha M·\\alpha N·D\\_F·D\\_F$$\n\n \\\\(\\alpha \\\\) 常用的配置为 \\\\(1, 0.75, 0.5, 0.25\\\\) 。当 \\\\(\\alpha \\\\) 等于1时就是标准的 MobileNet。通过参数 \\\\(\\alpha \\\\) 可以非常有效的将计算量和参数数量约减到 \\\\(\\alpha \\\\) 的平方倍。\n\n\n通过下图可以看出使用 \\\\(\\alpha \\\\) 系数进行网络参数的约减时，在ImageNet上的准确率，为准确率，参数数量和计算量之间的权衡提供了参考（每一个项中最前面的数字表示 \\\\(\\alpha \\\\) 的取值）\n\n![](/img/2018-06-24-youhua-4.jpg)\n<div align=center>\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n<br/>\n\n**分辨率因子** \\\\(\\beta\\\\) 的取值范围在 \\\\((0,1]\\\\) 之间，是作用于每一个模块输入尺寸的约减因子，简单来说就是将输入数据以及由此在每一个模块产生的特征图都变小了，\n\n结合宽度因子 \\\\(\\alpha \\\\) ，deep-wise 结合 \\\\(1x1\\\\) 方式的卷积核计算量为：\n\n$$D\\_K·D\\_K·\\alpha M·\\beta D\\_F·\\beta D\\_F+\\alpha M·\\alpha N·\\beta D\\_F·\\beta D\\_F$$\n\n下图为使用不同的 \\\\(\\beta\\\\) 系数作用于标准 MobileNet 时，对精度和计算量以的影响（224，192，160，128指的是输入分辨率）:\n\n\n![](/img/2018-06-24-youhua-5.jpg)\n<div align=center>\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n>要注意再使用宽度和分辨率参数调整网络结构之后，都要从随机初始化重新训练才能得到新网络\n\n<br/>\n### 实验结果\n\n**1) 基础网络**\n\nMobileNet极大地降低了网络参数数量和计算量，但是相比起经典的基础网络，其精度并未明显的降低\n\n![](/img/2018-06-24-youhua-6.jpg)\n<div align=center>\n\n<center><small><font color=gray>   与VGG相比，在ImageNet分类任务上的精度差距较小   </font></small></center>\n</div>\n\n\n![](/img/2018-06-24-youhua-7.jpg)\n<div align=center>\n\n<center><small><font color=gray> 与经典的小型网络SqueezeNet相比，在精度和计算量方面都有明显提升   </font></small></center>\n</div>\n\n\n\n**2) 人脸属性提取**\n\nMobileNet 的一个使用情景是压缩具有未知或复杂训练程序的大型系统。在人脸属性分类任务中，本文证明了 MobileNet 与 distillation 间的协同关系，这是网络的一种知识迁移技术。本文在 YFCC100M 多属性数据集上训练。\n\n使用 MobileNet 架构提取一个人脸属性分类器。distillation是通过训练分类器模拟一个更大的模型的输出，而非人工标注标签工作，因此能够从大型（可能是无限大）未标记数据集训练。\n\n\n![](/img/2018-06-24-youhua-8.jpg)\n<div align=center>\n\n<center><small><font color=gray>  结合 distillation 的可扩展性和MobileNet 的简约参数化，相比于一个具有7500万超参数和16亿 Mult-Adds 的大型人脸属性分类器，终端系统不仅不需要正则化，而且表现出更好的性能  </font></small></center>\n</div>\n\n**3) 人脸识别**\n\nFaceNet 模型是目前顶级水平的人脸识别模型，它基于 triplet loss 构建 faceEmbedding。为了能够在移动设备上运行 FaceNet 模型，使用 distillation来最小化训练数据上 FaceNet和 MobileNet 输出的方差。\n\n下图列出了输入尺寸较小的MobileNet在此任务上的性能表现。\n\n![](/img/2018-06-24-youhua-9.jpg)\n<div align=center>\n<center><small><font color=gray>  在没有过分损失精度的情况下，运算量大大减少  </font></small></center>\n</div>\n\n\n**4) 目标检测**\n\nMobileNet 可以作为一个有效的基网络部署在目标检测系统上。基于2016 COCO 数据集，比较了在 COCO 数据上训练的 MobileNet 进行目标检测的结果。   \n\n下图列出了在 Faster-RCNN 和 SSD 框架下，MobileNet，VGG 以及 Inception V2 作为基础网络的对比结果。\n\n![](/img/2018-06-24-youhua-10.jpg)\n<div align=center>\n<center><small><font color=gray>  在不同而检测框架和输入尺寸设定下，以MobileNet为基础网络的检测框架表现出了不明显逊色于两个基础网络的性能，而且在计算量和模型尺寸方面有较大优势  </font></small></center>\n</div>\n\n\n\n\n\n<br/>\n### 总结\n\nMobileNet在计算量，存储空间和准确率方面取得了非常不错的平衡。与VGG16相比，在很小的精度损失情况下，将运算量减小了30倍。通过实验结果，我们认为MobileNet的设计思想会在自动驾驶汽车，机器人和无人机等对实时性、存储空间、能耗有严格要求的终端智能应用中发挥显著作用。\n\n\n## 2. Xception\n\n>论文地址：[Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/abs/1610.02357)    \n>发布时间：2016 年 10 月\n\n\n\n## 3. ResNeXt\n\n>论文地址：[Aggregated Residual Transformations for Deep Neural Networks](https://arxiv.org/abs/1611.05431)      \n>发布时间：2016 年 11 月（v1）   2017 年 4 月 （v2）\n\n\n\ncardinality，原文的解释是 the size of the set of transformations。分支的个数就是 cardinality\n\n**增加 cardinality 比增加深度和宽度更有效**\n\n![](/img/2018-06-24-youhua-11.jpg)\n<div align=center>\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n计算一下便可得：由 Figure1 得知， \n\nresnet 的参数量为： \n$$256×64+3×3×64×64+64×256=70k $$\nResNeXt 的参数量为： \n\n$$C×(256×d+3×3×d×d+d×256)$$\n\n当 C 取 \\\\(32\\\\) ， \\\\(d=4\\\\) 时，上式也等于 \\\\(70k\\\\) 。 \n\n![](/img/2018-06-24-youhua-104.jpg)\n<div align=center>\n\n<center><small><font color=gray>  在每个conv中，总的通道数要比Resnet多的多，但是两者的参数量是一样的   </font></small></center>\n</div>\n\n\n![](/img/2018-06-24-youhua-12.jpg)\n<div align=center>\n\n<center><small><font color=gray>  三种等价设计  </font></small></center>\n</div>\n\n\n\n## 4. ShuffleNet\n\n>论文地址：[ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices](https://arxiv.org/abs/1707.01083)     \n>**发布时间：**2017 年 7 月（v1）    2017 年 12 月（v2）\n\n其中，最先进的网络例如 Xception 和 ResNeXt 将有效的深度可分离卷积或群卷积引入构建 block 中，在表示能力和计算消耗之间取得很好的折中。但是，我们注意到这**两个设计都没有充分采用 \\\\(1×1\\\\) 的逐点卷积**，因为这需要很大的计算复杂度。例如，在 ResNeXt 中 \\\\(3×3\\\\) 卷积配有群卷积，逐点卷积占了 93.4% 的 multiplication-adds。\n\n\n**在小型网络中，昂贵的逐点卷积造成有限的通道之间充满约束，这会显著的损失精度。**为了解决这个问题，一个直接的方法是**应用通道稀疏连接，例如组卷积(group convolutions)**。通过确保每个卷积操作仅在对应的输入通道组上，组卷积可以显著的降低计算损失。\n\n然而，如果多个组卷积堆叠在一起，会有一个副作用： 某个通道输出仅从一小部分输入通道中导出，如下图(a)所示，**这样的属性降低了通道组之间的信息流通，降低了信息表示能力。**\n![](/img/2018-06-24-youhua-13.jpg)\n<div align=center>\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n\n如果我们**允许组卷积能够得到不同组的输入数据**，即上图(b)所示效果，那么**输入和输出通道会是全关联的**。具体来说，对于上一层输出的通道，我们可做一个混洗 (Shuffle) 操作，如上图(c)所示，再分成几个组，feed 到下一层。\n\n对于这个混洗操作，有一个有效高雅(efficiently and elegantly)的实现:\n\n对于一个卷积层分为g组，有一个有效高雅(efficiently and elegantly)的实现:\n\n1. 有 \\\\(g×n\\\\) 个输出通道\n2. reshape 为 \\\\((g,n)\\\\) \n3. 再转置为 \\\\((n,g)\\\\) \n4. 平坦化,再分回 \\\\(g\\\\) 组作为下一层的输入\n\n示意图如下：\n\n![](/img/2018-06-24-youhua-14.jpg)\n<div align=center>\n\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n\n### Shuffle unit\n\n我们已经了解通道混洗的好，而在实际过程中我们构建了一个 **ShuffleNet unit**，便于构建实际模型。\n\n![](/img/2018-06-24-youhua-15.jpg)\n<div align=center>\n\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n-  图(a)是一个残差模块。对于主分支部分，我们可将其中标准卷积3×33×3拆分成深度分离卷积。我们将第一个 \\\\(1×1\\\\) 卷积替换为逐点组卷积，再作通道混洗(即(b))。其中的 DWConv 指的是 depthwise convolution。\n\n-  图(b)即 ShuffleNet unit，相比于图a,只是将第一个 \\\\(1x1\\\\) 卷积改成了 group convolution，同时后续增加通道 shuffle。\n  \n-  图(c)即是做降采样的 ShuffleNet unit，这主要做了两点修改：\n\n  -  在辅分支加入步长为2的 \\\\(3×3\\\\) 平均池化\n  -  原本做元素相加的操作转为了通道级联，这扩大了通道维度，增加的计算成本却很少\n\n归功于逐点群卷积和通道混洗，ShuffleNet unit 可以高效的计算。相比于其他先进的单元，在相同设置下复杂度较低。\n\n例如：给定输入大小 \\\\(h×w×c\\\\) ,通道数为 \\\\(c\\\\) 。对于的 bottleneck 通道为mm:\n\n-  ResNet unit 需要 \\\\(h·w·(2·c·m+9m^2)FLOPS\\\\) 计算量。\n-  ResNeXt 需要 \\\\(h·w(2·c·m+9·m^2/g)FLOPS\\\\) \n-  而 ShuffleNet unit 只需要 \\\\( h·w(2·c·m/g+9·m)FLOPS\\\\) . \n\n其中 \\\\(g\\\\) 代表组卷积数目。即表示：**给定一个计算限制，ShuffleNet 可以使用更宽的特征映射。我们发现这对小型网络很重要，因为小型网络没有足够的通道传递信息**\n\n>**需要注意的是：**虽然深度卷积可以减少计算量和参数量，但在低功耗设备上，与密集的操作相比，计算/存储访问的效率更差。故在ShuffleNet上我们只在bottleneck上使用深度卷积，尽可能的减少开销。\n\n\n\n### 整体架构\n\n在上面的基本单元基础上，我们提出了ShuffleNet的整体架构：\n\n![](/img/2018-06-24-youhua-16.jpg)\n<div align=center>\n<center><small><font color=gray>    </font></small></center>\n</div>\n\n\n\n\n\n### 实验性能\n\n\n![](/img/2018-06-24-youhua-17.jpg)\n<div align=center>\n<center><small><font color=gray> 不同配置下的网络性能对比图   </font></small></center>\n</div>\n\n- “num x”表示 feature map 的 channels 数量与原始结构的比例\n- arch2: 将 stage3 中的两个单元移除，同时增加整体的网络宽度。对于小网络，feture map的宽度至关重要\n\n\n### 总结\n\nShuffleNet 的核心就是用 pointwise group convolution，channel shuffle 和 depthwise separable convolution 代替 ResNet block 的相 应层构成了 ShuffleNet uint，达到了减少计算量和提高准确率的目的。\n\nchannel shuffle 解决了多个 group convolution 叠加出现的边界效应，pointwise group convolution 和 depthwise separable convolution 主要减少了计算量。\n\n\n## 转载与参考\n\n1. [深度解读谷歌MobileNet](https://blog.csdn.net/t800ghb/article/details/78879612)\n2. [mobilenet网络的理解](https://blog.csdn.net/wfei101/article/details/78310226)\n3. [轻量级网络--ShuffleNet论文解读](https://blog.csdn.net/u011974639/article/details/79200559)\n\n\n\n\n\n\n\n\n\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>","tags":["深度学习"],"categories":["深度学习"]},{"title":"计算机视觉基础","url":"/2018/06/22/cv2/","content":"\n\n## 高低频\n\n>**高频和低频就是信号变化的频率**      \n>**图像的频率：灰度值变化剧烈程度的指标，是灰度在平面空间上的梯度**\n  \n<!-- more -->  \n  \n对于声音来说其变化是一维的，是时域信号即信号大小随着时间的变化而变化         \n对于图像来说其变化是二维的，是空域信号即信号随着空间坐标的变化而变化 \n\n所谓高频，是指一个信号的变化速度较快，这是一个相对概念。     \n在图像上来说，就是一片图像的亮度变化较多且明显；      \n在音频领域，是指一个震荡频率较低的声波；       \n在电学领域，是指可以被电感严重阻碍的交变电流。     \n换句话说，**_某一/某些事物将其某一可以量化的性质，按照时间或者不同事物有序列排列为横轴，该性质的数值为纵轴，这样所形成的图像看上去相对变化较大的，就是高频信号。_**低频正好相反。     \n空域的高频可以理解为细节信号，比如当一幅大图缩小n倍后，图像的细节就看不到了，我们就可以理解为高频信号被滤掉了，而剩下的图像部分就可理解为低频了。\n\n\n\n图像的高低频是对图像各个位置之间**强度变化**的一种度量方法.\n\n**低频分量：**主要对整幅图像的强度的综合度量，描述大范围的信息   \n**高频分量：**主要是对图像边缘和轮廓的度量，描述具体的细节\n\n- 如果一副图像的各个位置的强度大小相等，则图像只存在低频分量，从图像的频谱图上看，只有一个主峰，且位于频率为零的位置。\n\n- 如果一副图像的各个位置的强度变化剧烈，则图像不仅存在低频分量，同时也存在多种高频分量，从图像的频谱上看，不仅有一个主峰，同时也存在多个旁峰。\n\n    \n>在灰度图像中，亮度变化小的区域主要是低频成分，而亮度变化剧烈的区域（比如物体的边缘）主要是高频成分。\n>\n>简言之，就是图像中像素灰度值变化快的就是高频部分，变化慢的就是低频部分。\n>\n>另外噪声（即噪点）也是这样,在一个像素所在的位置,之所以是噪点,就是因为它与正常的点颜色不一样了，也就是说该像素点灰度值明显不一样了,,也就是灰度有快速地变化了,所以是高频部分，因此有噪声在高频这么一说\n\n<br/>\n\n\n## 滤波\n\n>**图像其实是一种波，可以用波的算法处理图像。**\n\n\n\n ![](/img/2018-06-22-cv2-101.jpg)\n<center><small><font color=gray> 这是一张 400 x 400 的图片，一共包含了 16 万个像素点。   </font></small></center>\n\n如果把每一行所有像素（上例是400个）的红、绿、蓝的值，依次画成三条曲线，就得到了下面的图形。\n\n![](/img/2018-06-22-cv2-1.jpg)\n\n可以看到，每条曲线都在不停的上下波动。有些区域的波动比较小，有些区域突然出现了大幅波动（比如 54 和 324 这两点）。\n\n对比一下图像就能发现，曲线波动较大的地方，也是图像出现突变的地方。\n\n\n![](/img/2018-06-22-cv2-2.jpg)\n\n这说明波动与图像是紧密关联的。**图像本质上就是各种色彩波的叠加。**\n\n<br/>\n\n<font size=\"+1\">**低通滤波**</font>\n\n>**低通滤波器（lowpass）：减弱或阻隔高频信号，保留低频信号**\n\n\n![](/img/2018-06-22-cv2-3.jpg)\n<center><small><font color=gray> 蓝线是原始的波形，绿线是低通滤波lowpass后的波形。可以看到，绿线的波动比蓝线小很多，非常平滑   </font></small></center>\n\n<br/>\n\n<font size=\"+1\">**高通滤波**</font>\n\n>**高通滤波器（highpass）：减弱或阻隔低频信号，保留高频信号**\n\n\n![](/img/2018-06-22-cv2-4.jpg)\n<center><small><font color=gray>  黄线是原始的波形，蓝线是高通滤波highpass后的波形。可以看到，黄线的三个波峰和两个波谷（低频波动），在蓝线上都消失了，而黄线上那些密集的小幅波动（高频波动），则是全部被蓝线保留  </font></small></center>\n\n\n![](/img/2018-06-22-cv2-5.jpg)\n<center><small><font color=gray> 上图有三根曲线，黄线是高频波动，红线是低频波动。它们可以合成为一根曲线，就是绿线   </font></small></center>\n\n![](/img/2018-06-22-cv2-6.jpg)\n<center><small><font color=gray> 上图中，绿线进行低通滤波和高通滤波后，得到两根黑色的曲线，它们的波形跟原始的黄线和红线是完全一致的   </font></small></center>\n\n<br/>\n\n<font size=\"+1\">**对图像进行滤波**</font>\n\n\n>lowpass 使得图像的高频区域变成低频，即色彩变化剧烈的区域变得平滑，也就是出现模糊效果\n\n![](/img/2018-06-22-cv2-7.jpg)\n\n![](/img/2018-06-22-cv2-8.jpg)\n\n<center><small><font color=gray>  上图中，红线是原始的色彩曲线，蓝线是低通滤波后的曲线  </font></small></center>\n\n\n\n>highpass 正好相反，过滤了低频，只保留那些变化最快速最剧烈的区域，也就是图像里面的物体边缘，所以常用于边缘识别\n\n\n![](/img/2018-06-22-cv2-9.jpg)\n\n![](/img/2018-06-22-cv2-10.jpg)\n<center><small><font color=gray>   红线是原始的色彩曲线，蓝线是高通滤波后的曲线 </font></small></center>\n\n下面这个[网址](http://fellipe.com/demos/lena-js/)，可以将滤波器拖到图像上，产生过滤后的效果。\n\n<br/>\n\n<font size=\"+1\">**高斯滤波**</font>\n\n>高斯滤波器是一种线性平滑滤波器，能够有效的抑制噪声，平滑图像。    \n>其作用原理和均值滤波器类似，都是取滤波器窗口内的像素的均值作为输出。其窗口模板的系数和均值滤波器不同，均值滤波器的模板系数都是相同的为1；而高斯滤波器的模板系数，则随着距离模板中心的增大而系数减小。所以，高斯滤波器相比于均值滤波器对图像个模糊程度较小\n\n离散化窗口滑窗卷积的时，主要利用的是高斯核，高斯核一般是一个奇数的大小的高斯模板\n\n一个二维的高斯函数如下：    \n<font size=\"+1\">$$G(x,y)={\\frac{1}{2\\pi\\sigma^2}}e^{-{\\frac {x^2+y^2}{2\\sigma^2}}}$$</font>\n\n\n>其中 \\\\((x,y)\\\\) 为点坐标，在图像处理中可认为是整数； \\\\(σ\\\\) 是标准差     \n>\n>标准差越小，二维高斯图像越窄小，平滑效果不明显；标准差越大，而为高斯图像越矮宽，滤波效果比较明显\n\n要想得到一个高斯滤波器的模板，可以对高斯函数进行离散化，得到的高斯函数值作为模板的系数。     \n\n例如：要产生一个 \\\\(3×3\\\\) 的高斯滤波器模板，以模板的中心位置为坐标原点进行取样。模板在各个位置的坐标，如下所示（ \\\\(x\\\\) 轴水平向右， \\\\(y\\\\) 轴竖直向下）\n\n\n ![](/img/2018-06-22-cv2-102.jpg)\n\n最后得到高斯模板：\n\n![](/img/2018-06-22-cv2-11.jpg)\n<center><small><font color=gray> 左侧是常用的3*3的高斯模板，右侧是常用的5*5高斯模板   </font></small></center>\n\n>**归一化处理**\n>\n>为了尽可能的模拟高斯函数的相关性质，计算高斯模板的时候，计算出来的高斯模板中各个数值其和必须为1\n>\n>另外一种解释是：归一化之后，通过卷积计算出来的模板中心像素被限制到了0-255的灰度区间中。假若某一邻域内所有像素的灰度值为255，利用该模板进行卷积之后，求得的模板中心像素灰度值仍然为255；假若计算出来的高斯模板参数之和小于1，那么通过该模板进行卷积之后，模板中心像素的灰度值将小于255，偏离了实际的灰度值，产生了误差。\n\n\n ![](/img/2018-06-22-cv2-103.jpg)\n\n\n**高斯滤波性质**\n\n　　高斯函数具有五个重要的性质，这些性质使得它在早期图像处理中特别有用．这些性质表明，高斯平滑滤波器无论在空间域还是在频率域都是十分有效的低通滤波器，且在实际图像处理中得到了工程人员的有效使用．高斯函数具有五个十分重要的性质，它们是： \n\n1. 二维高斯函数具有**旋转对称性**，即滤波器在各个方向上的平滑程度是相同的．一般来说，一幅图像的边缘方向是事先不知道的，因此，在滤波前是无法确定一个方向上比另一方向上需要更多的平滑．旋转对称性意味着高斯平滑滤波器**在后续边缘检测中不会偏向任一方向**． \n2. 高斯函数是**单值函数**．这表明，高斯滤波器用像素邻域的加权均值来代替该点的像素值，而每一邻域像素点权值是随该点与中心点的距离单调增减的．这一性质是很重要的，因为边缘是一种图像局部特征，如果平滑运算对离算子中心很远的像素点仍然有很大作用，则平滑运算会使图像失真． \n3. 高斯函数本身这一事实的直接推论．图像常被不希望的高频信号所污染(噪声和细纹理)．而所希望的图像特征（如边缘），既含有低频分量，又含有高频分量．高斯函数付立叶变换的单瓣意味着平滑图像不会被不需要的**高频信号所污染，同时保留了大部分所需信号**． \n4. 高斯滤波器宽度(决定着平滑程度)是由参数σ表征的，而且σ和平滑程度的关系是非常简单的．**σ越大，高斯滤波器的频带就越宽，平滑程度就越好**．通过调节平滑程度参数σ，可在图像特征过分模糊(过平滑)与平滑图像中由于噪声和细纹理所引起的过多的不希望突变量(欠平滑)之间取得折衷． \n5. 由于高斯函数的**可分离性**，较大尺寸的高斯滤波器可以得以有效地实现．二维高斯函数卷积可以分两步来进行，首先将图像与一维高斯函数进行卷积，然后将卷积结果与方向垂直的相同一维高斯函数卷积．因此，二维高斯滤波的计算量随滤波模板宽度成**线性增长**而不是成平方增长．\n\n<br/>\n\n## SIFT\n\n<font size=\"+1\">**图像尺度空间 高斯卷积**</font>\n\n>尺度空间表达指的是不同高斯核所平滑后的图片的不同表达  \n\n平时生活中，用人眼去看一张照片时，随着观测距离的增加，图像会逐渐变得模糊。    \n那么计算机在“看”一张照片时，会从不同的“尺度”去观测照片，尺度越大，图像越模糊。\n\n**“尺度空间表达”指的是不同高斯核所平滑后的图片的不同表达，**    \n意思就是：原始照片的分辨率，和经过不同高斯核平滑后的照片的分辨率是一样的。但是，对于计算机来说，不同模糊程度，照片“看”上去的样子就不一样了。高斯核越大，图片“看”上去就越模糊。\n\n>那么，图片的模糊与找特征点有关系吗？\n\n计算机没有主观意识去识别哪里是特征点，它能做的，只是分辨出变化率最快的点。彩色图是三通道的，不好检测突变点。需要将RGB图转换为灰度图，此时灰度图为单通道，灰度值在0~255之间分布。\n\n高斯卷积之后，图像虽然变模糊了。但是整体的像素没有变，依然可以找到灰度值突变的点。而这些点，就可以作为候选特征点了，后期再进一步减少点的数量，提高准确率即可。\n\n<br/>\n\n<font size=\"+1\"> **金字塔多分辨率表达——降采样** </font>\n\n>图像金字塔化：先进行图像平滑，再进行降采样，根据降采样率不同，所得到一系列尺寸逐渐减小的图像\n\n若对一张图片进行降采样，其像素点就会减少，图片尺寸也会随之变小。那么给人的感觉就好比一个金字塔。\n\n ![](/img/2018-06-22-cv2-104.jpg)\n\n\n<br/>\n\n<font size=\"+1\">  **LOG（Laplassian of Gaussian）**   </font>\n\n\n前面提出的那种表达，各有各的优势：\n\n1. “尺度空间表达”在所有尺度上具有相同分辨率，而“图像金字塔化”在每层的表达上分辨率都会减少固定比率。\n2. “图像金字塔化”处理速度快，占用存储空间小，而“尺度空间表达”刚好相反。\n\n>那么将两者融合起来的话，就得到了LOG图像，高斯拉普拉斯变换图像\n\n**其步骤是：**    \n\n1. 先将照片降采样，得到了不同分辨率下的图像金字塔。    \n2. 再对每层图像进行高斯卷积。这样一来，原本的图像金字塔每层只有一张图像，而卷积后，每层又增加了多张不同模糊程度下的照片。\n\n ![](/img/2018-06-22-cv2-105.jpg)\n\n ![](/img/2018-06-22-cv2-106.jpg)\n\n然而，LOG图像还不是我们想要的，我们做那么多就是为了更好地获取特征点，所以还需要对LOG图像再进一步地优化。所以，DOG图像横空出世！！\n\n<br/>\n\n<font size=\"+1\"> **DOG（Difference of Gaussian）** </font>\n\n>DOG即高斯差分\n\n\n**构造高斯差分图像的步骤是：**在获得LOG图像后，用其相邻的图像进行相减，得到所有图像重新构造的金字塔就是DOG金字塔\n\n ![](/img/2018-06-22-cv2-107.jpg)\n<center><small><font color=gray>  左图是LOG图像，右图是DOG图像  </font></small></center>\n\n<br/>\n\n**1) 寻找极值点**\n\n当得到DOG金字塔后，我们接下来要做的是寻找DOG极值点。每个像素点与其周围的像素点比较，当其大于或者小于所有相邻点时，即为极值点。\n\n ![](/img/2018-06-22-cv2-108.jpg)\n<center><small><font color=gray>  以黄点为检测点，那么其周围的点，除了同层所包围的8个绿点外，还有上一层的9个点与下一层的9个点  </font></small></center>\n\n ![](/img/2018-06-22-cv2-109.jpg)\n\n**2) 去除边缘影响**\n\n有些极值点不是我们想要的，当中就有一大部分是边缘区域产生的极值点。因为物体的边缘轮廓在灰度图中，存在着灰度值的突变，这样的点在计算中就被“误以为”是特征值\n\n边缘区域在纵向上灰度值突变很大\n\n我们想到了Hessian矩阵，海塞矩阵是用来求曲率的，可以以函数的二阶偏导为元素，构成一个2x2的矩阵H\n\n ![](/img/2018-06-22-cv2-110.jpg)\n\n**3) 方向赋值**\n\n当我们精确定位关键点后，需要找到该特征点对应的尺度值 σ，根据这一尺度值，将对应的高斯图像的关键点进行有限差分，以3×1.5σ为半径的区域内图像梯度的幅角和幅值。\n\n然后利用直方图统计领域内像素对应的梯度和幅值：梯度方向角为横轴刻度，取45度为一个单位，那么横轴就有8个刻度；纵轴是对应梯度的幅值累加值。\n\n![](/img/2018-06-22-cv2-12.jpg)\n\n取幅值最高的方向为主方向。有的时候，会出现第二峰值，因为有较多的关键点是多方向的。如果直接把它忽略掉不加以考虑的话，最后对匹配精度的影响还是蛮大的。\n\n所以，为了匹配的稳定性，我们将超过峰值能量的百分之80的方向，称为辅方向。\n\n ![](/img/2018-06-22-cv2-111.jpg)\n\n**4) 关键点描述**\n\n到了这里，我们就已经得到赋值后的SIFT特征点了，其包含了位置，尺度，方向的信息。\n\n接下来的要做的是：关键点的描述，即用一组向量将关键点描述出来。\n\n1. 确定描述子采样区域\n2. 生成描述子\n3. 生成特征匹配点\n4. 对特征向量进行归一化处理，去除光照变化的影响\n\n<br/>\n\n<font size=\"+1\">   **图像相似性**  </font>\n\n当两幅图像的 SIFT 特征向量生成以后，下一步就可以采用关键点特征向量的欧式距离来作为两幅图像中关键点的相似性判定度量。\n\n ![](/img/2018-06-22-cv2-112.jpg)\n\n<br/>\n\n## 转载与参考\n\n1. [图像的高频和低频](https://blog.csdn.net/missingu1314/article/details/8675434)\n2. [为什么说图像的低频是轮廓，高频是噪声和细节](https://blog.csdn.net/charlene_bo/article/details/70877999)\n3. [图像与滤波 - 阮一峰](http://www.ruanyifeng.com/blog/2017/12/image-and-wave-filters.html![](/img/2018-06-22-cv2-13.jpg))\n4. [高斯滤波简介](https://www.cnblogs.com/qiqibaby/p/5289977.html)\n5. [图像处理基础(4)：高斯滤波器详解](https://www.cnblogs.com/wangguchangqing/p/6407717.html)\n6. [SIFT特征匹配算法介绍——寻找图像特征点的原理](https://blog.csdn.net/weixin_38404120/article/details/73740612)\n7. [SIFT](https://baike.baidu.com/item/SIFT/1396275?fr=aladdin)\n\n\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>","tags":["计算机视觉"],"categories":["计算机视觉"]},{"title":"LeetCode 算法题","url":"/2018/05/29/leetcode/","content":"\n\n## 1. Two Sum 两数之和\n\n给定一个整数数组和一个目标值，找出数组中和为目标值的两个数。\n\n你可以假设每个输入只对应一种答案，且同样的元素不能被重复利用。\n\n**示例:**\n\n>给定 nums = [2, 7, 11, 15], target = 9\n>\n>因为 nums[0] + nums[1] = 2 + 7 = 9    \n>所以返回 [0, 1]   \n\n\n<!-- more -->\n\n```python\nclass Solution:  \n    def twoSum(self,nums, target):  \n        \"\"\" \n        :type nums: List[int] \n        :type target: int \n        :rtype: List[int] \n        \"\"\"  \n        #用len()方法取得nums列表长度  \n        n = len(nums)  \n        #创建一个空字典  \n        d = {}  \n        for x in range(n):  \n            a = target - nums[x]  \n            #字典d中存在nums[x]时  \n            if nums[x] in d:  \n                return d[nums[x]],x  \n            #否则往字典增加键/值对  \n            else:  \n                d[a] = x  \n        #边往字典增加键/值对，边与nums[x]进行对比 \n```\n\n**一遍哈希表:**\n\n在进行迭代并将元素插入到表中的同时，我们还会回过头来检查表中是否已经存在当前元素所对应的目标元素。如果它存在，那我们已经找到了对应解，并立即将其返回。\n\n**复杂度分析：**\n\n- 时间复杂度：\\\\(O(n) \\\\) ， 我们只遍历了包含有 \\\\(n\\\\) 个元素的列表一次。在表中进行的每次查找只花费 \\\\(O(1)\\\\) 的时间。\n\n- 空间复杂度： \\\\(O(n)\\\\) ， 所需的额外空间取决于哈希表中存储的元素数量，该表最多需要存储 \\\\(n\\\\) 个元素。\n\n<br/>\n\n## 2. Add Two Numbers 两数相加\n\n给定两个非空链表来表示两个非负整数。位数按照逆序方式存储，它们的每个节点只存储单个数字。将两数相加返回一个新的链表。\n\n你可以假设除了数字 0 之外，这两个数字都不会以零开头。\n\n**示例：**\n\n>输入：(2 -> 4 -> 3) + (5 -> 6 -> 4)     \n>输出：7 -> 0 -> 8    \n>原因：342 + 465 = 807\n\n```python\nclass ListNode:\n    def __init__(self, x):\n        self.val = x\n        self.next = None\n\nclass Solution:\n    def addTwoNumbers(self, l1, l2):\n        \"\"\"\n        :type l1: ListNode\n        :type l2: ListNode\n        :rtype: ListNode\n        \"\"\"\n        a = 0\n        temp = ListNode(0)\n        l3 = temp\n        \n        while l1 != None or l2 != None or a != 0:\n            if l1 != None:\n                a += l1.val \n                l1 = l1.next\n            if l2 != None:\n                a += l2.val \n                l2 = l2.next\n            temp.next = ListNode(a%10)\n            temp = temp.next\n            a = a//10\n            #l3代替temp来输出链表  \n        return l3.next  \n```\n\n<br/>\n\n## 3. Longest Substring Without Repeating Characters 最长无重复字符的子串\n\n给定一个字符串，找出不含有重复字符的**最长子串**的长度。\n\n**示例：**\n\n>给定 `\"abcabcbb\"` ，没有重复字符的最长子串是 `\"abc\"` ，那么长度就是3。\n>\n>给定 `\"bbbbb\"` ，最长的子串就是 `\"b\"` ，长度是1。\n>\n>给定 `\"pwwkew\"` ，最长子串是 `\"wke\"` ，长度是3。请注意答案必须是一个**子串**，`\"pwke\"` 是 _子序列  _而不是子串。\n\n\n\n```python\nclass Solution(object):\n    def lengthOfLongestSubstring(self, s):\n        left = 0\n        ans = 0\n        last = {}\n        # left用于记录合法的左边界位置，last用于记录字符上一次出现的位置\n        \n        for i in range(len(s)):\n        \t# 子串中出现重复字符，变更left至上一次s[i]出现位置之后，使得子串合法\n            if s[i] in last and last[s[i]] >= left:\n                left = last[s[i]] + 1\n            else:\n                ans = max(ans, i-left+1)   # 这一行代码放在else里比放在else外（不要else） 块25%\n            last[s[i]] = i \n            \n        return ans\n```\n\n\n\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>","tags":["算法"],"categories":["算法"]},{"title":"PyTorch 入门","url":"/2018/05/29/pytorch/","content":"\n[PyTorch Cookbook（常用代码段整理合集）](https://zhuanlan.zhihu.com/p/59205847)\n\n[PyTorch 大批量数据在单个或多个 GPU 训练指南\n](https://www.pytorchtutorial.com/pytorch-large-batches-multi-gpu-and-distributed-training/#_GPU)\n\n[PyTorch中在反向传播前为什么要手动将梯度清零？\n](https://www.zhihu.com/question/303070254/answer/573037166)\n\n# 基础\n\n## Numpy to Torth\n\n>[python、PyTorch 图像读取与 numpy 转换](https://blog.csdn.net/yskyskyer123/article/details/80707038)\n\nTorch 自称为神经网络界的 Numpy, 因为他能将 torch 产生的 tensor 放在 GPU 中加速运算 (前提是你有合适的 GPU), 就像 Numpy 会把 array 放在 CPU 中加速运算. 所以神经网络的话, 当然是用 Torch 的 tensor 形式数据最好咯. 就像 Tensorflow 当中的 tensor 一样.\n\n当然, 我们对 Numpy 还是爱不释手的, 因为我们太习惯 numpy 的形式了. 不过 torch 看出来我们的喜爱, 他把 torch 做的和 numpy 能很好的兼容. 比如这样就能自由地转换 numpy array 和 torch tensor 了:\n\n```python\nimport torch\nimport numpy as np\n\nnp_data = np.arange(6).reshape((2, 3))\ntorch_data = torch.from_numpy(np_data)\ntensor2array = torch_data.numpy()\nprint(\n    '\\nnumpy array:', np_data,          # [[0 1 2], [3 4 5]]\n    '\\ntorch tensor:', torch_data,      #  0  1  2 \\n 3  4  5    [torch.LongTensor of size 2x3]\n    '\\ntensor to array:', tensor2array, # [[0 1 2], [3 4 5]]\n)\n```\n\n## 数学运算\n\n其实 torch 中 tensor 的运算和 numpy array 的如出一辙, 我们就以对比的形式来看. 如果想了解 torch 中其它更多有用的运算符, [API 就是你要去的地方](http://pytorch.org/docs/torch.html#math-operations)\n\n```python\n# abs 绝对值计算\ndata = [-1, -2, 1, 2]\ntensor = torch.FloatTensor(data)  # 转换成32位浮点 tensor\nprint(\n    '\\nabs',\n    '\\nnumpy: ', np.abs(data),          # [1 2 1 2]\n    '\\ntorch: ', torch.abs(tensor)      # [1 2 1 2]\n)\n\n# sin   三角函数 sin\nprint(\n    '\\nsin',\n    '\\nnumpy: ', np.sin(data),      # [-0.84147098 -0.90929743  0.84147098  0.90929743]\n    '\\ntorch: ', torch.sin(tensor)  # [-0.8415 -0.9093  0.8415  0.9093]\n)\n\n# mean  均值\nprint(\n    '\\nmean',\n    '\\nnumpy: ', np.mean(data),         # 0.0\n    '\\ntorch: ', torch.mean(tensor)     # 0.0\n)\n```\n\n除了简单的计算, 矩阵运算才是神经网络中最重要的部分. 所以我们展示下矩阵的乘法. 注意一下包含了一个 numpy 中可行, 但是 torch 中不可行的方式\n\n```python\n# matrix multiplication 矩阵点乘\ndata = [[1,2], [3,4]]\ntensor = torch.FloatTensor(data)  # 转换成32位浮点 tensor\n# correct method\nprint(\n    '\\nmatrix multiplication (matmul)',\n    '\\nnumpy: ', np.matmul(data, data),     # [[7, 10], [15, 22]]\n    '\\ntorch: ', torch.mm(tensor, tensor)   # [[7, 10], [15, 22]]\n)\n\n# !!!!  下面是错误的方法 !!!!\ndata = np.array(data)\nprint(\n    '\\nmatrix multiplication (dot)',\n    '\\nnumpy: ', data.dot(data),        # [[7, 10], [15, 22]] 在numpy 中可行\n    '\\ntorch: ', tensor.dot(tensor)     # torch 会转换成 [1,2,3,4].dot([1,2,3,4) = 30.0\n)\n```\n\n新版本中(>=0.3.0), 关于 tensor.dot() 有了新的改变, 它只能针对于一维的数组. 所以上面的有所改变\n\n```python\ntensor.dot(tensor)     # torch 会转换成 [1,2,3,4].dot([1,2,3,4) = 30.0\n\n# 变为\ntorch.dot(tensor.dot(tensor)\n```\n\n<br/>\n\n## Variable\n\n在 Torch 中的 Variable 就是一个存放会变化的值的地理位置. 里面的值会不停的变化. 就像一个裝鸡蛋的篮子, 鸡蛋数会不停变动. 那谁是里面的鸡蛋呢, 自然就是 Torch 的 Tensor 咯. 如果用一个 Variable 进行计算, 那返回的也是一个同类型的 Variable.\n\n```python\nimport torch\nfrom torch.autograd import Variable # torch 中 Variable 模块\n\n# 先生鸡蛋\ntensor = torch.FloatTensor([[1,2],[3,4]])\n# 把鸡蛋放到篮子里, requires_grad是参不参与误差反向传播, 要不要计算梯度\nvariable = Variable(tensor, requires_grad=True)\n\nprint(tensor)\n\"\"\"\n 1  2\n 3  4\n[torch.FloatTensor of size 2x2]\n\"\"\"\n\nprint(variable)\n\"\"\"\nVariable containing:\n 1  2\n 3  4\n[torch.FloatTensor of size 2x2]\n\"\"\"\n```\n\n**获取 Variable 里面的数据**\n\n直接 `print(variable)` 只会输出 Variable 形式的数据, 在很多时候是用不了的(比如想要用 plt 画图), 所以我们要转换一下, 将它变成 tensor 形式.\n\n```python\nprint(variable)     #  Variable 形式\n\"\"\"\nVariable containing:\n 1  2\n 3  4\n[torch.FloatTensor of size 2x2]\n\"\"\"\n\nprint(variable.data)    # tensor 形式\n\"\"\"\n 1  2\n 3  4\n[torch.FloatTensor of size 2x2]\n\"\"\"\n\nprint(variable.data.numpy())    # numpy 形式\n\"\"\"\n[[ 1.  2.]\n [ 3.  4.]]\n\"\"\"\n```\n\n<br/>\n\n## 激励函数\n\n**导入模块**\n\n```python\nimport torch\nimport torch.nn.functional as F     # 激励函数都在这\nfrom torch.autograd import Variable\n```\n\n**做一些假数据来观看图像**\n\n```python\nx = torch.linspace(-5, 5, 200)  # x data (tensor), shape=(100, 1)\nx = Variable(x)\nx_np = x.data.numpy()  # 换成 numpy array, 出图时用\n```\n\n**不同的激励函数**\n\n```python\n# 几种常用的 激励函数\ny_relu = F.relu(x).data.numpy()\ny_sigmoid = F.sigmoid(x).data.numpy()\ny_tanh = F.tanh(x).data.numpy()\ny_softplus = F.softplus(x).data.numpy()\n# y_softmax = F.softmax(x)  softmax 比较特殊, 不能直接显示, 不过他是关于概率的, 用于分类\n```\n\n**观察图像**\n\n```python\nimport matplotlib.pyplot as plt  \n\nplt.figure(1, figsize=(8, 6))\nplt.subplot(221)\nplt.plot(x_np, y_relu, c='red', label='relu')\nplt.ylim((-1, 5))\nplt.legend(loc='best')\n\nplt.subplot(222)\nplt.plot(x_np, y_sigmoid, c='red', label='sigmoid')\nplt.ylim((-0.2, 1.2))\nplt.legend(loc='best')\n\nplt.subplot(223)\nplt.plot(x_np, y_tanh, c='red', label='tanh')\nplt.ylim((-1.2, 1.2))\nplt.legend(loc='best')\n\nplt.subplot(224)\nplt.plot(x_np, y_softplus, c='red', label='softplus')\nplt.ylim((-0.2, 6))\nplt.legend(loc='best')\n\nplt.show()\n```\n\n ![](/img/2018-05-29-pytorch-101.jpg)\n \n\n<br/>\n \n \n# 数据处理\n\n## DataLoader\n\nTorch 中提供了一种帮你整理你的数据结构的好东西, 叫做 `DataLoader`, 我们能用它来包装自己的数据, 进行批训练. 而且批训练可以有很多种途径\n\n**DataLoader**\n\n`DataLoader` 是 torch 给你用来包装你的数据的工具. 所以你要讲自己的 (numpy array 或其他) 数据形式装换成 Tensor, 然后再放进这个包装器中. 使用 `DataLoader` 有什么好处呢? 就是他们帮你有效地迭代数据, 举例:\n\n```python\nimport torch\nimport torch.utils.data as Data\ntorch.manual_seed(1)    # reproducible\n\nBATCH_SIZE = 5      # 批训练的数据个数\n\nx = torch.linspace(1, 10, 10)       # x data (torch tensor)\ny = torch.linspace(10, 1, 10)       # y data (torch tensor)\n\n# 先转换成 torch 能识别的 Dataset\ntorch_dataset = Data.TensorDataset(x, y)\n\n# 把 dataset 放入 DataLoader\nloader = Data.DataLoader(\n    dataset=torch_dataset,      # torch TensorDataset format\n    batch_size=BATCH_SIZE,      # mini batch size\n    shuffle=True,               # 要不要打乱数据 (打乱比较好)\n    num_workers=2,              # 多线程来读数据\n)\n\nfor epoch in range(3):   # 训练所有!整套!数据 3 次\n    for step, (batch_x, batch_y) in enumerate(loader):  # 每一步 loader 释放一小批数据用来学习\n        # 假设这里就是你训练的地方...\n\n        # 打出来一些数据\n        print('Epoch: ', epoch, '| Step: ', step, '| batch x: ',\n              batch_x.numpy(), '| batch y: ', batch_y.numpy())\n\n\"\"\"\nEpoch:  0 | Step:  0 | batch x:  [ 6.  7.  2.  3.  1.] | batch y:  [  5.   4.   9.   8.  10.]\nEpoch:  0 | Step:  1 | batch x:  [  9.  10.   4.   8.   5.] | batch y:  [ 2.  1.  7.  3.  6.]\nEpoch:  1 | Step:  0 | batch x:  [  3.   4.   2.   9.  10.] | batch y:  [ 8.  7.  9.  2.  1.]\nEpoch:  1 | Step:  1 | batch x:  [ 1.  7.  8.  5.  6.] | batch y:  [ 10.   4.   3.   6.   5.]\nEpoch:  2 | Step:  0 | batch x:  [ 3.  9.  2.  6.  7.] | batch y:  [ 8.  2.  9.  5.  4.]\nEpoch:  2 | Step:  1 | batch x:  [ 10.   4.   8.   1.   5.] | batch y:  [  1.   7.   3.  10.   6.]\n\"\"\"\n```\n\n可以看出, 每步都导出了5个数据进行学习. 然后每个 epoch 的导出数据都是先打乱了以后再导出.\n\n\n## ImageFolder\n\n```python\nfrom torchvision.datasets import ImageFolder\n\nImageFolder(root,transform=None,target_transform=None,loader=default_loader)\n```\n\n假设所有的文件按文件夹保存好，每个文件夹下面存贮同一类别的图片，文件夹的名字为分类的名字。\n\n- `root`: 在指定的 root 路径下面寻找图片 \n- `transform`: 对 PIL Image 进行转换操作,transform 输入是 loader 读取图片返回的对象 \n- `target_transform`: 对 label 进行变换 \n- `loader`: 指定加载图片的函数，默认操作是读取 PIL image 对象\n\n**实例**\n\n```python\nfrom torchvision.datasets import ImageFolder\n\ndataset=ImageFolder('data/dogcat_2/')\n```\n\n**输出对应文件夹的 label**\n\n```python\nprint(dataset.class_to_idx)    \n\n# {'cat': 0, 'dog': 1}\n```\n\n**所有图片的路径和对应的 label**\n\n```python\nprint(dataset.imgs)  \n\n# [(‘data/dogcat_2/cat/cat.12484.jpg’, 0), (‘data/dogcat_2/cat/cat.12485.jpg’, 0), ... (‘data/dogcat_2/dog/dog.12498.jpg’, 1), (‘data/dogcat_2/dog/dog.12499.jpg’, 1)]\n```\n\n**输出图片信息**\n\n```python\nprint(dataset[0][1]) #第二维度为1 ，表示label\nprint(dataset[0][0]) #第二维度为0，表示图片数据\n\n# 输出: \n# 0 \n# < PIL.Image.Image image mode=RGB size=497x500 at 0x7F25F3D31E10>\n```\n**输出图片大小**\n\n```python\n# 输出第 0 张图片的大小\nprint(dataset[0][0].size())\n\n# torch.Size([3, 224, 224])\n```\n\n**加上 transforms**\n\n```python\nfrom torchvision.datasets import ImageFolder\nimport torch\nfrom torchvision import transforms\n\n#加上transforms\nnormalize=transforms.Normalize(mean=[.5,.5,.5],std=[.5,.5,.5])\ntransform=transforms.Compose([\n    transforms.RandomReSizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(), #将图片转换为Tensor,归一化至[0,1]\n    normalize\n])\n\ndataset=ImageFolder('data/dogcat_2/',transform=transform)\nprint(dataset[0][0].size())    # n输出第 0 张图片的大小\n\n# torch.Size([3, 224, 224])\n```\n\n\n\n\n## 图像处理\n\n>[transforms的二十二个方法](https://blog.csdn.net/u011995719/article/details/85107009)\n\n## 定义自己的数据集\n\n>不使用 ImageFolder 来处理定义自己的数据集\n\n```python\n#数据处理\nimport os\nimport torch\nfrom torch.utils import data\nfrom PIL import Image\nimport numpy as np\nfrom torchvision import transforms\n\ntransform=transforms.Compose([\n    transforms.Resize(224), #缩放图片，保持长宽比不变，最短边的长为224像素,\n    transforms.CenterCrop(224), #从中间切出 224*224的图片\n    transforms.ToTensor(), #将图片转换为Tensor,归一化至[0,1]\n    transforms.Normalize(mean=[.5,.5,.5],std=[.5,.5,.5]) #标准化至[-1,1]\n])\n\n#定义自己的数据集合\nclass DogCat(data.Dataset):\n\n    def __init__(self,root):\n        #所有图片的绝对路径\n        imgs=os.listdir(root)\n\n        self.imgs=[os.path.join(root,k) for k in imgs]\n        self.transforms=transform\n\n    def __getitem__(self, index):\n        img_path=self.imgs[index]\n        #dog-> 1 cat ->0\n        label=1 if 'dog' in img_path.split('/')[-1] else 0\n        pil_img=Image.open(img_path)\n        if self.transforms:\n            data=self.transforms(pil_img)\n        else:\n            pil_img=np.asarray(pil_img)\n            data=torch.from_numpy(pil_img)\n        return data,label\n\n    def __len__(self):\n        return len(self.imgs)\n\ndataSet=DogCat('./data/dogcat')\n\nprint(dataSet[0])\n```\n\n# 网络搭建\n\n## 回归\n \n**建立数据集**\n\n我们创建一些假数据来模拟真实的情况. 比如一个一元二次函数: `y = a * x^2 + b`, 我们给 `y` 数据加上一点噪声来更加真实的展示它.\n\n```python\nimport torch\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\n\nx = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)\ny = x.pow(2) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)\n\n# 用 Variable 来修饰这些数据 tensor\nx, y = torch.autograd.Variable(x), Variable(y)\n\n# 画图\nplt.scatter(x.data.numpy(), y.data.numpy())\nplt.show()\n```\n\n\n\n**建立神经网络**\n\n建立一个神经网络我们可以直接运用 torch 中的体系. 先定义所有的层属性 (`__init__()`), 然后再一层层搭建(`forward(x)`)层于层的关系链接\n\n```python\nimport torch\nimport torch.nn.functional as F     # 激励函数都在这\n\nclass Net(torch.nn.Module):  # 继承 torch 的 Module\n    def __init__(self, n_feature, n_hidden, n_output):\n        super(Net, self).__init__()     # 继承 __init__ 功能\n        # 定义每层用什么样的形式\n        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # 隐藏层线性输出\n        self.predict = torch.nn.Linear(n_hidden, n_output)   # 输出层线性输出\n\n    def forward(self, x):   # 这同时也是 Module 中的 forward 功能\n        # 正向传播输入值, 神经网络分析出输出值\n        x = F.relu(self.hidden(x))      # 激励函数(隐藏层的线性值)\n        x = self.predict(x)             # 输出值\n        return x\n\nnet = Net(n_feature=1, n_hidden=10, n_output=1)\n\nprint(net)  # net 的结构\n\"\"\"\nNet (\n  (hidden): Linear (1 -> 10)\n  (predict): Linear (10 -> 1)\n)\n\"\"\"\n```\n\n**训练网络**\n\n```python\n# optimizer 是训练的工具\noptimizer = torch.optim.SGD(net.parameters(), lr=0.5)  # 传入 net 的所有参数, 学习率\nloss_func = torch.nn.MSELoss()      # 预测值和真实值的误差计算公式 (均方差)\n\nfor t in range(100):\n    prediction = net(x)     # 喂给 net 训练数据 x, 输出预测值\n\n    loss = loss_func(prediction, y)     # 计算两者的误差\n\n    optimizer.zero_grad()   # 清空上一步的残余更新参数值\n    loss.backward()         # 误差反向传播, 计算参数更新值\n    optimizer.step()        # 将参数更新值施加到 net 的 parameters 上\n```\n\n\n**可视化训练过程**\n\n```python\nimport matplotlib.pyplot as plt\n\nplt.ion()   # 画图\nplt.show()\n\nfor t in range(100):\n\n    ...  # 同上\n    loss.backward()\n    optimizer.step()\n\n    # 接着上面来\n    if t % 5 == 0:\n        # plot and show learning process\n        plt.cla()\n        plt.scatter(x.data.numpy(), y.data.numpy())\n        plt.plot(x.data.numpy(), prediction.data.numpy(), 'r-', lw=5)\n        plt.text(0.5, 0, 'Loss=%.4f' % loss.data[0], fontdict={'size': 20, 'color':  'red'})\n        plt.pause(0.1)\n```\n\n<br/>\n\n## 分类\n\n**建立数据集**\n\n```python\nimport torch\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\n\n# 假数据\nn_data = torch.ones(100, 2)         # 数据的基本形态\nx0 = torch.normal(2*n_data, 1)      # 类型0 x data (tensor), shape=(100, 2)\ny0 = torch.zeros(100)               # 类型0 y data (tensor), shape=(100, 1)\nx1 = torch.normal(-2*n_data, 1)     # 类型1 x data (tensor), shape=(100, 1)\ny1 = torch.ones(100)                # 类型1 y data (tensor), shape=(100, 1)\n\n# 注意 x, y 数据的数据形式是一定要像下面一样 (torch.cat 是在合并数据)\nx = torch.cat((x0, x1), 0).type(torch.FloatTensor)  # FloatTensor = 32-bit floating\ny = torch.cat((y0, y1), ).type(torch.LongTensor)    # LongTensor = 64-bit integer\n\n# torch 只能在 Variable 上训练, 所以把它们变成 Variable\nx, y = Variable(x), Variable(y)\n\n# plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=y.data.numpy(), s=100, lw=0, cmap='RdYlGn')\n# plt.show()\n\n# 画图\nplt.scatter(x.data.numpy(), y.data.numpy())\nplt.show(\n```\n\n\n**建立神经网络**\n\n```python\nimport torch\nimport torch.nn.functional as F     # 激励函数都在这\n\nclass Net(torch.nn.Module):     # 继承 torch 的 Module\n    def __init__(self, n_feature, n_hidden, n_output):\n        super(Net, self).__init__()     # 继承 __init__ 功能\n        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # 隐藏层线性输出\n        self.out = torch.nn.Linear(n_hidden, n_output)       # 输出层线性输出\n\n    def forward(self, x):\n        # 正向传播输入值, 神经网络分析出输出值\n        x = F.relu(self.hidden(x))      # 激励函数(隐藏层的线性值)\n        x = self.out(x)                 # 输出值, 但是这个不是预测值, 预测值还需要再另外计算\n        return x\n\nnet = Net(n_feature=2, n_hidden=10, n_output=2) # 几个类别就几个 output\n\nprint(net)  # net 的结构\n\"\"\"\nNet (\n  (hidden): Linear (2 -> 10)\n  (out): Linear (10 -> 2)\n)\n\"\"\"\n```\n\n**训练网络**\n\n```python\n# optimizer 是训练的工具\noptimizer = torch.optim.SGD(net.parameters(), lr=0.02)  # 传入 net 的所有参数, 学习率\n# 算误差的时候, 注意真实值!不是! one-hot 形式的, 而是1D Tensor, (batch,)\n# 但是预测值是2D tensor (batch, n_classes)\nloss_func = torch.nn.CrossEntropyLoss()\n\nfor t in range(100):\n    out = net(x)     # 喂给 net 训练数据 x, 输出分析值\n\n    loss = loss_func(out, y)     # 计算两者的误差\n\n    optimizer.zero_grad()   # 清空上一步的残余更新参数值\n    loss.backward()         # 误差反向传播, 计算参数更新值\n    optimizer.step()        # 将参数更新值施加到 net 的 parameters 上\n```\n\n**可视化训练过程**\n\n```python\nimport matplotlib.pyplot as plt\n\nplt.ion()   # 画图\nplt.show()\n\nfor t in range(100):\n\n    ...\n    loss.backward()\n    optimizer.step()\n\n    # 接着上面来\n    if t % 2 == 0:\n        plt.cla()\n        # 过了一道 softmax 的激励函数后的最大概率才是预测值\n        prediction = torch.max(F.softmax(out), 1)[1]\n        pred_y = prediction.data.numpy().squeeze()\n        target_y = y.data.numpy()\n        plt.scatter(x.data.numpy()[:, 0], x.data.numpy()[:, 1], c=pred_y, s=100, lw=0, cmap='RdYlGn')\n        accuracy = sum(pred_y == target_y)/200  # 预测中有多少和真实值一样\n        plt.text(1.5, -4, 'Accuracy=%.2f' % accuracy, fontdict={'size': 20, 'color':  'red'})\n        plt.pause(0.1)\n\nplt.ioff()  # 停止画图\nplt.show()\n```\n\n\n\n## 快速搭建 \n\n我们先看看之前写神经网络时用到的步骤. 我们用 `net1` 代表这种方式搭建的神经网络.\n    \n```python\nclass Net(torch.nn.Module):\n    def __init__(self, n_feature, n_hidden, n_output):\n        super(Net, self).__init__()\n        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n        self.predict = torch.nn.Linear(n_hidden, n_output)\n\n    def forward(self, x):\n        x = F.relu(self.hidden(x))\n        x = self.predict(x)\n        return x\n\nnet1 = Net(1, 10, 1)   # 这是我们用这种方式搭建的 net1\n```\n\n我们用 class 继承了一个 torch 中的神经网络结构, 然后对其进行了修改, 不过还有更快的一招, 用一句话就概括了上面所有的内容!\n    \n```python\nnet2 = torch.nn.Sequential(\n    torch.nn.Linear(1, 10),\n    torch.nn.ReLU(),\n    torch.nn.Linear(10, 1)\n)\n```\n\n我们再对比一下两者的结构:\n    \n```python\nprint(net1)\n\"\"\"\nNet (\n  (hidden): Linear (1 -> 10)\n  (predict): Linear (10 -> 1)\n)\n\"\"\"\nprint(net2)\n\"\"\"\nSequential (\n  (0): Linear (1 -> 10)\n  (1): ReLU ()\n  (2): Linear (10 -> 1)\n)\n\"\"\"\n```\n\n我们会发现 `net2` 多显示了一些内容, 这是为什么呢? 原来他把激励函数也一同纳入进去了, 但是 `net1` 中, 激励函数实际上是在 `forward()` 功能中才被调用的. 这也就说明了, 相比 `net2`, `net1` 的好处就是, 你可以根据你的个人需要更加个性化你自己的前向传播过程, 比如(RNN). 不过如果不需要七七八八的过程, 相信 `net2` 这种形式更适合.\n\n\n## 保存与提取\n\n我们快速地建造数据, 搭建网络:\n\n```python\ntorch.manual_seed(1)    # reproducible\n\n# 假数据\nx = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)\ny = x.pow(2) + 0.2*torch.rand(x.size())  # noisy y data (tensor), shape=(100, 1)\nx, y = Variable(x, requires_grad=False), Variable(y, requires_grad=False)\n\n\ndef save():\n    # 建网络\n    net1 = torch.nn.Sequential(\n        torch.nn.Linear(1, 10),\n        torch.nn.ReLU(),\n        torch.nn.Linear(10, 1)\n    )\n    optimizer = torch.optim.SGD(net1.parameters(), lr=0.5)\n    loss_func = torch.nn.MSELoss()\n\n    # 训练\n    for t in range(100):\n        prediction = net1(x)\n        loss = loss_func(prediction, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n```\n\n接下来我们有两种途径来保存\n\n```python\ntorch.save(net1, 'net.pkl')  # 保存整个网络\ntorch.save(net1.state_dict(), 'net_params.pkl')   # 只保存网络中的参数 (速度快, 占内存少)\n```\n\n**提取**\n\n1. **提取整个神经网络**, 网络大的时候可能会比较慢\n\n>这个方法要 import 原来的 pytorch 模型类，或者直接将该类复制回来\n\n```python\nclass CNN(nn.Module):\n\t...\n\nnet2 = torch.load('net.pkl') # net.pkl 中保存的类要和上边的 CNN 类相同\nprediction = net2(x)\n```\n\n2. **只提取网络参数**\n\n这个方法会提取所有的参数，然后再放到你的新建网络中\n\n```python\ndef restore_params():\n    # 新建 net3\n    net3 = torch.nn.Sequential(\n        torch.nn.Linear(1, 10),\n        torch.nn.ReLU(),\n        torch.nn.Linear(10, 1)\n    )\n\n    # 将保存的参数复制到 net3\n    net3.load_state_dict(torch.load('net_params.pkl'))\n    prediction = net3(x)\n```\n\n**显示结果**\n\n```python\n# 保存 net1 (1. 整个网络, 2. 只有参数)\nsave()\n\n# 提取整个网络\nrestore_net()\n\n# 提取网络参数, 复制到新网络\nrestore_params()\n```\n\n\n这样我们能看到三个网络一模一样\n\n\n\n\n## 优化器 Optimizer\n\n\n\n**伪数据**\n\n```python\nimport torch\nimport torch.utils.data as Data\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\ntorch.manual_seed(1)    # reproducible\n\nLR = 0.01\nBATCH_SIZE = 32\nEPOCH = 12\n\n# fake dataset\nx = torch.unsqueeze(torch.linspace(-1, 1, 1000), dim=1)\ny = x.pow(2) + 0.1*torch.normal(torch.zeros(*x.size()))\n\n# plot dataset\nplt.scatter(x.numpy(), y.numpy())\nplt.show()\n\n# 使用上节内容提到的 data loader\ntorch_dataset = Data.TensorDataset(x, y)\nloader = Data.DataLoader(dataset=torch_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)\n```\n\n**每个优化器优化一个神经网络**\n\n为了对比每一种优化器, 我们给他们各自创建一个神经网络, 但这个神经网络都来自同一个 `Net` 形式\n\n\n```python\n# 默认的 network 形式\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.hidden = torch.nn.Linear(1, 20)   # hidden layer\n        self.predict = torch.nn.Linear(20, 1)   # output layer\n\n    def forward(self, x):\n        x = F.relu(self.hidden(x))      # activation function for hidden layer\n        x = self.predict(x)             # linear output\n        return x\n\n# 为每个优化器创建一个 net\nnet_SGD         = Net()\nnet_Momentum    = Net()\nnet_RMSprop     = Net()\nnet_Adam        = Net()\nnets = [net_SGD, net_Momentum, net_RMSprop, net_Adam]\n```\n\n**优化器 Optimizer**\n\n接下来在创建不同的优化器, 用来训练不同的网络. 并创建一个 `loss_func` 用来计算误差. 我们用几种常见的优化器, `SGD`, `Momentum`, `RMSprop`, `Adam`.\n\n```python\n# different optimizers\nopt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)\nopt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8)\nopt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)\nopt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))\noptimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]\n\nloss_func = torch.nn.MSELoss()\nlosses_his = [[], [], [], []]   # 记录 training 时不同神经网络的 loss\n```\n\n**训练/出图**\n\n```python\nfor epoch in range(EPOCH):\n    print('Epoch: ', epoch)\n    for step, (b_x, b_y) in enumerate(loader):\n\n        # 对每个优化器, 优化属于他的神经网络\n        for net, opt, l_his in zip(nets, optimizers, losses_his):\n            output = net(b_x)              # get output for every net\n            loss = loss_func(output, b_y)  # compute loss for every net\n            opt.zero_grad()                # clear gradients for next train\n            loss.backward()                # backpropagation, compute gradients\n            opt.step()                     # apply gradients\n            l_his.append(loss.data.numpy())     # loss recoder\n```\n\n`SGD` 是最普通的优化器, 也可以说没有加速效果, 而 `Momentum` 是 `SGD` 的改良版, 它加入了动量原则. 后面的 `RMSprop` 又是 `Momentum` 的升级版. 而 `Adam` 又是 `RMSprop` 的升级版. 不过从这个结果中我们看到, `Adam` 的效果似乎比 `RMSprop` 要差一点. 所以说并不是越先进的优化器, 结果越佳. 我们在自己的试验中可以尝试不同的优化器, 找到那个最适合你数据/网络的优化器\n\n## 卷积神经网络\n\n**MNIST手写数据**\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.utils.data as Data\nimport torchvision      # 数据库模块\nimport matplotlib.pyplot as plt\n\ntorch.manual_seed(1)    # reproducible\n\n# Hyper Parameters\nEPOCH = 1           # 训练整批数据多少次, 为了节约时间, 我们只训练一次\nBATCH_SIZE = 50\nLR = 0.001          # 学习率\nDOWNLOAD_MNIST = True  # 如果你已经下载好了mnist数据就写上 Fasle\n\n\n# Mnist 手写数字\ntrain_data = torchvision.datasets.MNIST(\n    root='./mnist/',    # 保存或者提取位置\n    train=True,  # this is training data\n    transform=torchvision.transforms.ToTensor(),    # 转换 PIL.Image or numpy.ndarray 成\n                                                    # torch.FloatTensor (C x H x W), 训练的时候 normalize 成 [0.0, 1.0] 区间\n    download=DOWNLOAD_MNIST,          # 没下载就下载, 下载了就不用再下了\n)\n```\n\n同样, 我们除了训练数据, 还给一些测试数据, 测试看看它有没有训练好\n\n```python\ntest_data = torchvision.datasets.MNIST(root='./mnist/', train=False)\n\n# 批训练 50samples, 1 channel, 28x28 (50, 1, 28, 28)\ntrain_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n\n# 为了节约时间, 我们测试时只测试前2000个\ntest_x = torch.unsqueeze(test_data.test_data, dim=1).type(torch.FloatTensor)[:2000]/255.   # shape from (2000, 28, 28) to (2000, 1, 28, 28), value in range(0,1)\ntest_y = test_data.test_labels[:2000]\n```\n\n**CNN模型**\n\n和以前一样, 我们用一个 class 来建立 CNN 模型. 这个 CNN 整体流程是 卷积(`Conv2d`) -> 激励函数(`ReLU`) -> 池化, 向下采样 (`MaxPooling`) -> 再来一遍 -> 展平多维的卷积成的特征图 -> 接入全连接层 (`Linear`) -> 输出\n\n```python\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Sequential(  # input shape (1, 28, 28)\n            nn.Conv2d(\n                in_channels=1,      # input height\n                out_channels=16,    # n_filters\n                kernel_size=5,      # filter size\n                stride=1,           # filter movement/step\n                padding=2,      # 如果想要 con2d 出来的图片长宽没有变化, padding=(kernel_size-1)/2 当 stride=1\n            ),      # output shape (16, 28, 28)\n            nn.ReLU(),    # activation\n            nn.MaxPool2d(kernel_size=2),    # 在 2x2 空间里向下采样, output shape (16, 14, 14)\n        )\n        self.conv2 = nn.Sequential(  # input shape (16, 14, 14)\n            nn.Conv2d(16, 32, 5, 1, 2),  # output shape (32, 14, 14)\n            nn.ReLU(),  # activation\n            nn.MaxPool2d(2),  # output shape (32, 7, 7)\n        )\n        self.out = nn.Linear(32 * 7 * 7, 10)   # fully connected layer, output 10 classes\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = x.view(x.size(0), -1)   # 展平多维的卷积图成 (batch_size, 32 * 7 * 7)\n        output = self.out(x)\n        return output\n\ncnn = CNN()\nprint(cnn)  # net architecture\n\"\"\"\nCNN (\n  (conv1): Sequential (\n    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): ReLU ()\n    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n  )\n  (conv2): Sequential (\n    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): ReLU ()\n    (2): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n  )\n  (out): Linear (1568 -> 10)\n)\n\"\"\"\n```\n\n**训练**\n\n下面我们开始训练, 将 `x` `y` 都用 `Variable` 包起来, 然后放入 `cnn` 中计算 `output`, 最后再计算误差. 下面代码省略了计算精确度 `accuracy` 的部分\n\n\n```PYTHON\noptimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\nloss_func = nn.CrossEntropyLoss()   # the target label is not one-hotted\n\n# training and testing\nfor epoch in range(EPOCH):\n    for step, (b_x, b_y) in enumerate(train_loader):   # 分配 batch data, normalize x when iterate train_loader\n        output = cnn(b_x)               # cnn output\n        loss = loss_func(output, b_y)   # cross entropy loss\n        optimizer.zero_grad()           # clear gradients for this training step\n        loss.backward()                 # backpropagation, compute gradients\n        optimizer.step()                # apply gradients\n\n\"\"\"\n...\nEpoch:  0 | train loss: 0.0306 | test accuracy: 0.97\nEpoch:  0 | train loss: 0.0147 | test accuracy: 0.98\nEpoch:  0 | train loss: 0.0427 | test accuracy: 0.98\nEpoch:  0 | train loss: 0.0078 | test accuracy: 0.98\n\"\"\"\n```\n\n最后我们再来取10个数据, 看看预测的值到底对不对:\n\n```PYTHON\ntest_output = cnn(test_x[:10])\npred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\nprint(pred_y, 'prediction number')\nprint(test_y[:10].numpy(), 'real number')\n\n\"\"\"\n[7 2 1 0 4 1 4 9 5 9] prediction number\n[7 2 1 0 4 1 4 9 5 9] real number\n\"\"\"\n```\n\n\n# 网络优化方法\n\n## 1. Dropout\n\n**做点数据**\n\n自己做一些伪数据, 用来模拟真实情况. 数据少, 才能凸显过拟合问题, 所以我们就做10个数据点\n\n```python\nimport torch\nimport matplotlib.pyplot as plt\n\ntorch.manual_seed(1)    # reproducible\n\nN_SAMPLES = 20\nN_HIDDEN = 300\n\n# training data\nx = torch.unsqueeze(torch.linspace(-1, 1, N_SAMPLES), 1)\ny = x + 0.3*torch.normal(torch.zeros(N_SAMPLES, 1), torch.ones(N_SAMPLES, 1))\n\n# test data\ntest_x = torch.unsqueeze(torch.linspace(-1, 1, N_SAMPLES), 1)\ntest_y = test_x + 0.3*torch.normal(torch.zeros(N_SAMPLES, 1), torch.ones(N_SAMPLES, 1))\n\n# show data\nplt.scatter(x.data.numpy(), y.data.numpy(), c='magenta', s=50, alpha=0.5, label='train')\nplt.scatter(test_x.data.numpy(), test_y.data.numpy(), c='cyan', s=50, alpha=0.5, label='test')\nplt.legend(loc='upper left')\nplt.ylim((-2.5, 2.5))\nplt.show()\n```\n\n**搭建神经网络**\n\n我们在这里搭建两个神经网络, 一个没有 `dropout`, 一个有 `dropout`. 没有 `dropout` 的容易出现 过拟合, 那我们就命名为 `net_overfitting`, 另一个就是 `net_dropped`. `torch.nn.Dropout(0.5)` 这里的 0.5 指的是随机有 50% 的神经元会被关闭/丢弃.\n\n```python\nnet_overfitting = torch.nn.Sequential(\n    torch.nn.Linear(1, N_HIDDEN),\n    torch.nn.ReLU(),\n    torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n    torch.nn.ReLU(),\n    torch.nn.Linear(N_HIDDEN, 1),\n)\n\nnet_dropped = torch.nn.Sequential(\n    torch.nn.Linear(1, N_HIDDEN),\n    torch.nn.Dropout(0.5),  # drop 50% of the neuron\n    torch.nn.ReLU(),\n    torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n    torch.nn.Dropout(0.5),  # drop 50% of the neuron\n    torch.nn.ReLU(),\n    torch.nn.Linear(N_HIDDEN, 1),\n)\n```\n\n**训练**\n\n训练的时候, 这两个神经网络分开训练. 训练的环境都一样\n\n```python\noptimizer_ofit = torch.optim.Adam(net_overfitting.parameters(), lr=0.01)\noptimizer_drop = torch.optim.Adam(net_dropped.parameters(), lr=0.01)\nloss_func = torch.nn.MSELoss()\n\nfor t in range(500):\n    pred_ofit = net_overfitting(x)\n    pred_drop = net_dropped(x)\n\n    loss_ofit = loss_func(pred_ofit, y)\n    loss_drop = loss_func(pred_drop, y)\n\n    optimizer_ofit.zero_grad()\n    optimizer_drop.zero_grad()\n    loss_ofit.backward()\n    loss_drop.backward()\n    optimizer_ofit.step()\n    optimizer_drop.step()\n```\n\n**对比测试结果**\n\n在这个 `for` 循环里, 我们加上画测试图的部分. 注意在测试时, 要将网络改成 `eval()` 形式, 特别是 `net_dropped`, `net_overfitting` 改不改其实无所谓. 画好图再改回 `train()` 模式\n\n```python\n    ...\n\n    optimizer_ofit.step()\n    optimizer_drop.step()\n\n    # 接着上面来\n    if t % 10 == 0:     # 每 10 步画一次图\n        # 将神经网络转换成测试形式, 画好图之后改回 训练形式\n        net_overfitting.eval()\n        net_dropped.eval()  # 因为 drop 网络在 train 的时候和 test 的时候参数不一样.\n\n        ...\n        test_pred_ofit = net_overfitting(test_x)\n        test_pred_drop = net_dropped(test_x)\n        ...\n\n        # 将两个网络改回 训练形式\n        net_overfitting.train()\n        net_dropped.train()\n```\n\n##  2. Batch Normalization\n\n批标准化通俗来说就是对每一层神经网络进行标准化 (normalize) 处理, 我们知道对输入数据进行标准化能让机器学习有效率地学习. 如果把每一层后看成这种接受输入数据的模式, 那我们何不 “批标准化” 所有的层呢?\n\n**做点数据**\n\n自己做一些伪数据, 用来模拟真实情况. 而且 Batch Normalization (之后都简称BN) 还能有效的控制坏的参数初始化 (initialization), 比如说 `ReLU` 这种激励函数最怕所有的值都落在附属区间, 那我们就将所有的参数都水平移动一个 -0.2 (`bias_initialization = -0.2`), 来看看 BN 的实力.\n\n```python\nimport torch\nfrom torch import nn\nfrom torch.nn import init\nimport torch.utils.data as Data\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 超参数\nN_SAMPLES = 2000\nBATCH_SIZE = 64\nEPOCH = 12\nLR = 0.03\nN_HIDDEN = 8\nACTIVATION = F.tanh     # 你可以换 relu 试试\nB_INIT = -0.2   # 模拟不好的 参数初始化\n\n# training data\nx = np.linspace(-7, 10, N_SAMPLES)[:, np.newaxis]\nnoise = np.random.normal(0, 2, x.shape)\ny = np.square(x) - 5 + noise\n\n# test data\ntest_x = np.linspace(-7, 10, 200)[:, np.newaxis]\nnoise = np.random.normal(0, 2, test_x.shape)\ntest_y = np.square(test_x) - 5 + noise\n\ntrain_x, train_y = torch.from_numpy(x).float(), torch.from_numpy(y).float()\ntest_x = torch.from_numpy(test_x).float()\ntest_y = torch.from_numpy(test_y).float()\n\ntrain_dataset = Data.TensorDataset(train_x, train_y)\ntrain_loader = Data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2,)\n\n# show data\nplt.scatter(train_x.numpy(), train_y.numpy(), c='#FF9359', s=50, alpha=0.2, label='train')\nplt.legend(loc='upper left')\nplt.show()\n```\n\n**搭建神经网络**\n\n这里就教你如何构建带有 BN 的神经网络的. BN 其实可以看做是一个 layer (`BN layer`). 我们就像平时加层一样加 `BN layer` 就好了. 注意, 我还对输入数据进行了一个 BN 处理, 因为如果你把输入数据看出是 从前面一层来的输出数据, 我们同样也能对她进行 BN.\n\n\n```python\nclass Net(nn.Module):\n    def __init__(self, batch_normalization=False):\n        super(Net, self).__init__()\n        self.do_bn = batch_normalization\n        self.fcs = []   # 太多层了, 我们用 for loop 建立\n        self.bns = []\n        self.bn_input = nn.BatchNorm1d(1, momentum=0.5)   # 给 input 的 BN\n\n        for i in range(N_HIDDEN):               # 建层\n            input_size = 1 if i == 0 else 10\n            fc = nn.Linear(input_size, 10)\n            setattr(self, 'fc%i' % i, fc)       # 注意! pytorch 一定要你将层信息变成 class 的属性! 我在这里花了2天时间发现了这个 bug\n            self._set_init(fc)                  # 参数初始化\n            self.fcs.append(fc)\n            if self.do_bn:\n                bn = nn.BatchNorm1d(10, momentum=0.5)\n                setattr(self, 'bn%i' % i, bn)   # 注意! pytorch 一定要你将层信息变成 class 的属性! 我在这里花了2天时间发现了这个 bug\n                self.bns.append(bn)\n\n        self.predict = nn.Linear(10, 1)         # output layer\n        self._set_init(self.predict)            # 参数初始化\n\n    def _set_init(self, layer):     # 参数初始化\n        init.normal_(layer.weight, mean=0., std=.1)\n        init.constant_(layer.bias, B_INIT)\n\n    def forward(self, x):\n        pre_activation = [x]\n        if self.do_bn: x = self.bn_input(x)    # 判断是否要加 BN\n        layer_input = [x]\n        for i in range(N_HIDDEN):\n            x = self.fcs[i](x)\n            pre_activation.append(x)    # 为之后出图\n            if self.do_bn: x = self.bns[i](x)  # 判断是否要加 BN\n            x = ACTIVATION(x)\n            layer_input.append(x)       # 为之后出图\n        out = self.predict(x)\n        return out, layer_input, pre_activation\n\n# 建立两个 net, 一个有 BN, 一个没有\nnets = [Net(batch_normalization=False), Net(batch_normalization=True)]\n```\n\n**训练**\n\n```python\nopts = [torch.optim.Adam(net.parameters(), lr=LR) for net in nets]\n\nloss_func = torch.nn.MSELoss()\n\nlosses = [[], []]  # 每个网络一个 list 来记录误差\nfor epoch in range(EPOCH):\n    print('Epoch: ', epoch)\n    for step, (b_x, b_y) in enumerate(train_loader):\n        for net, opt in zip(nets, opts):     # 训练两个网络\n            pred, _, _ = net(b_x)\n            loss = loss_func(pred, b_y)\n            opt.zero_grad()\n            loss.backward()\n            opt.step()    # 这也会训练 BN 里面的参数\n```\n\n**画图**\n\n```python\nf, axs = plt.subplots(4, N_HIDDEN+1, figsize=(10, 5))\n\ndef plot_histogram(l_in, l_in_bn, pre_ac, pre_ac_bn):\n    ...\n\nfor epoch in range(EPOCH):\n    layer_inputs, pre_acts = [], []\n    for net, l in zip(nets, losses):\n        # 一定要把 net 的设置成 eval 模式, eval下的 BN 参数会被固定\n        net.eval()\n        pred, layer_input, pre_act = net(test_x)\n        l.append(loss_func(pred, test_y).data[0])\n        layer_inputs.append(layer_input)\n        pre_acts.append(pre_act)\n        # 收集好信息后将 net 设置成 train 模式, 继续训练\n        net.train()\n    plot_histogram(*layer_inputs, *pre_acts)     # plot histogram\n\n    # 后面接着之前 for loop 中的代码来\n    for step, (b_x, b_y) in enumerate(train_loader):\n    ...\n```\n\n\n# GPU 训练\n\n我们定义一个辅助函数，以便在有 GPU 时选择 GPU 为目标设备，否则就默认选择 CPU。\n\n```python\ndef get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n```\n\n```python\ndevice = get_default_device()\n\nprint(device)   # device(type='cuda')\n```\n\n# 函数\n\n## 维度调整\n\n```python\nimport numpy as np\nimport torch\n\na = np.arange(24).reshape(2,3,4)\nb = a[:,-1,:]\nprint(b.shape)\n\n# (2, 4)\n\nx = torch.from_numpy(a)\ny = x[:,-1,:].unsqueeze(1)\nprint(y.size())     # torch.Size([2, 1, 4])\nz = y.expand(2,4,4)\nprint(z.size())     # torch.Size([2, 4, 4])\n```\n\n`expand `\n\n扩展某个size为1的维度。如(2,2,1)扩展为(2,2,3)\n\n```python\nimport torch\nx=torch.randn(2,2,1)\nprint(x)\ny=x.expand(2,2,3)\nprint(y)\n```\n\n输出：\n\n```python\ntensor([[ 0.2000,  0.3000,  0.2000],\n        [ 1.3000,  1.3000,  1.3000],\n        [ 2.3000,  2.3000,  2.3000],\n        [ 3.2000,  3.2000,  3.1000]])\ntensor([[ 1.1000,  2.2000,  1.3000],\n        [ 2.1000,  2.2000,  2.3000],\n        [ 2.1000,  2.2000,  2.3000],\n        [ 1.1000,  1.2000,  0.3000]])\ntensor([[ 0.2000],\n        [ 1.3000],\n        [ 2.1000],\n        [ 3.2000]])\n```\n\n`squeeze`\n\n将维度为1的压缩掉。如size为（3,1,1,2），压缩之后为（3,2）\n\n```python\nimport torch\na=torch.randn(2,1,1,3)\nprint(a)\nprint(a.squeeze())\n```\n\n输出：\n\n```python\ntensor([[[[-0.2320,  0.9513,  1.1613]]],\n \n \n        [[[ 0.0901,  0.9613, -0.9344]]]])\ntensor([[-0.2320,  0.9513,  1.1613],\n        [ 0.0901,  0.9613, -0.9344]])\n```\n\n`unsqueeze(n)`\n\n在第 n 个位置增加一维，如 (2,3) 在 unsqueeze(1) 后为 (2,1,3)    \n可以用来增加 batch 的位置\n\n\n`max`\n\n返回最大值，或指定维度的最大值以及 index\n\n`argmax`\n\n返回最大值的 index\n\n>[pytorch之expand，gather，squeeze，sum，contiguous，softmax，max，argmax](https://blog.csdn.net/hbu_pig/article/details/81454503#max)\n\n# 错误汇总\n\n>[Pytorch 错误汇总](https://blog.csdn.net/weixin_40841247/article/details/88682551)\n\n```\nExpected stride to be a single integer value or a list of 1 values to match the convolution dimensions, but got stride=[1, 1]\n```\n\nmodel 输入 tensor 错误，形状应为 `batch, c, w, h`\n\n```\nAttributeError: Can't get attribute 'Net' on <module '__main__'>\n```\n\n在导入模型的时候没有把类的定义添加或者 import 到加载模型的这个 py 文件中","tags":["PyTorch"],"categories":["PyTorch"]},{"title":"目标检测中的 IoU 与 mAP","url":"/2018/05/28/IoUmAP/","content":"\n\n\n\n## IoU (Intersection over Union)\n\nIoU 是一个简单的测量标准，只要是在输出中得出一个预测范围(bounding boxex)的任务都可以用 IoU 来进行测量。为了可以使 IoU 用于测量任意大小形状的物体检测，我们需要： \n\n1. ground-truth bounding boxes（人为在训练集图像中标出要检测物体的大概范围）； \n2. 我们的算法得出的结果范围。\n\n<!-- more -->\n\n![](/img/2018-05-28-IoUmAP-101.jpg)\n\n\n![](/img/2018-05-28-IoUmAP-102.jpg)\n \n$$IoU=\\frac {Area\\ of\\ Overlap}{Area\\ of\\ Union}$$\n \n >一般来说，这个 score ＞ 0.5 可以被认为一个不错的结果\n \n**python 实现**\n\n```python\ndef bb_intersection_over_union(boxA, boxB):\n    # determine the (x, y)-coordinates of the intersection rectangle\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n\n    # compute the area of intersection rectangle\n    interArea = (xB - xA + 1) * (yB - yA + 1)\n\n    # compute the area of both the prediction and ground-truth\n    # rectangles\n    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the interesection area\n    iou = interArea / float(boxAArea + boxBArea - interArea)\n\n    # return the intersection over union value\n    return iou\n\n```\n\n>检测物体轮廓不一定非得是方框，也可以是沿着物体的边线，在实际的任务中，根据不同的任务要求来写不同具体实现的检测方法，但说白了其实都是IoU或者IU。 \n>\n>另外 mean IU 指的是不同类别识别准确度的平均值，比如一幅图中要识别三个物体，mean IU 就是三个物体分别准确度加起来的平均值。\n\n<br/>\n\n## mAP (Mean Average Precision)\n\n1. 对于某个**类别C**，在某一张图片上首先计算C在一张图片上的 \\\\(Precision=\\frac {在一张图片上类别C识别正确的个数（也就是IoU>0.5）}{一张图片上类别C的总个数}\\\\) \n\n$$Precision\\_C\\ =\\ \\frac {N\\(TruePositives\\)\\_C}{N\\(TotalObjects\\)\\_C}$$\n\n2. 依然对于某个类别C，可能在多张图片上有该类别，下面计算类别C的AP指数： \\\\(AP\\ =\\ \\frac {每张图片上的Precision求和}{含有类别C的图片数目}\\\\)\n\n$$AveragePrecision\\_C\\ =\\ \\frac {\\sum Precision\\_C}{N\\(TotalImages\\)\\_C}$$ \n\n3. 对于整个数据集，存在多个类别C1、C2、C3： \\\\(mAP\\ =\\ \\frac {上一步计算的所有类别的AP和}{总的类别数目相当于所有类别的AP的平均值}\\\\) \n\n$$MeanAveragePrecision\\ =\\ \\frac {\\sum AveragePrecision\\_C}{N(Classes)}$$\n\n\n## 参考\n\n1. [What is mAP ? Understanding the statistic of choice for comparing Object Detection models](https://link.zhihu.com/?target=http%3A//tarangshah.com/blog/2018-01-27/what-is-map-understanding-the-statistic-of-choice-for-comparing-object-detection-models/)\n2. [目标检测中的mAP是什么含义?](https://www.zhihu.com/question/53405779)\n3. [深度学习中IU、IoU(Intersection over Union)的概念理解以及python程序实现](https://blog.csdn.net/iamoldpan/article/details/78799857)\n\n\n\n\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>","tags":["目标检测"],"categories":["计算机视觉"]},{"title":"Perceptual Loss 感知损失 图像超分辨重建（二）","url":"/2018/04/28/PerceptualLosses/","content":"\n\n## 1 概述\n\n>文章：[Perceptual Losses for Real-Time Style-Transfer and Super-Resolution](https://arxiv.org/pdf/1603.08155.pdf)      \n>作者： Justin Johnson, Alexandre Alahi, Li Fei-Fei\n\n\n相较于其他机器学习任务，如物体检测（object detection）或者实例分割（instance segmentation），超分辨重建技术中学习任务的损失函数的定义通常都相对简单粗暴，由于我们重建的目的是为了使得重建的高分辨率图片与真实高清图片之间的峰值信噪比（Peak Signal-to-Noise Ratio, PSNR）尽可能的大，因此绝大多数的基于深度学习的超分辨重建研究都直接的将损失函数设计为**平均均方差（Mean Square Error, MSE）**，即计算两幅图片所有对应像素位置点之间的均方差，由于MSE Loss要求像素点之间位置一一对应，因此又被称作**Per-Pixel Loss**。\n\n<!-- more -->\n\n但随着技术的发展，研究者慢慢发现Per-Pixel Loss的**局限性**。考虑一个极端的情况，将高清原图向任意方向偏移一个像素，事实上图片本身的分辨率与风格并未发生太大的改变，但Per-Pixel Loss却会因为这一个像素的偏移而出现显著的上升，因此Per-Pixel Loss的约束并不能反应图像高级的特征信息（high-level features）。\n\n**图像风格转换算法**的成功，在生成图像领域，产生了一个非常重要的**idea**，那就是可以**将卷积神经网络提取出的feature，作为目标函数的一部分**，通过比较待生成的图片经过CNN的feature值与目标图片经过CNN的feature值，使得待生成的图片与目标图片在语义上更加相似(相对于Pixel级别的损失函数)。\n\n>图像风格转换算法将图片生成以生成的方式进行处理，如风格转换，是从一张噪音图（相当于白板）中得到一张结果图，具有图片A的内容和图片B的风格。而Perceptual Losses则是将生成问题看做是变换问题。即生成图像是从内容图中变化得到。\n\n图像风格转换是针对待生成的图像进行求导，CNN的反向传播由于参数众多，是非常慢的，同样利用卷积神经网络的feature产生的loss，训练了一个神经网络，将内容图片输入进去，可以直接输出转换风格后的图像。而将低分辨率的图像输入进去，可以得到高分辨率的图像。**因为只进行一次网络的前向计算，速度非常快**，可以达到实时的效果。\n\n\n研究图像风格迁移的研究者们相对于Per-Pixel Loss在2016年的CVPR会议上提出了**Perceptual Loss**的概念。\n\n## 2 构架\n\n下面这个网络图是论文的精华所在。图中将网络分为**Transform网络**和**Loss网络**两种，在使用中，Transform网络用来对图像进行转换，它的参数是变化的，而Loss网络，则保持参数不变，Transform的结果图，风格图和内容图都通过Loss Net得到每一层的feature激活值，并以之进行Loss计算。\n\n![](/img/2018-04-28-PerceptualLosses-1.jpg)\n<center><small><font color=gray>基于Perceptual Loss的全卷积网络结构</font></small></center>\n\n<br />\n\n>在风格转换上，输入 \\\\(x=y\\_c\\\\) 是内容图片。而在图片高清化上， \\\\(x\\\\) 是低分辨率图片，内容图片是高分辨率图片，风格图片未曾使用。\n\n\n## 3 网络细节\n\n\n网络细节的设计大体遵循DCGAN中的**设计思路**：\n\n- 不使用pooling层，而是使用strided和fractionally strided卷积来做downsampling和upsampling，\n- 使用了五个residual blocks\n- 除了输出层之外的所有的非residual blocks后面都跟着spatial batch normalization和ReLU的非线性激活函数。\n- 输出层使用一个scaled tanh来保证输出值在 \\\\([0, 255]\\\\) 内。\n- 第一个和最后一个卷积层使用9×9的核，其他卷积层使用 \\\\(3\\times 3\\\\) 的核。\n\n\n## 4 损失函数\n\n\n同[图像风格转换(Image style transfer)](https://link.zhihu.com/?target=http%3A//blog.csdn.net/stdcoutzyx/article/details/53771471)算法类似，论文定义了两种损失函数。其中，损失网络都使用在ImageNet上训练好的VGG net，使用 \\\\(φ\\\\) 来表示损失网络。\n\n**损失函数由三部分组成**\n\n### 4.1 Feature Reconstruction Loss\n\n\n$$l^{\\phi,j}\\_{feat}(\\hat{y},y)={\\frac {1}{C\\_jH\\_jW\\_j}}{\\Vert {\\phi\\_{j}(\\hat{y})-\\phi\\_{j}({y})} \\Vert}^{2}\\_{2}$$\n\n- \\\\(j\\\\) 表示网络的第 \\\\(j\\\\) 层       \n-  \\\\(C\\_jH\\_jW\\_j\\\\) 表示第 \\\\(j\\\\) 层的 feature_map 的 size\n\n**使用不同层的重建效果如下：**\n\n![](/img/2018-04-28-PerceptualLosses-2.jpg)\n<center><small><font color=gray>不同深度的卷积层提取的图片特征示意图</font></small></center>\n\n<br />\n\n### 4.2 Style Reconstruction Loss\n\n对于风格重建的损失函数，首先要先计算Gram矩阵，\n\n\n$$G^{\\phi}\\_{j}(x)\\_{c,c′}={\\frac {1}{C\\_jH\\_jW\\_j}}{\\sum^{H\\_{j}}\\_{h=1}}{\\sum^{W\\_{j}}\\_{w=1}}{\\phi}\\_{j}(x)\\_{h,w,c}{\\phi}\\_{j}(x)\\_{h,w,c′}$$\n\n产生的 feature_map 的大小为 \\\\(C\\_jH\\_jW\\_j\\\\)，可以看成是 \\\\(C\\_j\\\\) 个特征，这些特征两两之间的内积的计算方式如上。\n\n\n$$l^{\\phi,j}\\_{style}(\\hat{y},y)=\\Vert {G\\_{j}^{\\phi}(\\hat{y})-G\\_{j}^{\\phi}({y})} \\Vert^{2}\\_{F}$$\n\n两张图片，在loss网络的每一层都求出Gram矩阵，然后对应层之间计算欧式距离，最后将不同层的欧氏距离相加，得到最后的风格损失。\n\n\n不同层的风格重建效果如下：\n\n![](/img/2018-04-28-PerceptualLosses-3.jpg)\n\n### 4.3 Simple Loss Function\n\n第三个部分不是必须的\n\n#### 1) Pixel Loss\n\npixel loss 是输出 \\\\(\\hat y\\\\) 和目标 \\\\(y\\\\) 之间的欧几里得距离，只在网络有需要匹配的 ground-truth target  \\\\(y\\\\) 时才使用\n\n$$l\\_{pixel}(\\hat{y}, y)=\\Vert {\\hat{y}}-y \\Vert^{2}\\_{2}$$\n\n\n#### 2）Total Variation Regularization\n\nTotal Variation Loss，实际上是一个平滑项（一个正则化项），目的是使生成的图像在局部上尽可能平滑，而它的定义和马尔科夫随机场（MRF）中使用的平滑项非常相似。\n\n$$l\\_{TV}(\\hat y)={\\sum}\\_{n}{\\Vert \\hat{y}\\_{n+1}- \\hat{y}\\_{n} \\Vert}^{2}\\_{2}$$\n\n- 其中 \\\\(y\\_{n+1}\\\\) 是 \\\\(y\\_n\\\\) 的相邻像素\n\n\n## 5 基于Per-Pixel Loss的超分辨重建网络\n\n基于Per-Pixel Loss的超分辨重建网络目标在于**直接最小化高清原图与超分辨重建图像之间的差异**，使得超分辨重建图像逐步逼近原图的清晰效果。但Perceptual Loss最小化的是**原图与重建图像的特征图之间的差异**，为了提高计算效率，Perceptual Loss中的特征图由固定权重值的卷积神经网络提取，例如在ImageNet数据集上预训练得到的VGG16网络，如下图所示，不同深度的卷积层提取的特征信息不同，反映的图像的纹理也不同。\n\n![](/img/2018-04-28-PerceptualLosses-4.jpg)\n<center><small><font color=gray>不同深度的卷积层提取的图片特征示意图</font></small></center>\n\n<br />\n\n因此研究者们在训练超分辨神经网络时，利用**跨间隔的卷积层**（strided convolution layer）代替池化层（pooling layer）构建全卷积神经网络（Fully Convolutional Network, FCN）进行超分辨重建，并在卷积层之间添加**残差结构**（residual block）以在保证网络拟合性能的前提下加深网络深度获得更佳表现。最终利用VGG16网络对原图与重建图像进行特征提取，最小化两者特征图之间的差异使得超分辨重建图像不断逼近原图的分辨率。\n\n## 参考以及转载\n\n1. [感知损失(Perceptual Losses)](https://zhuanlan.zhihu.com/p/24720434)\n\n\n\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>","tags":["超分辨"],"categories":["计算机视觉"]},{"title":"python-opencv 方法总结","url":"/2018/04/28/opencv/","content":"\n\n### 读取图片\n\n```python\nimport cv2\nimport numpy as np\n\n# read the original\nimg = cv2.imread('test.jpg')\n```\n\n<!-- more -->\n### 显示图像\n\n```python\ncv2.imshow('original', img)   # 第一个参数是窗口的名字\n\ncv2.waitKey(0) \n\"\"\"\ncv2.waitKey() 是一个键盘绑定函数。需要指出的是它的时间尺度是毫\n秒级。函数等待特定的几毫秒，看是否有键盘输入。特定的几毫秒之内，如果\n按下任意键，这个函数会返回按键的ASCII 码值，程序将会继续运行。如果没\n有键盘输入，返回值为-1，如果我们设置这个函数的参数为0，那它将会无限\n期的等待键盘输入。它也可以被用来检测特定键是否被按下，例如按键a 是否\n被按下，这个后面我们会接着讨论。\n\"\"\"\n```\n\n### 图像尺寸\n\n```\nprint(im.shape)\n\n# (370, 463, 3)\n```\n\n### 保存图像\n\n```\ncv2.imwrite('lena.png',img)\n```\n\n\n### 颜色空间转换\n\n```\ncv2.cvtColor(input_image ，flag)  # flag是转换类型\n```\n\nBGR和灰度图的转换使用 `cv2.COLOR_BGR2GRAY` \nBGR和HSV的转换使用 `cv2.COLOR_BGR2HSV`\n\n### 图像缩放\n\n```\ncv2.resize(src,dsize,dst=None,fx=None,fy=None,interpolation=None)\n```\n\n```python\nres=cv2.resize(image,(2*width,2*height),interpolation=cv2.INTER_CUBIC) \n# 或者 \nres=cv2.resize(image,None,fx=2,fy=2,interpolation=cv2.INTER_CUBIC) \n# 此处None本应该是输出图像的尺寸，因为后边设置了缩放因子  \n```\n\n- scr：原图\n- dsize：输出图像尺寸\n- fx：沿水平轴的比例因子\n- fy：沿垂直轴的比例因子\n- interpolation：插值方法\n\n| interpolation 选项 | 所用的插值方法 | \n| - | :- |  \n| INTER_NEAREST | 最近邻插值 |  \n| INTER_LINEAR | 双线性插值（默认设置） |\n| INTER_AREA | 使用像素区域关系进行重采样。 它可能是图像抽取的首选方法，因为它会产生无云纹理的结果。 但是当图像缩放时，它类似于INTER_NEAREST方法。|\n|INTER_CUBIC | 4x4像素邻域的双三次插值|\n|INTER_LANCZOS4 | 8x8像素邻域的Lanczos插值|\n\n### 通道的拆分/合并处理\n\n有时需要对BGR三个通道分别进行操作。这时需要将BGR拆分成单个通道。同时有时需要把独立通道的图片合并成一个BGR图像。\n\n\n**使用OpenCV库函数版本**\n    \n```\nimport cv2 import numpy as np \nimport matplotlib.pyplot as plt\nimage=cv2.imread('pitt1.jpg')\nrows,cols,ch=image.shape \n#拆分通道，cv2.split()是一个比较耗时的操作。只有需要时使用，尽量Numpy\nb,g,r=cv2.split(image) \nprint b.shape #(768,1024) \n#合并通道 \nimage=cv2.merge(b,g,r) \n```    \n\n\n**使用Numpy索引版本**\n    \n```\nimport cv2 import numpy as np \nimport matplotlib.pyplot as plt\nimage=cv2.imread('pitt1.jpg')\nrows,cols,ch=image.shape \n#直接获取 \nb=img[:,:,0]\n```","tags":["python 模块"],"categories":["python"]},{"title":"TensorFlow 函数总结","url":"/2018/04/28/tensorflow/","content":"\n## tf 函数\n\n\n\n### tf.split\ntf.split()：axis的意思就是输入张量的哪一个维度，如果是0就表示对第0维度进行切割。num\\_or\\_size\\_splits就是切割的数量，如果是2就表示输入张量被切成2份，每一份是一个列表。\n\n```python\ntf.split(\n    value,\n    num_or_size_splits,\n    axis=0,\n    num=None,\n    name='split'\n)\n```\n如果 num\\_or\\_size\\_splits 传入的是一个整数，这个整数代表这个张量最后会被切成几个小张量。此时，传入 axis 的数值就代表切割哪个维度（从0开始计数）。调用 tf.split(my\\_tensor, 2，0) 返回两个 10 * 30 * 40 的小张量。\n\n\n```python\nimport tensorflow as tf\n\nA = [[1, 2, 3], [4, 5, 6]]\na0 = tf.split(A, num_or_size_splits=3, axis=1)#不改变维数（！！）\na1 = tf.unstack(A, num=3,axis=1)\na2 = tf.split(A, num_or_size_splits=2, axis=0)\na3 = tf.unstack(A, num=2,axis=0)\nwith tf.Session() as sess:\n    print(sess.run(a0))\n    print(sess.run(a1))\n    print(sess.run(a2))\n    print(sess.run(a3))\n       \n[array([[1],[4]]), array([[2],[5]]), array([[3],[6]])]\n[array([1, 4]), array([2, 5]), array([3, 6])] \n[array([[1, 2, 3]]), array([[4, 5, 6]])] \n[array([1, 2, 3]), array([4, 5, 6])]    \n```\n\n如果 num\\_or\\_size\\_splits 传入的是一个向量，那么向量有几个分量就分成几份，切割的维度还是由 axis 决定。比如调用 `tf.split(my\\_tensor, [10, 5, 25], 2)`，则返回三个张量分别大小为 20 × 30 × 10、20 × 30 × 5、20 × 30 × 25。很显然，传入的这个向量各个分量加和必须等于 axis 所指示原张量维度的大小 (10 + 5 + 25 = 40)。\n\n<!-- more -->\n### tf.concat\n\n连接两个（或多个）通道（矩阵）\n\n```python\ntf.concat(\n    values,\n    axis,\n    name='concat'\n)\n\n# axis：0表示行，1表示列\n```\n\n```pythonn\n>>> t1 = [[1, 2, 3], [4, 5, 6]]\n>>> t2 = [[7, 8, 9], [10, 11, 12]]\n\n>>> print(sess.run(tf.concat([t1, t2], 0)))\n[[ 1  2  3]\n [ 4  5  6]\n [ 7  8  9]\n [10 11 12]]\n\n>>> print(sess.run(tf.concat([t1, t2], 1)))\n[[ 1  2  3  7  8  9]\n [ 4  5  6 10 11 12]]\n \n# tensor t3 with shape [2, 3]\n# tensor t4 with shape [2, 3]\ntf.shape(tf.concat([t3, t4], 0))  # [4, 3]\ntf.shape(tf.concat([t3, t4], 1))  # [2, 6] \n\n\n>>> t1 = [[[1, 1, 1],[2, 2, 2]],[[3, 3, 3],[4, 4, 4]]]\n>>> t2 = [[[5, 5, 5],[6, 6, 6]],[[7, 7, 7],[8, 8, 8]]]\n\n>>> print(sess.run(tf.concat([t1, t2], 1)))\n[[[1 1 1]\n  [2 2 2]\n  [5 5 5]\n  [6 6 6]]\n\n [[3 3 3]\n  [4 4 4]\n  [7 7 7]\n  [8 8 8]]]\n\n>>> print(sess.run(tf.concat([t1, t2], 0)))\n[[[1 1 1]\n  [2 2 2]]\n\n [[3 3 3]\n  [4 4 4]]\n\n [[5 5 5]\n  [6 6 6]]\n\n [[7 7 7]\n  [8 8 8]]]\n```\n\n\n### tf.cond\n\n```python\ncond （ \n    pred ， \n    true_fn = None ， \n    false_fn = None ， \n    strict = False ， \n    name = None ， \n    fn1 = None ， \n    fn2 = None\n ）\n```\n\n如果断言 pred 为 true 则返回 true_fn() ，否则返回 false_fn()\n\n**例子**\n\n```python\nimport tensorflow as tf\na = tf.constant(2)\nb = tf.constant(3)\nx = tf.constant(4)\ny = tf.constant(5)\nz = tf.multiply(a, b)\nresult = tf.cond(x < y, lambda: tf.add(x, z), lambda: tf.square(y))\nwith tf.Session() as session:\n    print(result.eval())\n    print(z.eval())\n\n>>>10\n>>>6\n```\n\n**作用**\n\n在 dropout 中判断是否在训练：\n\n```python\nself.is_training = tf.placeholder(tf.bool)\n\n...\n...\n...\n\ndef dropout_with_keep():\n    return tf.nn.dropout(conv_a, dropout_keep_prob)\n\ndef dropout_no_keep():\n    return tf.nn.dropout(conv_a, 1.0)\n\nif dropout_keep_prob != -1:\n    conv_o_dr = tf.cond(is_training, dropout_with_keep, dropout_no_keep)\nelse:\n    conv_o_dr = conv_a\n\n```\n**或者：**\n\n```python\nwith tf.variable_scope('control'):\n    # it controls dropout and batch_norm layers\n    is_training = tf.placeholder_with_default(True, [], 'is_training')\n\n...\n...\n...\n\ndef _dropout(X, is_training, rate=0.5):\n    keep_prob = tf.constant(\n        1.0 - rate, tf.float32,\n        [], 'keep_prob'\n    )\n    result = tf.cond(\n        is_training,\n        lambda: tf.nn.dropout(X, keep_prob),\n        lambda: tf.identity(X),\n        name='dropout'\n    )\n    return result\n```\n\n### tf.tile\n\n`tf.tile()` 应用于需要张量扩展的场景，具体说来就是： \n如果现有一个形状如 `[width, height]` 的张量，需要得到一个基于原张量的，形状如 `[batch_size,width,height]` 的张量，其中每一个 batch 的内容都和原张量一模一样。\n\n```python\ntile(\n    input,\n    multiples,\n    name=None\n)\n```\n\n**示例**\n\n\n```python\nimport tensorflow as tf\ntemp = tf.tile([1,2,3],[2])\ntemp2 = tf.tile([[1,2],[3,4],[5,6]],[2,3])\nwith tf.Session() as sess:\n    print(sess.run(temp))\n    \t\n    \t# [1 2 3 1 2 3]\n    \ntemp = tf.tile([[1,2,3],[1,2,3]],[1,1])\ntemp2 = tf.tile([[1,2,3],[1,2,3]],[2,1])\ntemp3 = tf.tile([[1,2,3],[1,2,3]],[2,2])\nwith tf.Session() as sess:\n    print(sess.run(temp))\n    \n    \t# [[1 2 3] \n\t\t  [1 2 3]]\n\t\t  \n    print(sess.run(temp2))\n    \n    \t# [[1 2 3] \n\t\t  [1 2 3] \n\t\t  [1 2 3] \n\t\t  [1 2 3]]\n\t\t  \n    print(sess.run(temp3))\n    \n    \t# [[1 2 3 1 2 3] \n\t\t  [1 2 3 1 2 3] \n\t\t  [1 2 3 1 2 3] \n\t\t  [1 2 3 1 2 3]]\n```\n\n\n\n### tf.contrib.layers.flatten\n\n`tf.contrib.layers.flatten(P)` 这个函数就是把P保留第一个维度，把第一个维度包含的每一子张量展开成一个行向量，返回张量是一个二维的，`shape = (batch_size, ...)`, 一般用于卷积神经网络全链接层前的预处理。\n\n例如 CNN 的 conv 层输出的 tensor 的 shape 为  \\\\(\\text{[batch\\_size, height, width, channel]}\\\\), 刚展开会就是  \\\\(\\text{[batch\\_size, height × width × channel]}\\\\)。 \n\n### tf.contrib.layers.fully\\_connection\n\n\n`tf.contrib.layers.fully_connection(F，num_output,activation_fn)` 这个函数就是全链接成层, `F` 是输入，`num_output` 是下一层单元的个数，`activation_fn` 是激活函数，默认是 `relu`\n\n## tf.Variable与tf.get\\_variable()\n```    \ntf.Variable(initial_value=None, trainable=True, \\\n\tcollections=None, validate_shape=True, \\\n\tcaching_device=None, name=None, \\\n\tvariable_def=None, dtype=None, \\\n\texpected_shape=None, import_scope=None)\n```\n\n   \n```    \ntf.get_variable(name, shape=None, dtype=None, \\\n\tinitializer=None, regularizer=None, trainable=True, \\\n\tcollections=None, caching_device=None, \\\n\tpartitioner=None, validate_shape=True, custom_getter=None)\n```\n\n**区别**\n\n1. 使用`tf.Variable`时，如果检测到命名冲突，系统会自己处理。使用`tf.get_variable()`时，系统不会处理冲突，而会报错。\n\n\t基于这两个函数的特性，当我们需要共享变量的时候，需要使用tf.get_variable()。在其他情况下，这两个的用法是一样的\n\n2. 对于tf.Variable函数，变量名称是一个可选的参数，通过name=\"v\"的形式给出。而tf.get_variable函数，变量名称是一个必填的参数，它会根据变量名称去创建或者获取变量。\t\n\t\n    \n```python\nimport tensorflow as tf\nw_1 = tf.Variable(3,name=\"w_1\")\nw_2 = tf.Variable(1,name=\"w_1\")\nprint w_1.name\nprint w_2.name\n\n#输出\n#w_1:0\n#w_1_1:0\n```\n\n```python\nimport tensorflow as tf\n\nw_1 = tf.get_variable(name=\"w_1\",initializer=1)\nw_2 = tf.get_variable(name=\"w_1\",initializer=2)\n\n#错误信息\n#ValueError: Variable w_1 already exists, disallowed. Did\n#you mean to set reuse=True in VarScope?\n```\n\n```python\n#定义的基本等价  \nv = tf.get_variable(\"v\",shape=[1],initializer.constant_initializer(1.0))  \nv = tf.Variable(tf.constant(1.0,shape=[1]),name=\"v\")  \n```\n    \n    \n### tf.name\\_scope() / tf.variable\\_scope()    \n    \n>主要目的是为了更加方便地管理参数命名  \n\n**tf.name\\_scope 主要结合 tf.Variable() 来使用，方便参数命名管理。**\n\n```python\nimport tensorflow as tf\n\nwith tf.name_scope('conv1') as scope:\n    weights1 = tf.Variable([1.0, 2.0], name='weights')\n    bias1 = tf.Variable([0.3], name='bias')\n\n# 下面是在另外一个命名空间来定义变量的\nwith tf.name_scope('conv2') as scope:\n    weights2 = tf.Variable([4.0, 2.0], name='weights')\n    bias2 = tf.Variable([0.33], name='bias')\n\n# 所以，实际上weights1 和 weights2 这两个引用名指向了不同的空间，不会冲突\nprint(weights1.name)\nprint(weights2.name)\n\n>>>conv1/weights:0\n>>>conv2/weights:0\n```\n\n**tf.variable\\_scope() 主要结合 tf.get\\_variable() 来使用，实现变量共享。**\n\n\n```python\nimport tensorflow as tf\n# 注意， bias1 的定义方式\nwith tf.variable_scope('v_scope') as scope1:\n    Weights1 = tf.get_variable('Weights', shape=[2, 3])\n#     bias1 = tf.Variable([0.52], name='bias')\n\n# 下面来共享上面已经定义好的变量\n# note: 在下面的 scope 中的get_variable()变量必须已经定义过了，才能设置 reuse=True，否则会报错\nwith tf.variable_scope('v_scope', reuse=True) as scope2:\n    Weights2 = tf.get_variable('Weights')\n    bias2 = tf.Variable([0.52], name='bias')\n\nprint(Weights1.name)\nprint(Weights2.name)\nprint(bias2.name)\n\n>>>v_scope/Weights:0\n>>>v_scope/Weights:0\n>>>v_scope_1/bias:0\n```\n\n### tf.control\\_dependencies()\n\n```python\ntf.control_dependencies(self, control_inputs)\n```\n\n通过以上的解释，我们可以知道，该函数接受的参数control_inputs，是Operation或者Tensor构成的list。返回的是一个上下文管理器，该上下文管理器用来控制在该上下文中的操作的依赖。也就是说，上下文管理器下定义的操作是依赖control_inputs中的操作的，control_dependencies用来控制control_inputs中操作执行后，才执行上下文管理器中定义的操作。\n\n**例子**\n\n如果我们想要确保获取更新后的参数，name我们可以这样组织我们的代码。\n\n```\nopt = tf.train.Optimizer().minize(loss)\n\nwith tf.control_dependencies([opt]): #先执行opt\n  updated_weight = tf.identity(weight)  #再执行该操作\n\nwith tf.Session() as sess:\n  tf.global_variables_initializer().run()\n  sess.run(updated_weight, feed_dict={...}) # 这样每次得到的都是更新后的weight\n```\n\n### tf.placeholder\\_with_default()\n\n```python\nplaceholder_with_default(\n    input,\n    shape,\n    name=None\n)\n```\n\n该函数将返回一个张量。与 input 具有相同的类型。一个占位符张量，默认为 input 的占位符张量 (如果未送入)。\n\n```python\nwith tf.variable_scope('inputs'):\n    X = tf.placeholder_with_default(\n        data['x_batch'], [None, IMAGE_SIZE, IMAGE_SIZE, 3], 'X')\n    Y = tf.placeholder_with_default(\n        data['y_batch'], [None, NUM_CLASSES], 'Y')\n```\n\n\n### tf.device()\n\n如果需要切换成CPU运算，可以调用 `tf.device(device_name)` 函数，其中 `device_name` 格式如 `/cpu:0` 其中的0表示设备号，TF 不区分 CPU 的设备号，设置为0即可。GPU 区分设备号 `\\gpu:0` 和 `\\gpu:1` 表示两张不同的显卡。 \n\n在一些情况下，我们即使是在GPU下跑模型，也会将部分Tensor储存在内存里，因为这个Tensor可能太大了，显存不够放，相比于显存，内存一般大多了，于是这个时候就常常人为指定为CPU设备。这种形式我们在一些代码中能见到。如：\n\n```python\nwith tf.device('/cpu:0'):\n    build_CNN() # 此时，这个CNN的Tensor是储存在内存里的，而非显存里。\n```\n\n**例子**\n\n```python\nwith tf.device('/cpu:0'), tf.variable_scope('input_pipeline'):\n    data = _get_data(NUM_CLASSES, IMAGE_SIZE)\n```\n\n\n### tf.Graph()\n\n\n一个TensorFlow的运算，被表示为一个数据流的图。 \n一幅图中包含一些操作（Operation）对象，这些对象是计算节点。前面说过的Tensor对象，则是表示在不同的操作（operation）间的数据节点\n\n你一旦开始你的任务，就已经有一个默认的图已经创建好了。而且可以通过调用tf.get_default_graph()来访问到。 \n添加一个操作到默认的图里面，只要简单的调用一个定义了新操作的函数就行。比如下面的例子展示的：\n\n```python\nimport tensorflow as tf\nimport numpy as np\n\nc=tf.constant(value=1)\nassert c.graph is tf.get_default_graph()\nprint(c.graph)\nprint(tf.get_default_graph())\n\n# 输出\n<tensorflow.python.framework.ops.Graph object at 0x107324cc0>\n<tensorflow.python.framework.ops.Graph object at 0x107324cc0>\n```\n\n另外一种典型的用法就是要使用到 `Graph.as_default()` 的上下文管理器（ context manager），它能够在这个上下文里面覆盖默认的图。如下例：\n\n```python\nimport tensorflow as tf\nimport numpy as np\n\nc = tf.constant(value=1)\nprint(c.graph)\nprint(tf.get_default_graph())\nprint()\n\ng = tf.Graph()\nprint(\"g:\", g)\nwith g.as_default():\n    d = tf.constant(value=2)\n    print(d.graph)\n    print()\n\ng2 = tf.Graph()\nprint(\"g2:\", g2)\nwith g2.as_default():\n    e = tf.constant(value=15)\n    print(e.graph)\n    print()\n\nf = tf.constant(value=1)\nprint(f.graph)\n\n\n# 输出\n<tensorflow.python.framework.ops.Graph object at 0x104845da0>\n<tensorflow.python.framework.ops.Graph object at 0x104845da0>\n\ng: <tensorflow.python.framework.ops.Graph object at 0x1815af77f0>\n<tensorflow.python.framework.ops.Graph object at 0x1815af77f0>\n\ng2: <tensorflow.python.framework.ops.Graph object at 0x1815af7748>\n<tensorflow.python.framework.ops.Graph object at 0x1815af7748>\n\n<tensorflow.python.framework.ops.Graph object at 0x104845da0>\n```\n\n可以看到，如果在 `with` 外的话，graph 是系统默认的图，而不是带 `with` 的默认图\n\n\n### 基本 CNN 函数\n\n```python\ndef _batch_norm(X, is_training):\n    return tf.layers.batch_normalization(\n        X, scale=False, center=True,\n        momentum=BATCH_NORM_MOMENTUM,\n        training=is_training, fused=True\n    )\n\n\ndef _global_average_pooling(X):\n    return tf.reduce_mean(\n        X, axis=[1, 2],\n        name='global_average_pooling'\n    )\n\n\ndef _max_pooling(X):\n    return tf.nn.max_pool(\n        X, [1, 3, 3, 1], [1, 2, 2, 1], 'SAME',\n        name='max_pooling'\n    )\n\n\ndef _avg_pooling(X):\n    return tf.nn.avg_pool(\n        X, [1, 3, 3, 1], [1, 2, 2, 1], 'SAME',\n        name='avg_pooling'\n    )\n\ndef _nonlinearity(X):\n    return tf.nn.relu(X, name='ReLU')\n\n\ndef _dropout(X, is_training, rate=0.5):\n    keep_prob = tf.constant(\n        1.0 - rate, tf.float32,\n        [], 'keep_prob'\n    )\n    result = tf.cond(\n        is_training,\n        lambda: tf.nn.dropout(X, keep_prob),\n        lambda: tf.identity(X),\n        name='dropout'\n    )\n    return result\n\n\n\n# Reading cifar dataset\ndef unpickle(file):\n    import cPickle\n    f = open(file, 'rb')\n    dict = cPickle.load(f)\n    f.close()\n    return dict\n```\n\n\n### tf.one_hot()\n\n```python\nimport tensorflow as tf\n\n\nCLASS = 8\nlabel1 = tf.constant([0, 1, 2, 3, 4, 5, 6, 7])\nsess1 = tf.Session()\nprint('label1:', sess1.run(label1))\nb = tf.one_hot(label1, CLASS, 1, 0)\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    sess.run(b)\n    print('after one_hot', sess.run(b))\n    \n# 输出\nlabel1: [0 1 2 3 4 5 6 7]\nafter one_hot [[1 0 0 0 0 0 0 0]\n\t\t\t\t[0 1 0 0 0 0 0 0]\n \t\t\t\t[0 0 1 0 0 0 0 0]\n \t\t\t\t[0 0 0 1 0 0 0 0]\n\t\t\t\t[0 0 0 0 1 0 0 0]\n \t\t\t\t[0 0 0 0 0 1 0 0]\n\t\t\t\t[0 0 0 0 0 0 1 0]\n\t\t\t\t[0 0 0 0 0 0 0 1]]    \n```\n## tf 梯度\n\n### apply_gradients()\n\n`apply_gradients(grads_and_vars,global_step=None,name=None)`\n\n**作用：**    \n把梯度 “应用”（Apply）到变量上面去。其实就是按照梯度下降的方式加到上面去。这是 minimize() 函数的第二个步骤。 返回一个应用的操作。 \n\n**参数:**      \n`grads_and_vars`: `compute_gradients()` 函数返回的 `(gradient, variable)` 对的列表       \n`global_step`: Optional Variable to increment by one after the variables have been updated.        \n`name`: 可选，名字       \n\n\n### tf.gradients()\n\n```python\ntf.gradients(ys, xs, \n             grad_ys=None, \n             name='gradients',\n             colocate_gradients_with_ops=False,\n             gate_gradients=False,\n             aggregation_method=None,\n             stop_gradients=None)\n```\n\n对求导函数而言，其主要功能即求导公式： \\\\(\\frac{∂y}{∂x}\\\\) 。在tensorflow中， \\\\(y\\\\) 和 \\\\(x\\\\) 都是 tensor。\n\n\n更进一步，`tf.gradients()` 接受求导值 `ys` 和 `xs` 不仅可以是 tensor，还可以是 list，形如 `[tensor1, tensor2, …, tensorn]`。当 `ys` 和 `xs` 都是 list 时，它们的求导关系为：\n\n假设返回值是 \\\\([grad1, grad2, grad3]，ys=[y1, y2]，xs=[x1, x2, x3]\\\\) 。则，真实的计算过程为: \n\n$$grad1=\\frac{∂ y1}{∂x1}+\\frac{∂y2}{∂x1}$$\n$$grad2=\\frac{∂ y1}{∂x2}+\\frac{∂y2}{∂x2}$$\n$$grad3=\\frac{∂ y1}{∂x3}+\\frac{∂y2}{∂x3}$$\n\n**实例**\n\n以线性回归为例，实践 `tf.gradients()` 的基础功能。线性回归： \\\\(y=3×x+2\\\\) \n\n\n```python\nimport numpy as np\nimport tensorflow as tf\n\n\nsess = tf.Session()\n\nx_input = tf.placeholder(tf.float32, name='x_input')\ny_input = tf.placeholder(tf.float32, name='y_input')\nw = tf.Variable(2.0, name='weight')\nb = tf.Variable(1.0, name='biases')\ny = tf.add(tf.multiply(x_input, w), b)\nloss_op = tf.reduce_sum(tf.pow(y_input - y, 2)) / (2 * 32)\ntrain_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss_op)\n\ngradients_node = tf.gradients(loss_op, w)\nprint(gradients_node)\n\n\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n'''构造数据集'''\nx_pure = np.random.randint(-10, 100, 32)\nx_train = x_pure + np.random.randn(32) / 10  # 为x加噪声\ny_train = 3 * x_pure + 2 + np.random.randn(32) / 10  # 为y加噪声\n\nfor i in range(20):\n    _, gradients, loss = sess.run([train_op, gradients_node, loss_op],\n                                  feed_dict={x_input: x_train[i], y_input: y_train[i]})\n    print(\"epoch: {} \\t loss: {} \\t gradients: {}\".format(i, loss, gradients))\n\nsess.close()\n\n\n### 输出\n[<tf.Tensor 'gradients_1/Mul_grad/Reshape_1:0' shape=() dtype=float32>]\nepoch: 0 \t loss: 0.06110262870788574 \t gradients: [-0.064300433]\nepoch: 1 \t loss: 108.1637191772461 \t gradients: [-212.91476]\nepoch: 2 \t loss: 34.10615921020508 \t gradients: [61.373066]\nepoch: 3 \t loss: 16.358537673950195 \t gradients: [64.797104]\n\n...\nepoch: 18 \t loss: 0.0004277984262444079 \t gradients: [0.28949624]\nepoch: 19 \t loss: 0.00298550957813859 \t gradients: [1.0919241]\n```\n\n可以看到梯度逐渐减小，说明模型逐渐收敛\n\n\n### tf.stop_gradients()\n\nstop_gradients 也是一个 list，list 中的元素是 tensorflow graph 中的 op，一旦进入这个list，将不会被计算梯度，更重要的是，在该op之后的BP计算都不会运行。\n\n```python\na = tf.constant(0.)\nb = 2 * a\nc = a + b\ng = tf.gradients(c, [a, b])\n\n# 输出\n\n计算得 g = [3.0, 1.0]。因为 ∂c/∂a=∂a/∂a+∂b/∂a=3.0\n```\n\n但如果冻结operator a和b的梯度计算：\n\n```python\na = tf.constant(0.)\nb = 2 * a\ng = tf.gradients(a + b, [a, b], stop_gradients=[a, b])\n\n# 输出\n计算得g=[1.0, 1.0]\n```\n\n上面的代码也等效于：\n\n```python\na = tf.stop_gradient(tf.constant(0.))\nb = tf.stop_gradient(2 * a)\ng = tf.gradients(a + b, [a, b])\n```\n\n### 处理梯度\n\n\n1. 计算全部gradient \n\n  \n`gradient_all = optimizer.compute_gradients(loss)`   \n  \n2. 得到可进行梯度计算的变量  \n\n \n`grads_vars = [v for (g,v) in gradient_all if g is not None]`   \n\n3. 得到所需梯度   \n\n`gradient = optimizer.compute_gradients(loss, grads_vars)`   \n  \n4. 生成holder  \n\n  \n`grads_holder = [(tf.placeholder(tf.float32, shape=g.get_shape()), v) for (g,v) in gradient]`   \n  \n5. 继续进行BP算法   \n\n\n`train_op = optimizer.apply_gradients(grads_holder)`   \n\n## tf.contrib.slim\n\n### tf.contrib.slim.conv2d\n\n```python\nconvolution(inputs,\n          num_outputs,\n          kernel_size,\n          stride=1,\n          padding='SAME',\n          data_format=None,\n          rate=1,\n          activation_fn=nn.relu,\n          normalizer_fn=None,\n          normalizer_params=None,\n          weights_initializer=initializers.xavier_initializer(),\n          weights_regularizer=None,\n          biases_initializer=init_ops.zeros_initializer(),\n          biases_regularizer=None,\n          reuse=None,\n          variables_collections=None,\n          outputs_collections=None,\n          trainable=True,\n          scope=None)\n```\n\n- inputs：同样是指需要做卷积的输入图像\n- num_outputs：指定卷积核的个数（就是filter的个数）\n- kernel_size：用于指定卷积核的维度（卷积核的宽度，卷积核的高度）\n- stride：为卷积时在图像每一维的步长\n- padding：为padding的方式选择，VALID或者SAME\n- data_format：是用于指定输入的input的格式\n- rate：使用atrous convolution的膨胀率\n- activation_fn：用于激活函数的指定，默认的为ReLU函数\n- normalizer_fn：用于指定正则化函数\n- normalizer_params：用于指定正则化函数的参数\n- weights_initializer：用于指定权重的初始化程序\n- weights_regularizer：为权重可选的正则化程序\n- biases_initializer：用于指定biase的初始化程序\n- biases_regularizer: biases可选的正则化程序\n- reuse：指定是否共享层或者和变量\n- variable_collections：指定所有变量的集合列表或者字典\n- outputs_collections：指定输出被添加的集合\n- trainable：卷积层的参数是否可被训练\n- scope：共享变量所指的variable_scope\n\n\n\n\n## 1\n\n```python\n        # fc1\n        layer = tf.layers.dense(\n            inputs=self.tf_obs,\n            units=10,   # 输出个数\n            activation=tf.nn.tanh,  # 激励函数\n            kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.3),\n            bias_initializer=tf.constant_initializer(0.1),\n            name='fc1'\n        )\n        # fc2\n        all_act = tf.layers.dense(\n            inputs=layer,\n            units=self.n_actions,   # 输出个数\n            activation=None,    # 之后再加 Softmax\n            kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.3),\n            bias_initializer=tf.constant_initializer(0.1),\n            name='fc2'\n        )\n\n        self.all_act_prob = tf.nn.softmax(all_act, name='act_prob')  # 激励函数 softmax 出概率\n```\n\n## 2\n\n更新固定网络参数\n\n```python\n        with tf.variable_scope('update_oldpi'):\n            self.update_oldpi_op = [oldp.assign(p) for p, oldp in zip(pi_params, oldpi_params)]\n```\n\n\n## 待记录\n\nhttps://github.com/balancap/SSD-Tensorflow/blob/master/datasets/pascalvoc_to_tfrecords.py\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n","tags":["TensorFlow"],"categories":["TensorFlow"]},{"title":"SRCNN 图像超分辨重建（一）","url":"/2018/04/27/SuperResolution/","content":"\n\n\n\n\n\n## 1 概述\n\n\n分辨率极限，无论对于图像重建或是图像后处理算法的研究者，都是一项无法回避的技术指标。\n\n- **时间分辨率**性能决定了视频输出的**帧率**，即实时效果；\n- **空间分辨率**性能决定了图像的**画面清晰度**究竟是720P，1080P，还是4K；\n- **色阶分辨率**性能决定了图像**显示色彩的丰满程度与粒度**。\n\n因此，分辨率是一幅图像、一段视频的核心。\n\n<!-- more -->\n\n\n>**应用场景举例：**      \n图片压缩与传输，即以较低的码率进行图像编码，在传输过程中可极大节省转发服务器的流量带宽，在客户端解码得到相对低清晰度的图片，最后通过超分辨重建技术处理获得高清晰度图片\n\n**传统超分辨重建技术大体上可分为4类：**\n\n1. 预测型（prediction-based）\n2. 边缘型（edge-based） \n3. 统计型（statistical）\n4. 图像块型（patch-based/example-based）\n\n>目前大家使用最多的是**图像块型**\n\nSR 也可分为两类:\n\n1. 从多张低分辨率图像重建出高分辨率图像\n2. 从单张低分辨率图像重建出高分辨率图像\n\n基于深度学习的SR，主要是**基于单张低分辨率的重建方法**，即**Single Image Super-Resolution (SISR)**。\n\nSISR是一个**逆问题**，对于一个低分辨率图像，可能存在许多不同的高分辨率图像与之对应，因此通常在求解高分辨率图像时会加一个**先验信息进行规范化约束**。\n\n在传统的方法中，这个先验信息可以通过若干成对出现的低-高分辨率图像的实例中学到。**而基于深度学习的SR通过神经网络直接学习分辨率图像到高分辨率图像的端到端的映射函数。**\n\n我们在图像块型领域选择了4篇基于深度学习的图像块型超分辨重建的经典论文进行关键技术点分析。    \n从论文中我们可以看出研究者们对于超分辨任务的不同的理解与解决问题思路。\n\n## 2 SRCNN\n>Super-Resolution Convolutional Neural Network      \n>SRCNN, PAMI 2016, http://mmlab.ie.cuhk.edu.hk/projects/SRCNN.html\n\nSRCNN是基于深度学习的超分辨重建领域的开山之作，继承了传统机器学习领域稀疏编码的思想，该方法对于一个低分辨率图像，先使用双三次（bicubic）插值将其放大到目标大小，再通过三层卷积网络做非线性映射，得到的结果作为高分辨率图像输出。\n\n### 2.1 结构\n\n**利用三层卷积层分别实现:** \n\n1. 图像的图像块抽取与稀疏字典建立\n2. 图像高、低分辨率特征之间的非线性映射\n3. 高分辨率图像块的重建\n\n\n![](/img/2018-04-27-SuperResolution-1.jpg)\n\n\n**SRCNN的流程为：**\n\n1. 先将低分辨率图像使用双三次差值放大至目标尺寸（如放大至2倍、3倍、4倍），此时仍然称放大至目标尺寸后的图像为低分辨率图像(Low-resolution image)，即图中的输入(input)；\n2. 将低分辨率图像输入三层卷积神经网络，\n\n\t>举例：在论文中的其中一实验相关设置，对YCrCb颜色空间中的Y通道进行重建，网络形式为 \n\t>    \n\t>(conv1+relu1) \\\\(\\to\\\\) (conv2+relu2)  \\\\(\\to\\\\) (conv3+relu3)      \n\t>\n\t>三个卷积层使用的卷积核的大小分为为 \\\\(9\\times 9\\\\) ,  \\\\(1\\times 1\\\\) 和 \\\\(5\\times 5\\\\) ，前两个的输出特征个数分别为 \\\\(64\\\\) 和 \\\\(32\\\\)。   \n\t>\n\t>第一层卷积：卷积核尺寸 \\\\(9×9\\ (f\\_1×f\\_1)\\\\) ，卷积核数目 \\\\(64\\ (n\\_1)\\\\) ，输出64张特征图；    \n\t第二层卷积：卷积核尺寸 \\\\(1×1\\ (f\\_2×f\\_2)\\\\) ，卷积核数目 \\\\(32\\ (n\\_2)\\\\) ，输出32张特征图；     \n\t第三层卷积：卷积核尺寸 \\\\(5×5\\ (f\\_3×f\\_3)\\\\) ，卷积核数目 \\\\(1\\ (n\\_3)\\\\) ，输出1张特征图即为最终重建高分辨率图像。\n\n具体地，假设需要处理的低分辨率图片的尺寸为 \\\\(H \\times  W \\times  C\\\\) ， 其中 \\\\(H、W、C\\\\) 分别表示图片的长、宽和通道数；SRCNN第一层卷积核尺寸为 \\\\(C \\times  f\\_1 \\times  f\\_1 \\times  n\\_1\\\\) ,可以理解为在低分辨率图片上滑窗式地提取 \\\\(f\\_1 \\times  f\\_1\\\\) 的图像块区域进行n1种类型的卷积操作。在全图范围内，每一种类型卷积操作都可以输出一个特征向量，最终 \\\\(n\\_1\\\\) 个特征向量构成了低分辨率图片的稀疏表示的字典，字典的维度为 \\\\(H\\_1 \\times  W\\_1 \\times  n\\_1\\\\) ；\n\n\nSRCNN第二层卷积核尺寸为 \\\\(n\\_1 \\times  1 \\times  1 \\times  n\\_2\\\\) ，以建立由低分辨率到高分辨率稀疏表示字典之间的非线性映射，输出的高分辨率稀疏字典的维度为 \\\\(H\\_1 \\times  W\\_1 \\times  n\\_2\\\\) ，值得注意的是在这一步中SRCNN并未采用全连接层（fully connected layer）来进行特征图或是稀疏字典之间的映射，而是采用1x1卷积核，从而使得空间上每一个像素点位置的映射都共享参数，即每一个空间位置以相同的方式进行非线性映射； \n\n\n\nSRCNN第三层卷积核尺寸为 \\\\(n\\_2 \\times  f\\_3 \\times  f\\_3 \\times  C\\\\) ，由高分辨率稀疏字典中每一个像素点位置的 \\\\(n\\_2 \\times  1\\\\) 向量重建 \\\\(f\\_3 \\times  f\\_3\\\\) 图像块，图像块之间相互重合覆盖，最终实现图片的超分辨率重建。\n\n\n\n![](/img/2018-04-27-SuperResolution-2.jpg)\n\n\n**训练过程：**\n\n1. 训练数据集：论文中某一实验采用91张自然图像作为训练数据集，对训练集中的图像先使用双三次差值缩小到低分辨率尺寸，再将其放大到目标放大尺寸，最后切割成诸多33×33图像块作为训练数据，作为标签数据的则为图像中心的21×21图像块（与卷积层细节设置相关）；\n\n\t**注：**最原始的SRCNN输入不是低分辨率图像，而是低分辨率双立方插值后的图片\n\n2. 损失函数：采用MSE函数作为卷积神经网络损失函数；\n\n3. 卷积层细节设置：第一层卷积核 \\\\(9×9\\\\) ，得到特征图尺寸为 \\\\((33-9)/1+1=25\\\\) ，第二层卷积核 \\\\(1×1\\\\) ，得到特征图尺寸不变，第三层卷积核 \\\\(5×5\\\\) ，得到特征图尺寸为 \\\\((25-5)/1+1=21\\\\) 。训练时得到的尺寸为 \\\\(21×21\\\\) ，因此图像中心的 \\\\(21×21\\\\) 图像块作为标签数据。（卷积训练时不进行padding）\n\n\n### 2.2 重建效果\n\n对SR的质量进行定量评价常用的两个指标是 **PSNR(Peak Signal-to-Noise Ratio 峰值信噪比)** 和 **SSIM（Structure Similarity Index 结构相似性）**。这两个值越高代表重建结果的像素值和金标准越接近，下图表明，在不同的放大倍数下，SRCNN都取得比传统方法好的效果。\n\n![](/img/2018-04-27-SuperResolution-3.jpg)\n\n该文章分别用Timofte数据集（包含91幅图像）和ImageNet大数据集进行训练。相比于双三次插值和传统的稀疏编码方法，SRCNN得到的高分辨率图像更加清晰，下图是一个放大倍数为3的例子。\n\n![](/img/2018-04-27-SuperResolution-4.jpg)\n\n\n**实践：**\n\n1. [基于SRCNN的表情包超分辨率](https://blog.csdn.net/m0_37510087/article/details/79367649) [（代码）](https://github.com/AdamZhuang/machine-learning/blob/master/super_resolution/srcnn.py)\n2. [Tensorflow实现二次元图片的超分辨率](https://blog.csdn.net/AIchipmunk/article/details/53704139)\n\n\n\n## 参考以及转载\n\n1. [深度学习之图像超分辨重建技术](https://zhuanlan.zhihu.com/p/34979257)\n2. [深度学习在图像超分辨率重建中的应用](http://i.dataguru.cn/mportal.php?aid=10906&mod=view)\n3. [图像超分辨率重建之SRCNN](https://blog.csdn.net/Autism_/article/details/79401798)\n\n\n\n\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>","tags":["超分辨"],"categories":["计算机视觉"]},{"title":"matlab 基本函数","url":"/2018/04/25/matlab/","content":"\n### 添加图例\n\n使用 `hold on` \n\n```\n>> plot(x,y);\n>> hold on\n>> plot(x,y2);\n>> plot(x,y3);\n```\n<!-- more -->\n\n### 正态分布\n\n```\nY = normpdf(X,mu,sigma)   %pdf mu 均值 sigma 方差\np = normcdf(x,mu,sigma)   %cdf\n```\n\n\n### 泊松分布\n\n```\nR = poissrnd(LAMBDA,m,n)  % 产生以Lambda为平均值的m行n列Poisson 随机数\n\nY = poisspdf(X,lambda)   % 由x产生mean是lambda的泊松分布的pdf\n\nY = poisscdf(X,lambda)   % 由x产生mean是lambda的泊松分布的cdf\n```","tags":["matlab"],"categories":["matlab"]},{"title":"python 类中的 __str__() 和 __repr__() 方法","url":"/2018/04/24/strrepr/","content":"\n\n\n## 综述\n\nPython 定义了 `__str__()` 和 `__repr__()` 两种方法，`__str__()` 用于显示给用户，而 `__repr__()` 用于显示给开发人员。\n\n<!-- more -->\n\n<br />\n\n\nPython中这个 `_repr_` 函数，对应 `repr(object)` 这个函数，返回一个可以用来表示对象的可打印字符串：\n\n  1. 尝试生成这样一个字符串，将其传给 `eval()` 可重新生成同样的对象 ；\n\n  2. 否则，生成用尖括号包住的字符串，包含类型名和额外的信息(比如地址) ；\n\n  3. 一个类(class)可以通过 `__repr__()` 成员来控制repr()函数作用在其实例上时的行为。\n\n\n<br />\n## 例子\n\n1. 当一个类没有定义 `__str__()` 和 `__repr__()` 时，直接在命令行中输入这个类的示例以及打印示例，生成用尖括号包住的字符串，包含类型名和额外的信息(比如地址)：\n\n```python\n>>> class A():\n...     pass\n...\n>>> a = A()\n>>> a\n<__main__.A object at 0x108b59320>\n>>> print(a)\n<__main__.A object at 0x108b59320>\n```\n\n2. 当一个类定义了 `__str__()` ，没有定义 `__repr__()` 时，直接在命令行中输入这个类的示例以及打印示例：\n\n\n```python\n>>> class B():\n...     def __str__(self):\n...             return 'B'\n...\n>>> b = B()\n>>> b\n<__main__.B object at 0x108b59358>\n>>> print(b)\nB\n```\n\n3. 当一个类定义了 `__repr__()` ，没有定义 `__str__()` 时，直接在命令行中输入这个类的示例以及打印示例：\n\n```python\n>>> class C():\n...     def __repr__(self):\n...             return 'c'\n...\n>>> c = C()\n>>> c\nc\n>>> print(c)\nc\n```\n\n>虽然没有定义 `__str__()`， 但是两种方法都能打印出 `c`\n\n\n4. 同时定义了两者：\n\n```python\n>>> class D():\n...     def __str__(self):\n...             return '__str__ d'\n...     def __repr__(self):\n...             return '__repr__ d'\n...\n>>> d = D():\n  File \"<stdin>\", line 1\n    d = D():\n           ^\nSyntaxError: invalid syntax\n>>> d = D()\n>>> d\n__repr__ d\n>>> print(d)\n__str__ d\n```\n\n## 定义方法\n\n1. 有一个偷懒的定义__repr__的方法：\n\n\n```python\n>>> class Person(object):\n...     def __str__(self):\n...             return 'Person'\n...     __repr__ = __str__\n...\n>>> p = Person()\n>>> p\nPerson\n>>> print(p)\nPerson\n```\n\n2. 另一种方式：\n\n```python\n>>> class ListStack:\n...     def __str__(self):\n...             return 'ListStack'\n...     def __repr__(self):\n...             return str(self)\n...\n>>> l = ListStack()\n>>> l\nListStack\n>>> print(l)\nListStack\n```\n\n但若是没有定义 `__str__()`：\n\n```python\n>>> class E():\n...     def __repr__(self):\n...             return str(self)\n...\n>>> e = E()\n>>> e\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 3, in __repr__\n  File \"<stdin>\", line 3, in __repr__\n  File \"<stdin>\", line 3, in __repr__\n  [Previous line repeated 196 more times]\nRecursionError: maximum recursion depth exceeded while calling a Python object\n```","tags":["python"],"categories":["python"]},{"title":"算法与数据结构（学校笔记）","url":"/2018/04/23/Datastructure/","content":"\n\n## 1 Algorithm 算法\n\n- An algorithm is a step-by-step procedure for performing some task in a finite amount of time.\n\n\n### 1.1 Principle of algorithm analysis\n\n**How do we define a“good” algorithm?**\n\n- **Running time** is a natural measure of “goodness,” since time is a precious resource—computer solutions should run as fast as possible\n-  **Space usage** is another major issue to consider when we design an algorithm, since we only have limited storage spaces\n\n<!-- more -->\n\n**Measuring the running time:**\n\n```python\nfrom time import time\n\nstartTime = time()\n\nfor i in range(20000000):\n    if i % 100000 == 0:\n        print(1)\n\nendTime = time()\n\nprint('time: %f seconds' % (endTime - startTime))\n```\n\n\n**1) Counting primitive operations**\n\nTo analyse the running time of an algorithm without performing experiments, we perform an analysis directly **on a high-level description of the algorithm**\n\n**primitive operations:**  \n\n- Assigning an identifier to an object\n- Determining the object associated with an identifier\n- Performing an arithmetic operation (for example, adding two numbers) \n- Comparing two numbers\n- Accessing a single element of a Python list by index\n- Calling a function (excluding operations executed within the function) \n- Returning from a function.\n\n\n**2) Measuring Operations as a Function of Input Size**\n\nTo capture the order of growth of an algorithm’s running time, we will associate, with each algorithm, a function `f(n)` that characterizes the number of primitive operations that are performed as a function of the input size n.\n\n**3) Focusing on the Worst-Case Input**\n\n![](/img/2018-04-23-Datastructure-1.jpg)\n\n### 1.2 algorithm analysis\n\n**1) The 7 functions used in algorithm analysis**\n\n We may use the following 7 functions to measure the time complexity of an algorithm: \n \n - constant\n - logarithm\n - linear\n - N-log-N\n - quadratic\n - cubic and other polynomials\n - exponential\n\n![](/img/2018-04-23-Datastructure-2.jpg)\n\n**2) Asymptotic Analysis 渐进分析**\n\n\n- In algorithm analysis, we focus on the **growth rate** of the running time **as a function of the input size n**, taking a “big-picture” approach\n- Vocabulary for the analysis and design of algorithms\n- “Sweet spot” for high-level reasoning about algorithms\n- **Coarse enough** to **supress unnecessary details**, e.g. architecture/language/compiler...\n- **Sharp enough** to make **meaningful comparisons** between algorithms\n\n\n**3) The big Oh notation**\n\n- Let `f(n)` and `g(n)` be functions mapping positive integers to positive real numbers.\n- We say that `f(n)` is `O(g(n))` if there is a real constant `c > 0` and an integer constant `n0 ≥ 1` such that\n`f(n) ≤ cg(n)`, for `n ≥ n0`\n- This definition is often referred to as the “big-Oh” notation\n- Example: The function `8n+5` is `O(n)`, `g(n) = n` `c = 9`, `n0 = 10`\n\n\n**4) Some Properties of the Big-Oh Notation**\n\nThe big-Oh notation allows us to ignore constant factors and lower-order terms and focus on the main components of a function that affect its growth.\n\n![](/img/2018-04-23-Datastructure-3.jpg)\n\nIn general, we should use the big-Oh notation to characterize a function as closely as possible\n\n### 1.3 Comparative analysis\n\n**Q:**  \nAn algorithm A, which has a running time of ( O(n), and an algorithm B, which has a running time of O(𝑛 ). Which algorithm is better?\n\n**A:**    \nAlgorithm A is asymptotically (渐近) better than algorithm B, Constant of A may very large ,so it is not always better than b.\n\n![](/img/2018-04-23-Datastructure-4.jpg)\n\n**1) The line of tractability**\n\n- To differentiate efficient and inefficient algorithms, the general line is between polynomial time algorithms and exponential time algorithms\n- The distinction between polynomial-time and exponential- time algorithms is considered a robust measure of tractability\n\n### 1.4 Recursion 递归\n\n**1) definition**\n\n- First, a recursive definition contains one or more **base cases**, which are defined non-recursively in terms of fixed quantities\n- Second, it also contains one or more **recursive cases**, which are defined by appealing to the definition of the function being defined\n\n![](/img/2018-04-23-Datastructure-5.jpg)\n\n**2) How Python implements recursion**\n\n\n- In Python, each time a function (recursive or otherwise) is called, a structure known as an **activation record** or frame is created to store information about the progress of that invocation of the function\n- This **activation record** stores <u>**the function** call’s **parameters** and **local variables** (哪个函数被call了，函数的参数parametersxg，call这个函数的位置location)</u>\n- When the execution of a function leads to a nested function call, the execution of the former call is suspended（悬挂，暂停） and its activation record **stores the place in the source code** at which the flow of control should continue upon return of the nested call\n\n\n**3) The recursive trace**\n\n ![](/img/2018-04-23-Datastructure-101.jpg)\n\n**4) 递归算法的通用结构：**\n\n```python\ndef f():\n\tmake()\n\tif check():\n\t\tf()\n```\n\n### 1.5 Binary search 二分法\n\n A classic and very useful **recursive algorithm**.\n \n When the sequence is <u>unsorted</u>, the standard approach to search for a target value is to <u>use a loop to examine every element, until either finding the target or exhausting the data set</u>; This is known as the **sequential search algorithm**\n \n**Code:**\n\n ![](/img/2018-04-23-Datastructure-102.jpg)\n\n\n\n**Time complexity:**\n\n The binary search algorithm runs in `O(logn)` time for a sorted sequence with n elements.\n \n`N *（1/2）^x = 1`   ---->    `x = logn`\n\n### 1.6 Bubble sort 冒泡排序\n\n**Its general procedure is:**\n\n1. Iterate over a list of numbers, **compare** every element i with the following element i+1, and s**wap them if i is larger**\n2. Iterate over the list again and repeat the procedure in step 1, but **ignore the last element in the list**\n3. Continuously iterate over the list, but each time ignore one more element at the tail of the list, until there is only one element left\n\n**Code**\n\n![](/img/2018-04-23-Datastructure-6.jpg)\n\n### 1.7 Quick sort 快速排序\n\n**1) 分治思想**\n\n快速排序采用的思想是**分治思想**\n\n分治算法的基本思想是将一个规模为N的问题分解为K个规模较小的子问题，这些子问题相互独立且与原问题性质相同。求出子问题的解，就可以得到原问题的解。下面这张图会说明分治算法是如何进行的：将cn分成了两个cn/2，转而分成了cn/4，cn/8......我们通过这样一层一层的求解规模小的子问题，将其合并之后就能求出原问题的解。\n\n ![](/img/2018-04-23-Datastructure-103.jpg)\n\n**2) 快速排序**\n\n• The main procedure of quick sort algorithm is:\n\n1. Pick an element, called a **pivot**, from the array\n2. **Partitioning**: reorder the array so that: \n\n- all elements with **values less than the pivot come before the pivot**, \n- while all elements with **values greater than the pivot come after it** \n- **(equal values can go either way)**. \n\n\tAfter this partitioning, the pivot is in its final position. This is called the **partition operation**\n\n3. **Recursively 递归** apply the above steps to the sub-array of elements with smaller values and separately to the sub-array of elements with greater values\n\n\n![](/img/2018-04-23-Datastructure-7.jpg)\n\n**时间复杂度**最理想 `O(nlogn)` 最差情况（原数列已经有序） `O(n^2)`\n\n```python\ndef QuickSort(myList, start, end):\n\n    # 判断low是否小于high,如果为false,直接返回\n    if start < end:\n        i,j = start,end\n        # 设置基准数\n        # i负责找比base小的数，j负责找比base大的数\n        base = myList[i]\n\n        while i < j:\n            # 如果列表后边的数,比基准数大或相等,则前移一位直到有比基准数小的数出现\n            while (i < j) and (myList[j] >= base):\n                j = j - 1\n\n            # 如找到,则把第j个元素赋值给第个元素i,此时表中i,j个元素相等\n            myList[i] = myList[j]\n\n            # 同样的方式比较前半区\n            while (i < j) and (myList[i] <= base):\n                i = i + 1\n            myList[j] = myList[i]\n        # 做完第一轮比较之后,列表被分成了两个半区,并且i=j,需要将这个数设置回base\n        myList[i] = base\n\n        # 递归前半区\n        QuickSort(myList, start, i - 1)\n        # 递归后半区\n        QuickSort(myList, j + 1, end)\n    return myList\n    \nmyList = [49,38,65,97,76,13,27,49]\nQuickSort(myList, 0, len(myList)-1)\n```\n\n交换类似于交叉交换：\n\n```python\n[6, 1, 2, 7, 9, 3, 4, 5, 10, 8]\n ·                    ·\n[5, 1, 2, 7, 9, 3, 4, 5, 10, 8]\n          ·           ·\n[5, 1, 2, 7, 9, 3, 4, 7, 10, 8]\n          ·        ·\n[5, 1, 2, 4, 9, 3, 4, 7, 10, 8]\n             ·     ·\n[5, 1, 2, 4, 9, 3, 9, 7, 10, 8]\n             ·  ·\n[5, 1, 2, 4, 3, 3, 9, 7, 10, 8]\n                ·\n[5, 1, 2, 4, 3, 3, 9, 7, 10, 8]\n                ·\n[5, 1, 2, 4, 3, 6, 9, 7, 10, 8]\n```\n\n## 2 Data structure 数据结构\n\n- A data structure is a <u>systematic</u> way of organizing and accessing data\n\n\n### 2.1 stack 堆栈\n\n-  A stack is **a collection of objects** that are inserted and removed according to the **last-in, first-out** **(LIFO)** principle.\n- A user may **insert objects** into a stack **at any time**, but may only **access or remove the most recently inserted object** that remains (at the so-called “**top**” of the stack).\n\n\n**1) example** \n\n**Web Browser**\n\n- Internet Web browsers store the addresses of recently visited sites in a stack. Each time a user visits a new site, that site’s address is “pushed” onto the stack of addresses. The browser then allows the user to “pop” back to previously visited sites using the “back” button.\n\n**Text editor** \n\n- Text editors usually provide an **“undo”** mechanism that cancels recent editing operations and reverts to former states of a document. This undo operation can be accomplished by keeping text changes in a stack.\n\n\n**2) The stack class**\n\nGenerally, a stack may contain the following methods:\n\n![](/img/2018-04-23-Datastructure-8.jpg)\n\n ![](/img/2018-04-23-Datastructure-104.jpg)\n\n\n```python\nclass ListStack:\n\n    def __init__(self):\n        self.__data = list()\n\n    def __len__(self):\n        return len(self.__data)\n\n    def is_empty(self):\n        return len(self.__data) == 0\n\n    def push(self, e):\n        self.__data.append(e)\n\n    def top(self):\n        if self.is_empty():\n            print('The stack is empty.')\n        else:\n            return self.__data[self.__len__()-1]\n\n    def pop(self):\n        if self.is_empty():\n            print('The stack is empty.')\n        else:\n            return self.__data.pop()\n\n    def __str__(self):\n        return str(self.__data)\n\n    def __repr__(self):\n        return str(self)\n```\n\n\n**3) Practice**\n\n1.  Reverse a list using stack\n\n ![](/img/2018-04-23-Datastructure-105.jpg)\n\n\n\n2.  Brackets match checking\n\n ![](/img/2018-04-23-Datastructure-106.jpg)\n\n\n3.  Matching Tags in HTML Language\n\n\n ![](/img/2018-04-23-Datastructure-107.jpg)\n\n\n\n\n### 2.2 Queue 队列\n- Queue is another fundamental data structure\n- A queue is a collection of objects that are inserted and\nremoved according to the **first-in, first-out (FIFO) principle**\n- Elements can be inserted at any time, but only the element that has been in the queue the longest can be next removed\n\n\n**1) the queue class**\n\nThe queue class may contain the following methods:\n\n![](/img/2018-04-23-Datastructure-9.jpg)\n\n\n![](/img/2018-04-23-Datastructure-10.jpg)\n\n```python\nclass ListQueue:\n    default_capacity = 10\n\n    def __init__(self):\n        self.__data = [None] * ListQueue.default_capacity\n        self.__size = 0\n        self.__front = 0\n        self.__end = 0\n\n\n    def __len__(self):\n        return self.__size\n\n    def is_empty(self):\n        return self.__size == 0\n\n    def first(self):\n        if self.is_empty():\n            print('Queue is empty.')\n        else:\n            return self.__data[self.__front]\n\n    def dequeue(self):\n        if self.is_empty():\n            print('Queue is empty.')\n            return None\n        else:\n            answer = self.__data[self.__front]\n            self.__data[self.__front] = None\n            self.__front = (self.__front + 1) % ListQueue.default_capacity\n            self.__size -= 1\n            return answer\n\n    def enqueue(self,e):\n        if self.__size == ListQueue.default_capacity:\n            print('The queue is full.')\n        else:\n            self.__data[self.__end] = e\n            self.__end = (self.__end + 1) % ListQueue.default_capacity\n            self.__size += 1\n\n    def __str__(self):\n        return str(self.__data)\n    def __repr__(self):\n        return str(self)\n```\n\n**2) Practice: Simulating a web service**\n\nAn online video website handles service requests in the following way:\n\n1. It maintains a service queue which stores all the unprocessed **service requests**.\n2. When a new service request arrives, it will **be saved at the end of the service queue**.\n3. The server of the website will process each service request **on a “first-come-first-serve” basis**.\n\n\n### 2.3 Linked list 链表\n\n**1) list in python**\n\n- Python’s list class is highly optimized, and often a great choice for storage\n- However, many programming languages do not support this kind of optimized list data type\n\n同一个值可以被不同的list分享\n\n同一个list的相同值也可以在同一个id地址\n\n![](/img/2018-04-23-Datastructure-11.jpg)\n\n![](/img/2018-04-23-Datastructure-12.jpg)\n\n![](/img/2018-04-23-Datastructure-13.jpg)\n\n**2) Compact array**\n\n- A collection of numbers are usually stored as a compact array in languages such as C/C++ and Java\n- A compact array is storing the bits that represent the primary data (**not reference**)\n- The overall memory usage will be much lower for a compact structure because there is no overhead devoted to the explicit storage of the sequence of memory references (in addition to the primary data)\n\n\n**3) Linked List** \n\n链表是一个基础的数据结构，可以使用链表去实现其他各种数据结构\n\n- A singly linked list, in its simplest form, is a collection of nodes that collectively form a linear sequence\n- Each node stores a reference to an object that is an element of the sequence, as well as a reference to the next node of the list\n\n内存地址不一定是连续的\n\n- The first and last nodes of a linked list are known as the **head** and **tail** of the list, respectively\n- By starting at the head, and moving from one node to another by following each node’s next reference, we can reach the tail of the list\n- We can identify the tail as the node having None as its next reference. This process is commonly known as traversing the linked list.\n- Because the next reference of a node can be viewed as a link or pointer to another node, the process of traversing a list is also known as **link hopping** or **pointer hopping**\n\n![](/img/2018-04-23-Datastructure-14.jpg)\n\n![](/img/2018-04-23-Datastructure-15.jpg)\n\n**4) Inserting an Element at the Head**\n\n![](/img/2018-04-23-Datastructure-16.jpg)\n\n\n![](/img/2018-04-23-Datastructure-17.jpg)\n\n\n**5) Inserting an Element at the Tail**\n\n不论首尾首先先新建一个Node\n\n![](/img/2018-04-23-Datastructure-18.jpg)\n\n![](/img/2018-04-23-Datastructure-19.jpg)\n\n![](/img/2018-04-23-Datastructure-20.jpg)\n\n\n**6) Removing an Element from the head**\n\n被移去的node仍旧存在在内存中，但已经不在链表中，python会自动删除这些数据\n\n![](/img/2018-04-23-Datastructure-21.jpg)\n\n![](/img/2018-04-23-Datastructure-22.jpg)\n\n![](/img/2018-04-23-Datastructure-23.jpg)\n\n**7) code**\n\n```python\nclass Node:\n    def __init__(self,element,pointer):\n        self.element = element\n        self.pointer = pointer\n\nclass LinkedList:\n\n    def __init__(self):\n        self.head = None\n        self.tail = None\n        self.size = 0\n\n    def add_first(self,e):\n        newest = Node(e,None)\n        newest.pointer = self.head\n        self.head = newest\n        self.size = self.size+1\n\n        if self.size == 1:\n            self.tail = newest\n\n    def add_last(self,e):\n        newest = Node(e,None)\n        self.tail.pointer = newest\n        self.tail = newest\n        self.size = self.size+1\n\n    def remove_first(self):\n        if self.size == 0:\n            print('The linked list is empty')\n        else:\n            self.head = self.head.pointer\n            self.size = self.size - 1\n```\n**8) Practice**\n\n1. Implement stack with a singly linked list\n\n之前的stack是用python的list实现，现在用链表实现\n\n```python\nclass LinkedStack:\n   \n   def __init__(self):\n       self.head = None\n       self.size = 0\n\n   def __len__(self):\n       return self.size\n\n   def is_empty(self):\n       return self.size == 0\n\n   def push(self,e):\n       self.head = Node(e,self.head)\n       self.size+=1\n\n   def top(self):\n       if self.is_empty():\n           print('Stack is empty')\n       else:\n           return self.head.element\n\n   def pop(self):\n       if self.is_empty():\n           print('Stack is empty')\n       else:\n           answer = self.head.element\n           self.head = self.head.pointer\n           self.size-=1\n           return answer\n```\n\n2. Implement queue with a singly linked list\n\n```python\nclass LinkedQueue:\n\n   def __init__(self):\n       self.head = None\n       self.tail = None\n       self.size = 0\n\n   def __len__(self):\n       return self.size\n\n   def is_empty(self):\n       return self.size == 0\n\n   def first(self):\n       if self.is_empty():\n           print('Queue is empty')\n       else:\n           return self.head.element\n\n   def dequeue(self):\n       if self.is_empty():\n           print('Queue is empty')\n       else:\n           answer = self.head.element\n           self.head = self.head.pointer\n           self.size-=1\n           if self.is_empty():\n               self.tail = None\n           return answer\n\n   def enqueue(self,e):\n       newest = Node(e,None)\n\n       if self.is_empty():\n           self.head = newest\n       else:\n           self.tail.pointer = newest\n       self.tail = newest\n       self.size+=1\n```\n\n\n### 2.4 Circularly Linked List 循环链表\n\n- The tail of a linked list can use its next reference to point back to the head of the list\n- Such a structure is usually called a circularly linked list\n\n无需储存head，因为head=tail.pointer\n\n![](/img/2018-04-23-Datastructure-24.jpg)\n\n**1) Example: Round-robin scheduler 循环调度**\n\nmulti-tasking    \n>一个处理器在某一时刻只会给一件任务提供服务，把时间分成很小的无数份，在不同的很小的时间内循环执行不同的程序\n\n- A round-robin scheduler iterates through a collection of elements in a circular fashion and “serves” each element by performing a given action on it\n- Such a scheduler is used, for example, to fairly allocate a resource that must be shared by a collection of clients\n- For instance, round-robin scheduling is often used to allocate slices of CPU time to various applications running concurrently on a computer\n\n\n**2) Implementing round-robin scheduler using standard queue**\n\n• A round-robin scheduler could be implemented with the **standard queue**, by repeatedly performing the following steps on queue Q:\n\n1. e = Q.dequeue()\n2. Service element e\n3. Q.enqueue(e)\n\n![](/img/2018-04-23-Datastructure-25.jpg)\n\n\n**3) Implement a Queue with a Circularly Linked List**\n\n![](/img/2018-04-23-Datastructure-26.jpg)\n\n\n### 2.5 Doubly linked list 双链表\n\n- For a singly linked list, we can efficiently insert a node at either end of a singly linked list, and can delete a node at the head of a list\n- But we cannot efficiently delete a node at the tail of the list      \n\n>普通的链表删除尾部的 node 的时间复杂度为 o(n)，因为要把倒数第二个的指针指向空，而只有往后的指针，没有往前的指针，所以要从头开始才能找到倒数第二个 node。\n\n- We can define a linked list in which each node keeps an explicit\nreference to the node before it and a reference to the node after it\n\n从双向链表中的任意一个结点开始，都可以很方便地访问它的前驱结点和后继结点\n\n\n- In order to avoid some special cases when operating near the boundaries of a doubly linked list, it helps to add special nodes at\nboth ends of the list: **a header node at the beginning of the list, and a trailer node at the end of the list**\n- These **“dummy” nodes** are known as sentinels (or guards), and they **do not store elements of the primary sequence**\n\n![](/img/2018-04-23-Datastructure-27.jpg)\n\n**Code**\n\n![](/img/2018-04-23-Datastructure-28.jpg)\n\n**1) Inserting in the middle of a doubly linked list**\n\n![](/img/2018-04-23-Datastructure-29.jpg)\n\n**2) Deleting from the doubly linked list**\n\n![](/img/2018-04-23-Datastructure-30.jpg)\n\n\n### 2.6 Tree 树\n\n- A tree is a data structure that stores elements **hierarchically（分层的）**\n- With the exception of the top element, each element in a tree has a **parent element** and zero or more **children elements**\n- We typically call the top element **the root of the tree（根节点）**, but it is drawn as the highest element\n\n![](/img/2018-04-23-Datastructure-31.jpg)\n\n**1) Edge 边 and path 路径**\n\n- An **edge 边** of tree T is a pair of nodes (u,v) such that u is the parent of v, or vice versa\n- A **path 路径** of T is a sequence of nodes such that any two consecutive nodes in the sequence form an edge\n- The **depth 深度** of a node v is the length of the path connecting root node and v（不包括自己）\n- **leaf node 叶节点**\n- **internal node 内部节点**\n\n**2) Ordered tree**\n\nA tree is **ordered** if there is a **meaningful linear order among the children of each node**（树中每个结点的各子树看成是从左到右有次序的）; such an order is usually visualized by arranging siblings **from left to right, according to their order** （之前公司的结构就是 Unordered Tree）\n\n![](/img/2018-04-23-Datastructure-32.jpg)\n\n\n**3) Binary tree**\n\nA binary tree is **an ordered tree** with the following properties:\n\n1. Every node has **at most two children**\n2. Each child node is labeled as being either a **left child** or a **right child** \n3. A **left child precedes a right child** in the order of children of a node\n\nThe subtree rooted at a left or right child of an internal node v is called a left subtree or right subtree, respectively, of v\n\nA binary tree is **proper** if each node **has either zero or two children**（要不没有，要不2个）. Some people also refer to such trees as being **full binary trees**\n\n**4) Example: Represent an expression with binary tree**\n\nAn arithmetic expression（算术表达式）can be represented by a binary tree whose leaves are associated with variables or constants, and whose internal nodes are associated with one of the operators +, −, ×, and /\n\n![](/img/2018-04-23-Datastructure-33.jpg)\n\n**5) Binary tree class**\n\n- We define a tree class based on a class called Node; an element is stored as a node\n- Each node contains **3 references**（像是四个格子，第一个是数据）\n\n\t1. one pointing to the parent node, \n\t2. two pointing to the child nodes\n\n**6) Implementing the binary tree**\n\n![](/img/2018-04-23-Datastructure-34.jpg)\n\n ![](/img/2018-04-23-Datastructure-108.jpg)\n\n**7) Depth first search (DFS) 深度优先搜索算法**\n\n ![](/img/2018-04-23-Datastructure-109.jpg)\n \n  ![](/img/2018-04-23-Datastructure-110.jpg)\n  \n**8) Breadth first search 广度优先搜索**\n\n\nStarts at the root and we visit all the positions at depth d before we visit the positions at depth d +1\n\n ![](/img/2018-04-23-Datastructure-111.jpg)\n \n**Example: finding the best strategy in a game**\n\n![](/img/2018-04-23-Datastructure-35.jpg)\n\n**Code**\n\n ![](/img/2018-04-23-Datastructure-112.jpg)","tags":["算法"],"categories":["算法"]},{"title":"YOLOv2 目标检测（六）","url":"/2018/04/22/YOLOv2/","content":"\n\n\n## 1 回顾YOLOv1\n\n![](/img/2018-04-22-YOLOv2-1.jpg)\n\n1. 给个一个输入图像，首先将图像划分成 \\\\(7\\times 7\\\\) 的网格。\n\n2. 对于每个网格，每个网格预测2个bouding box（每个box包含5个预测量）以及20个类别概率，总共输出 \\\\(7\\times 7\\times （2\\times 5+20）=1470\\\\) 个tensor\n\n3. 根据上一步可以预测出 \\\\(7 \\times  7 \\times  2 = 98\\\\) 个目标窗口，然后根据阈值去除可能性比较低的目标窗口，再由NMS去除冗余窗口即可。\n\nYOLOv1使用了end-to-end的回归方法，没有region proposal步骤，直接回归便完成了位置和类别的判定。种种原因使得YOLOv1在目标定位上不那么精准，直接导致YOLO的检测精度并不是很高。\n\n\n<!-- more -->\n\n## 2 YOLOv2 的改进\n\n![](/img/2018-04-22-YOLOv2-2.jpg)\n\n### 2.1 Batch Normalization\n\nCNN在训练过程中网络每层输入的分布一直在改变, 会使训练过程难度加大，但可以通过normalize每层的输入解决这个问题。新的YOLO网络在每一个卷积层后添加batch normalization，通过这一方法，mAP获得了2%的提升。batch normalization 也有助于规范化模型，可以在舍弃dropout优化后依然不会过拟合。\n\n\n### 2.2 High Resolution (高分辨率) Classifier\n\n\n目前的目标检测方法中，基本上都会使用ImageNet预训练过的模型（classifier）来提取特征，如果用的是AlexNet网络，那么输入图片会被resize到不足256 * 256，导致分辨率不够高，给检测带来困难。为此，新的YOLO网络把分辨率直接提升到了448 * 448，这也意味之原有的网络模型必须进行某种调整以适应新的分辨率输入。\n\n对于YOLOv2，作者首先对分类网络（自定义的darknet）进行了fine tune，分辨率改成448 * 448，在ImageNet数据集上训练10轮（10 epochs），训练后的网络就可以适应高分辨率的输入了。然后，作者对检测网络部分（也就是后半部分）也进行fine tune。这样通过提升输入的分辨率，mAP获得了4%的提升。\n\n\n\n### 2.3 Convolutional With Anchor Boxes\n\n之前的YOLO利用全连接层的数据完成边框的预测，导致丢失较多的空间信息，定位不准。作者在这一版本中借鉴了Faster R-CNN中的anchor思想，回顾一下，anchor是RNP网络中的一个关键步骤，说的是在卷积特征图上进行滑窗操作，每一个中心可以预测9种不同大小的建议框。看到YOLOv2的这一借鉴，我只能说SSD的作者是有先见之明的。\n\n![](/img/2018-04-22-YOLOv2-3.jpg)\n\n为了引入anchor boxes来预测bounding boxes，作者在网络中果断去掉了全连接层。剩下的具体怎么操作呢？首先，作者去掉了后面的一个池化层以确保输出的卷积特征图有更高的分辨率。然后，通过缩减网络，让图片输入分辨率为 \\\\(416 \\times  416\\\\) ，这一步的目的是为了让后面产生的卷积特征图宽高都为奇数，这样就可以产生一个center cell。作者观察到，大物体通常占据了图像的中间位置， 就可以只用中心的一个cell来预测这些物体的位置，否则就要用中间的4个cell来进行预测，这个技巧可稍稍提升效率。最后，YOLOv2使用了卷积层降采样（factor为32），使得输入卷积网络的 \\\\(416 \\times  416\\\\) 图片最终得到13 * 13的卷积特征图（416/32=13）。\n\n加入了anchor boxes后，可以预料到的结果是召回率上升，准确率下降。我们来计算一下，假设每个cell预测9个建议框，那么总共会预测 \\\\(13 \\* 13 \\* 9 = 1521\\\\) 个boxes，而之前的网络仅仅预测 \\\\(7 \\* 7 \\* 2 = 98\\\\) 个boxes。具体数据为：没有anchor boxes，模型recall为81%，mAP为69.5%；加入anchor boxes，模型recall为88%，mAP为69.2%。这样看来，准确率只有小幅度的下降，而召回率则提升了7%，说明可以通过进一步的工作来加强准确率，的确有改进空间。\n\n\n![](/img/2018-04-22-YOLOv2-4.jpg)\n\n### 2.4 Dimension Clusters（维度聚类）\n\n在Faster R-CNN和SSD中，先验框的维度（长和宽）都是手动设定的，带有一定的主观性。如果选取的先验框维度比较合适，那么模型更容易学习，从而做出更好的预测。因此，YOLOv2采用k-means聚类方法对训练集中的边界框做了聚类分析。 \n\nYOLOv2中利用K-means聚类方法，通过对数据集中的ground truth box做聚类，找到ground truth box的统计规律，从而可以自动找到更好的boxes宽高维度。以聚类个数k为anchor boxes个数，以k个聚类中心box的宽高维度为anchor box的维度。传统的K-means聚类方法使用的是欧氏距离函数，也就意味着较大的boxes会比较小的boxes产生更多的error，聚类结果可能会偏离。为此，作者采用的评判标准是IOU得分（也就是boxes之间的交集除以并集），这样的话，error就和box的尺度无关了，聚类的距离度量采用：\n\n$$d(box, centroid) = 1 - IOU(box, centroid)$$\n\n![](/img/2018-04-22-YOLOv2-5.jpg)\n\n  可以看到，平衡复杂度和IOU之后，最终得到k值为5，意味着作者选择了5种大小的box维度来进行定位预测，这与手动精选的box维度不同。结果中扁长的框较少，而瘦高的框更多（这符合行人的特征），这种结论如不通过聚类实验恐怕是发现不了的。\n\n\n### 2.5 Direct Location prediction\n\n前面讲到，YOLOv2借鉴RPN网络使用anchor boxes来预测边界框相对先验框的offsets。边界框的实际中心位置  \\\\((x,y)\\\\)  ，需要根据预测的坐标偏移值  \\\\((t\\_x, t\\_y)\\\\)  ，先验框的尺度  \\\\((w\\_a, h\\_a)\\\\)  以及中心坐标  \\\\((x\\_a, y\\_a)\\\\)  （特征图每个位置的中心点）来计算：\n\n$$x = (t_x\\times w_a)-x_a$$ \n\n$$y=(t_y\\times h_a) - y_a$$\n\n但是上面的公式是无约束的，预测的边界框很容易向任何方向偏移，如当  \\\\(t\\_x=1\\\\)  时边界框将向右偏移先验框的一个宽度大小，而当  \\\\(t\\_x=-1\\\\)  时边界框将向左偏移先验框的一个宽度大小，因此每个位置预测的边界框可以落在图片任何位置，这导致模型的不稳定性，在训练时需要很长时间来预测出正确的offsets。所以，YOLOv2 弃用了这种预测方式，而是沿用YOLOv1的方法，就是预测边界框中心点相对于对应cell左上角位置的相对偏移值，为了将边界框中心点约束在当前cell中，使用sigmoid函数处理偏移值，这样预测的偏移值在 \\\\((0,1)\\\\) 范围内（每个cell的尺度看做1）。总结来看，根据边界框预测的4个offsets  \\\\(t\\_x, t\\_y, t\\_w, t\\_h \\\\) ，可以按如下公式计算出边界框实际位置和大小：\n\n$$b\\_x = \\sigma (t\\_x)+c\\_x $$\n\n$$b\\_y = \\sigma (t\\_y) + c\\_y $$\n\n$$b\\_w = p\\_we^{t\\_w} $$\n\n$$b\\_h = p\\_he^{t\\_h}$$ \n\n其中  \\\\((c\\_x, x\\_y)\\\\)  为cell的左上角坐标，如图5所示，在计算时每个cell的尺度为1，所以当前cell的左上角坐标为 \\\\( (1,1)\\\\)  。由于sigmoid函数的处理，边界框的中心位置会约束在当前cell内部，防止偏移过多。而  \\\\(p\\_w\\\\)  和  \\\\(p\\_h\\\\)  是先验框的宽度与长度，前面说过它们的值也是相对于特征图大小的，在特征图中每个cell的长和宽均为1。这里记特征图的大小为  \\\\((W, H)\\\\)  （在文中是 ( \\\\(13, 13)\\\\)  )，这样我们可以将边界框相对于整张图片的位置和大小计算出来（4个值均在0和1之间）：\n\n$$b\\_x = (\\sigma (t\\_x)+c\\_x)/W $$\n\n$$ b\\_y = (\\sigma (t\\_y) + c\\_y)/H $$\n\n$$b\\_w = p\\_we^{t_w}/W $$\n\n$$ b\\_h = p\\_he^{t_h}/H $$\n\n如果再将上面的4个值分别乘以图片的宽度和长度（像素点值）就可以得到边界框的最终位置和大小了。这就是YOLOv2边界框的整个解码过程。约束了边界框的位置预测值使得模型更容易稳定训练，结合聚类分析得到先验框与这种预测方法，YOLOv2的mAP值提升了约5%。\n\n![](/img/2018-04-22-YOLOv2-6.jpg)\n\n### 2.6 New Network: Darknet-19\n\nYOLOv2采用了一个新的基础模型（特征提取器），称为Darknet-19，包括19个卷积层和5个maxpooling层，如图4所示。Darknet-19与VGG16模型设计原则是一致的，主要采用  \\\\(3\\times3\\\\)  卷积，采用  \\\\(2\\times2\\\\)  的maxpooling层之后，特征图维度降低2倍，而同时将特征图的channles增加两倍。与NIN(Network in Network)类似，Darknet-19最终采用global avgpooling做预测，并且在 \\\\(3\\times3\\\\)  卷积之间使用  \\\\(1\\times1\\\\)  卷积来压缩特征图channles以降低模型计算量和参数。Darknet-19每个卷积层后面同样使用了batch norm层以加快收敛速度，降低模型过拟合。在ImageNet分类数据集上，Darknet-19的top-1准确度为72.9%，top-5准确度为91.2%，但是模型参数相对小一些。使用Darknet-19之后，YOLOv2的mAP值没有显著提升，但是计算量却可以减少约33%。\n\n![](/img/2018-04-22-YOLOv2-7.jpg)\n\n### 2.7 Fine-Grained Features\n\nYOLOv2的输入图片大小为 \\\\(416\\times416\\\\)，经过5次maxpooling之后得到 \\\\(13\\times13\\\\) 大小的特征图，并以此特征图采用卷积做预测。 \\\\(13\\times13\\\\) 大小的特征图对检测大物体是足够了，但是对于小物体还需要更精细的特征图（Fine-Grained Features）。因此SSD使用了多尺度的特征图来分别检测不同大小的物体，前面更精细的特征图可以用来预测小物体。YOLOv2提出了一种passthrough层来利用更精细的特征图。YOLOv2所利用的Fine-Grained Features是 \\\\(26\\times26\\\\) 大小的特征图（最后一个maxpooling层的输入），对于Darknet-19模型来说就是大小为 \\\\(26\\times26\\times512\\\\) 的特征图。passthrough层与ResNet网络的shortcut类似，以前面更高分辨率的特征图为输入，然后将其连接到后面的低分辨率特征图上。前面的特征图维度是后面的特征图的2倍，passthrough层抽取前面层的每个 \\\\(2\\times2\\\\) 的局部区域，然后将其转化为channel维度，对于 \\\\(26\\times26\\times512\\\\) 的特征图，经passthrough层处理之后就变成了 \\\\(13\\times13\\times2048\\\\)的新特征图（特征图大小降低4倍，而channles增加4倍，图6为一个实例），这样就可以与后面的 \\\\(13\\times13\\times1024\\\\) 特征图连接在一起形成 \\\\(13\\times13\\times3072\\\\) 大小的特征图，然后在此特征图基础上卷积做预测。在YOLO的C源码中，passthrough层称为reorg layer。在TensorFlow中，可以使用tf.extract_image_patches或者tf.space_to_depth来实现passthrough层：\n\n```python\nout = tf.extract_image_patches(in, [1, stride, stride, 1], [1, stride, stride, 1], [1,1,1,1], padding=\"VALID\")\n// or use tf.space_to_depth\nout = tf.space_to_depth(in, 2)\n```\n\n![](/img/2018-04-22-YOLOv2-8.jpg)\n\n另外，作者在后期的实现中借鉴了ResNet网络，不是直接对高分辨特征图处理，而是增加了一个中间卷积层，先采用64个 \\\\(1\\times1\\\\) 卷积核进行卷积，然后再进行passthrough处理，这样 \\\\(26\\times26\\times512\\\\) 的特征图得到 \\\\(13\\times13\\times256\\\\) 的特征图。这算是实现上的一个小细节。使用Fine-Grained Features之后YOLOv2的性能有1%的提升。\n\n\n### 2.8 Multi-Scale Training\n\n由于YOLOv2模型中只有卷积层和池化层，所以YOLOv2的输入可以不限于  \\\\(416\\times416\\\\)  大小的图片。为了增强模型的鲁棒性，YOLOv2采用了多尺度输入训练策略，具体来说就是在训练过程中每间隔一定的iterations之后改变模型的输入图片大小。由于YOLOv2的下采样总步长为32，输入图片大小选择一系列为32倍数的值：  \\\\(\\\\{320, 352,..., 608\\\\}\\\\)  ，输入图片最小为  \\\\(320\\times320\\\\)  ，此时对应的特征图大小为  \\\\(10\\times10\\\\)  （不是奇数了，确实有点尴尬），而输入图片最大为  \\\\(608\\times608\\\\)  ，对应的特征图大小为  \\\\(19\\times19\\\\)  。在训练过程，每隔10个iterations随机选择一种输入图片大小，然后只需要修改对最后检测层的处理就可以重新训练。\n\n![](/img/2018-04-22-YOLOv2-9.jpg)\n\n采用Multi-Scale Training策略，YOLOv2可以适应不同大小的图片，并且预测出很好的结果。在测试时，YOLOv2可以采用不同大小的图片作为输入，在VOC 2007数据集上的效果如下图所示。可以看到采用较小分辨率时，YOLOv2的mAP值略低，但是速度更快，而采用高分辨输入时，mAP值更高，但是速度略有下降，对于  \\\\(544\\times544\\\\)  ，mAP高达78.6%。注意，这只是测试时输入图片大小不同，而实际上用的是同一个模型（采用Multi-Scale Training训练）。\n\n\n\n### 2.9 总结\n![](/img/2018-04-22-YOLOv2-10.jpg)\n\n\n总结来看，虽然YOLOv2做了很多改进，但是大部分都是借鉴其它论文的一些技巧，如Faster R-CNN的anchor boxes，YOLOv2采用anchor boxes和卷积做预测，这基本上与SSD模型（单尺度特征图的SSD）非常类似了，而且SSD也是借鉴了Faster R-CNN的RPN网络。从某种意义上来说，YOLOv2和SSD这两个one-stage模型与RPN网络本质上无异，只不过RPN不做类别的预测，只是简单地区分物体与背景。在two-stage方法中，RPN起到的作用是给出region proposals，其实就是作出粗糙的检测，所以另外增加了一个stage，即采用R-CNN网络来进一步提升检测的准确度（包括给出类别预测）。而对于one-stage方法，它们想要一步到位，直接采用“RPN”网络作出精确的预测，要因此要在网络设计上做很多的tricks。YOLOv2的一大创新是采用Multi-Scale Training策略，这样同一个模型其实就可以适应多种大小的图片了。\n\n\n## 3 YOLOv2 的训练\n### 3.1 YOLOv2 的训练\nYOLOv2的训练主要包括三个阶段。第一阶段就是先在ImageNet分类数据集上预训练Darknet-19，此时模型输入为  \\\\(224\\times224\\\\)  ，共训练160个epochs。然后第二阶段将网络的输入调整为  \\\\(448\\times448\\\\)  ，继续在ImageNet数据集上finetune分类模型，训练10个epochs，此时分类模型的top-1准确度为76.5%，而top-5准确度为93.3%。第三个阶段就是修改Darknet-19分类模型为检测模型，并在检测数据集上继续finetune网络。网络修改包括（网路结构可视化）：移除最后一个卷积层、global avgpooling层以及softmax层，并且新增了三个  \\\\(3\\times3\\times2014\\\\) 卷积层，同时增加了一个passthrough层，最后使用  \\\\(1\\times1\\\\)  卷积层输出预测结果，输出的channels数为：  \\\\(\\text{num\\_anchors}\\times(5+\\text{num\\_classes})\\\\)  ，和训练采用的数据集有关系。由于anchors数为5，对于VOC数据集输出的channels数就是125，而对于COCO数据集则为425。这里以VOC数据集为例，最终的预测矩阵为  \\\\(T\\\\)（shape为  \\\\((\\text{batch_size}, 13, 13, 125)\\\\)  ），可以先将其reshape为  \\\\((\\text{batch\\_size}, 13, 13, 5, 25)\\\\)  ，其中  \\\\(T[:,\\ :,\\ :,\\ :,\\ 0:4]\\\\)  为边界框的位置和大小  \\\\((t\\_x, t\\_y, t\\_w, t\\_h)\\\\)  ，  \\\\(T[:,\\ :,\\ :,\\ :,\\ 4]\\\\)  为边界框的置信度，而  \\\\(T[:,\\ :,\\ :,\\ :,\\ 5:]\\\\)  为类别预测值。\n\n![](/img/2018-04-22-YOLOv2-11.jpg)\n\n<small><center>YOLOv2训练的三个阶段</center></small>\n\n![](/img/2018-04-22-YOLOv2-12.jpg)\n\n<small><center>YOLOv2结构示意图</center></small>\n\nYOLOv2的网络结构以及训练参数我们都知道了，但是貌似少了点东西。仔细一想，原来作者并没有给出YOLOv2的训练过程的两个最重要方面，即先验框匹配（样本选择）以及训练的损失函数，难怪Ng说YOLO论文很难懂，没有这两方面的说明我们确实不知道YOLOv2到底是怎么训练起来的。不过默认按照YOLOv1的处理方式也是可以处理，我看了YOLO在TensorFlow上的实现[darkflow](https://link.zhihu.com/?target=https%3A//github.com/thtrieu/darkflow)（见_[yolov2/train.py](https://link.zhihu.com/?target=https%3A//github.com/thtrieu/darkflow/blob/master/darkflow/net/yolov2/train.py)_），发现它就是如此处理的：和YOLOv1一样，对于训练图片中的ground truth，若其中心点落在某个cell内，那么该cell内的5个先验框所对应的边界框负责预测它，具体是哪个边界框预测它，需要在训练中确定，即由那个与ground truth的IOU最大的边界框预测它，而剩余的4个边界框不与该ground truth匹配。YOLOv2同样需要假定每个cell至多含有一个grounth truth，而在实际上基本不会出现多于1个的情况。与ground truth匹配的先验框计算坐标误差、置信度误差（此时target为1）以及分类误差，而其它的边界框只计算置信度误差（此时target为0）。YOLOv2和YOLOv1的损失函数一样，为均方差函数。\n\n但是我看了YOLOv2的源码（训练样本处理与loss计算都包含在文件[region_layer.c](https://link.zhihu.com/?target=https%3A//github.com/pjreddie/darknet/blob/master/src/region_layer.c)中，YOLO源码没有任何注释，反正我看了是直摇头），并且参考国外的[blog](https://link.zhihu.com/?target=https%3A//towardsdatascience.com/training-object-detection-yolov2-from-scratch-using-cyclic-learning-rates-b3364f7e4755)以及[allanzelener/YAD2K](https://link.zhihu.com/?target=https%3A//github.com/allanzelener/YAD2K)（Ng深度学习教程所参考的那个Keras实现）上的实现，发现YOLOv2的处理比原来的v1版本更加复杂。先给出loss计算公式：\n\n![](/img/2018-04-22-YOLOv2-13.jpg)\n\n\n我们来一点点解释，首先  \\\\(W, H\\\\)  分别指的是特征图 \\\\(（ 13\\times13 ）\\\\) 的宽与高，而  \\\\(A\\\\)  指的是先验框数目（这里是5），各个  \\\\(\\lambda\\\\)  值是各个loss部分的权重系数。第一项loss是计算background的置信度误差，但是哪些预测框来预测背景呢，需要先计算各个预测框和所有ground truth的IOU值，并且取最大值Max_IOU，如果该值小于一定的阈值（YOLOv2使用的是0.6），那么这个预测框就标记为background，需要计算noobj的置信度误差。第二项是计算先验框与预测宽的坐标误差，但是只在前12800个iterations间计算，我觉得这项应该是在训练前期使预测框快速学习到先验框的形状。第三大项计算与某个ground truth匹配的预测框各部分loss值，包括坐标误差、置信度误差以及分类误差。先说一下匹配原则，对于某个ground truth，首先要确定其中心点要落在哪个cell上，然后计算这个cell的5个先验框与ground truth的IOU值（YOLOv2中bias_match=1），计算IOU值时不考虑坐标，只考虑形状，所以先将先验框与ground truth的中心点都偏移到同一位置（原点），然后计算出对应的IOU值，IOU值最大的那个先验框与ground truth匹配，对应的预测框用来预测这个ground truth。在计算obj置信度时，在YOLOv1中target=1，而YOLOv2增加了一个控制参数rescore，当其为1时，target取预测框与ground truth的真实IOU值。对于那些没有与ground truth匹配的先验框（与预测框对应），除去那些Max_IOU低于阈值的，其它的就全部忽略，不计算任何误差。这点在YOLOv3论文中也有相关说明：YOLO中一个ground truth只会与一个先验框匹配（IOU值最好的），对于那些IOU值超过一定阈值的先验框，其预测结果就忽略了。这和SSD与RPN网络的处理方式有很大不同，因为它们可以将一个ground truth分配给多个先验框。尽管YOLOv2和YOLOv1计算loss处理上有不同，但都是采用均方差来计算loss。另外需要注意的一点是，在计算boxes的  \\\\(w\\\\)  和  \\\\(h\\\\)  误差时，YOLOv1中采用的是平方根以降低boxes的大小对误差的影响，而YOLOv2是直接计算，但是根据ground truth的大小对权重系数进行修正：`l.coord_scale * (2 - truth.w*truth.h)`（这里 \\\\(w\\\\) 和 \\\\(h\\\\) 都归一化到 \\\\((0,1)\\\\) )，这样对于尺度较小的boxes其权重系数会更大一些，可以放大误差，起到和YOLOv1计算平方根相似的效果。\n\n最终的YOLOv2模型在速度上比YOLOv1还快（采用了计算量更少的Darknet-19模型），而且模型的准确度比YOLOv1有显著提升，详情见paper。\n\n\n\n### 3.2 YOLOv2在TensorFlow上实现\n\n这里参考YOLOv2在Keras上的复现（见[yhcc/yolo2](https://link.zhihu.com/?target=https%3A//github.com/yhcc/yolo2)）,使用TensorFlow实现YOLOv2在COCO数据集上的test过程。\n\n**首先是定义YOLOv2的主体网络结构Darknet-19：**\n\n```python\ndef darknet(images, n_last_channels=425):\n    \"\"\"Darknet19 for YOLOv2\"\"\"\n    net = conv2d(images, 32, 3, 1, name=\"conv1\")\n    net = maxpool(net, name=\"pool1\")\n    net = conv2d(net, 64, 3, 1, name=\"conv2\")\n    net = maxpool(net, name=\"pool2\")\n    net = conv2d(net, 128, 3, 1, name=\"conv3_1\")\n    net = conv2d(net, 64, 1, name=\"conv3_2\")\n    net = conv2d(net, 128, 3, 1, name=\"conv3_3\")\n    net = maxpool(net, name=\"pool3\")\n    net = conv2d(net, 256, 3, 1, name=\"conv4_1\")\n    net = conv2d(net, 128, 1, name=\"conv4_2\")\n    net = conv2d(net, 256, 3, 1, name=\"conv4_3\")\n    net = maxpool(net, name=\"pool4\")\n    net = conv2d(net, 512, 3, 1, name=\"conv5_1\")\n    net = conv2d(net, 256, 1, name=\"conv5_2\")\n    net = conv2d(net, 512, 3, 1, name=\"conv5_3\")\n    net = conv2d(net, 256, 1, name=\"conv5_4\")\n    net = conv2d(net, 512, 3, 1, name=\"conv5_5\")\n    shortcut = net\n    net = maxpool(net, name=\"pool5\")\n    net = conv2d(net, 1024, 3, 1, name=\"conv6_1\")\n    net = conv2d(net, 512, 1, name=\"conv6_2\")\n    net = conv2d(net, 1024, 3, 1, name=\"conv6_3\")\n    net = conv2d(net, 512, 1, name=\"conv6_4\")\n    net = conv2d(net, 1024, 3, 1, name=\"conv6_5\")\n    # ---------\n    net = conv2d(net, 1024, 3, 1, name=\"conv7_1\")\n    net = conv2d(net, 1024, 3, 1, name=\"conv7_2\")\n    # shortcut\n    shortcut = conv2d(shortcut, 64, 1, name=\"conv_shortcut\")\n    shortcut = reorg(shortcut, 2)\n    net = tf.concat([shortcut, net], axis=-1)\n    net = conv2d(net, 1024, 3, 1, name=\"conv8\")\n    # detection layer\n    net = conv2d(net, n_last_channels, 1, batch_normalize=0,\n                 activation=None, use_bias=True, name=\"conv_dec\")\n    return net\n```\n\n**然后实现对Darknet-19模型输出的解码：**    \n\n```python\ndef decode(detection_feat, feat_sizes=(13, 13), num_classes=80,\n           anchors=None):\n    \"\"\"decode from the detection feature\"\"\"\n    H, W = feat_sizes\n    num_anchors = len(anchors)\n    detetion_results = tf.reshape(detection_feat, [-1, H * W, num_anchors,\n                                        num_classes + 5])\n\n    bbox_xy = tf.nn.sigmoid(detetion_results[:, :, :, 0:2])\n    bbox_wh = tf.exp(detetion_results[:, :, :, 2:4])\n    obj_probs = tf.nn.sigmoid(detetion_results[:, :, :, 4])\n    class_probs = tf.nn.softmax(detetion_results[:, :, :, 5:])\n\n    anchors = tf.constant(anchors, dtype=tf.float32)\n\n    height_ind = tf.range(H, dtype=tf.float32)\n    width_ind = tf.range(W, dtype=tf.float32)\n    x_offset, y_offset = tf.meshgrid(height_ind, width_ind)\n    x_offset = tf.reshape(x_offset, [1, -1, 1])\n    y_offset = tf.reshape(y_offset, [1, -1, 1])\n\n    # decode\n    bbox_x = (bbox_xy[:, :, :, 0] + x_offset) / W\n    bbox_y = (bbox_xy[:, :, :, 1] + y_offset) / H\n    bbox_w = bbox_wh[:, :, :, 0] * anchors[:, 0] / W * 0.5\n    bbox_h = bbox_wh[:, :, :, 1] * anchors[:, 1] / H * 0.5\n\n    bboxes = tf.stack([bbox_x - bbox_w, bbox_y - bbox_h,\n                       bbox_x + bbox_w, bbox_y + bbox_h], axis=3)\n\n    return bboxes, obj_probs, class_probs\n```    \n\n我将YOLOv2的官方训练权重文件转换了TensorFlow的checkpoint文件（[下载链接](https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1ZeT5HerjQxyUZ_L9d3X52w)），具体的测试demo都放在我的[GitHub](https://link.zhihu.com/?target=https%3A//github.com/xiaohu2015/DeepLearning_tutorials/tree/master/ObjectDetections/yolo2)上了，感兴趣的可以去下载测试一下，至于train的实现就自己折腾吧，相对会棘手点。\n\n![](/img/2018-04-22-YOLOv2-14.jpg)\n\n## 4 YOLO9000\n\nYOLO9000是在YOLOv2的基础上提出的一种可以检测超过9000个类别的模型，其主要贡献点在于提出了一种分类和检测的联合训练策略。众多周知，检测数据集的标注要比分类数据集打标签繁琐的多，所以ImageNet分类数据集比VOC等检测数据集高出几个数量级。在YOLO中，边界框的预测其实并不依赖于物体的标签，所以YOLO可以实现在分类和检测数据集上的联合训练。对于检测数据集，可以用来学习预测物体的边界框、置信度以及为物体分类，而对于分类数据集可以仅用来学习分类，但是其可以大大扩充模型所能检测的物体种类。\n\n作者选择在COCO和ImageNet数据集上进行联合训练，但是遇到的第一问题是两者的类别并不是完全互斥的，比如\"Norfolk terrier\"明显属于\"dog\"，所以作者提出了一种层级分类方法（Hierarchical classification），主要思路是根据各个类别之间的从属关系（根据WordNet）建立一种树结构WordTree，结合COCO和ImageNet建立的WordTree如下图所示：\n\n![](/img/2018-04-22-YOLOv2-15.jpg)\n\n![](/img/2018-04-22-YOLOv2-16.jpg)\n\nWordTree中的根节点为\"physical object\"，每个节点的子节点都属于同一子类，可以对它们进行softmax处理。在给出某个类别的预测概率时，需要找到其所在的位置，遍历这个path，然后计算path上各个节点的概率之积。\n\n![](/img/2018-04-22-YOLOv2-17.jpg)\n\n<small><center>ImageNet与WordTree预测的对比</center></small>\n\n\n在训练时，如果是检测样本，按照YOLOv2的loss计算误差，而对于分类样本，只计算分类误差。在预测时，YOLOv2给出的置信度就是 Pr(physical \\space object) ，同时会给出边界框位置以及一个树状概率图。在这个概率图中找到概率最高的路径，当达到某一个阈值时停止，就用当前节点表示预测的类别。\n\n通过联合训练策略，YOLO9000可以快速检测出超过9000个类别的物体，总体mAP值为19,7%。我觉得这是作者在这篇论文作出的最大的贡献，因为YOLOv2的改进策略亮点并不是很突出，但是YOLO9000算是开创之举。\n\n## 5 YOLOv3\n\n\n近期，YOLOv3发布了，但是正如作者所说，这仅仅是他们近一年的一个工作报告（TECH REPORT），不算是一个完整的paper，因为他们实际上是把其它论文的一些工作在YOLO上尝试了一下。相比YOLOv2，我觉得YOLOv3最大的变化包括两点：使用残差模型和采用FPN架构。YOLOv3的特征提取器是一个残差模型，因为包含53个卷积层，所以称为Darknet-53，从网络结构上看，相比Darknet-19网络使用了残差单元，所以可以构建得更深。另外一个点是采用FPN架构（Feature Pyramid Networks for Object Detection）来实现多尺度检测。YOLOv3采用了3个尺度的特征图（当输入为  \\\\(416\\times416\\\\)  时）： \\\\( (13\\times13)\\\\)  ，  \\\\((26\\times26)\\\\)  ，  \\\\((52\\times52)\\\\)  ，VOC数据集上的YOLOv3网络结构如图15所示，其中红色部分为各个尺度特征图的检测结果。YOLOv3每个位置使用3个先验框，所以使用k-means得到9个先验框，并将其划分到3个尺度特征图上，尺度更大的特征图使用更小的先验框，和SSD类似。\n\n![](/img/2018-04-22-YOLOv2-18.jpg)\n\n<small><center>YOLOv3所用的Darknet-53模型</center></small>\n\n![](/img/2018-04-22-YOLOv2-19.jpg)\n\n<small><center>YOLOv3网络结构示意图（VOC数据集）</center></small>\n\nYOLOv3与其它检测模型的对比如下图所示，可以看到在速度上YOLOv3完胜其它方法，虽然AP值并不是最好的（如果比较AP-0.5，YOLOv3优势更明显）。\n\n![](/img/2018-04-22-YOLOv2-20.jpg)\n\n<small><center>YOLOv3在COCO测试集与其它检测算法对比图</center></small>\n\n## 6 小结\n\n从YOLO的三代变革中可以看到，在目标检测领域比较好的策略包含：设置先验框，采用全卷积做预测，采用残差网络，采用多尺度特征图做预测。期待未来有更好的策略出现。本人水平有限，文中难免出现错误，欢迎指正！\n\n\n## 转载与参考\n\n1. [目标检测|YOLOv2原理与实现(附YOLOv3)](https://zhuanlan.zhihu.com/p/35325884)\n\n\n\n\n\n\n\n\n\n\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>","tags":["目标检测"],"categories":["深度学习"]},{"title":"YOLO 目标检测（五）","url":"/2018/04/21/YOLOv1/","content":"\n\n\n## 1 前言\n\n近几年来，目标检测算法取得了很大的突破。比较流行的算法可以分为两类，一类是基于Region Proposal的R-CNN系算法（R-CNN，Fast R-CNN, Faster R-CNN），它们是two-stage的，需要先使用启发式方法（selective search）或者CNN网络（RPN）产生Region Proposal，然后再在Region Proposal上做分类与回归。而另一类是Yolo，SSD这类one-stage算法，其仅仅使用一个CNN网络直接预测不同目标的类别与位置。第一类方法是准确度高一些，但是速度慢，但是第二类算法是速度快，但是准确性要低一些。这可以在图2中看到。本文介绍的是Yolo算法，其全称是You Only Look Once: Unified, Real-Time Object Detection，其实个人觉得这个题目取得非常好，基本上把Yolo算法的特点概括全了：You Only Look Once说的是只需要一次CNN运算，Unified指的是这是一个统一的框架，提供end-to-end的预测，而Real-Time体现是Yolo算法速度快。这里我们谈的是Yolo-v1版本算法，其性能是差于后来的SSD算法的，但是Yolo后来也继续进行改进，产生了Yolo9000算法。本文主要讲述Yolo-v1算法的原理，特别是算法的训练与预测中详细细节，最后将给出如何使用TensorFlow实现Yolo算法。\n\n<!-- more -->\n\n## 2 YOLOv1\n\n### 2.1 滑动窗口与CNN\n\n采用滑动窗口的目标检测算法思路非常简单，它将检测问题转化为了图像分类问题。其基本原理就是采用**不同大小和比例**（宽高比）的窗口在整张图片上以一定的步长进行滑动，然后对这些**窗口对应的区域做图像分类**，这样就可以实现对整张图片的检测了。\n\n如下图所示，如DPM就是采用这种思路。但是这个方法有致命的**缺点**，就是你并**不知道要检测的目标大小是什么规模**，所以你要**设置不同大小和比例**的窗口去滑动，而且还要选取**合适的步长**。但是这样会产生很多的子区域，并且都要经过分类器去做预测，这需要很大的计算量，所以你的分类器不能太复杂，因为要保证速度。解决思路之一就是减少要分类的子区域，这就是R-CNN的一个改进策略，其采用了selective search方法来找到最有可能包含目标的子区域（Region Proposal），其实可以看成采用启发式方法过滤掉很多子区域，这会提升效率。\n\n![](/img/2018-04-21-YOLOv1-1.jpg)\n\n\n如果你使用的是CNN分类器，那么滑动窗口是非常耗时的。但是结合卷积运算的特点，我们可以使用CNN实现更高效的滑动窗口方法。\n这里要介绍的是一种**全卷积**的方法，简单来说就是网络中**用卷积层代替了全连接层**，如下图所示。\n>输入图片大小是16x16，经过一系列卷积操作，提取了2x2的特征图，但是这个2x2的图上每个元素都是和原图是一一对应的，如图上蓝色的格子对应蓝色的区域，这不就是相当于在原图上做大小为14x14的窗口滑动，且步长为2，共产生4个字区域。最终输出的通道数为4，可以看成4个类别的预测概率值，这样一次CNN计算就可以实现窗口滑动的所有子区域的分类预测。\n\n![](/img/2018-04-21-YOLOv1-2.jpg)\n\n这其实是overfeat算法的思路。之所可以CNN可以实现这样的效果是因为卷积操作的特性，就是**图片的空间位置信息的不变性**，尽管卷积过程中图片大小减少，但是**位置对应关系还是保存的**。说点题外话，这个思路也被R-CNN借鉴，从而诞生了Fast R-cNN算法。\n\n### 2.2 设计理念\n\n整体来看，Yolo算法采用一个单独的CNN模型实现end-to-end的目标检测，整个系统如图5所示：首先将输入图片resize到448x448，然后送入CNN网络，最后处理网络预测结果得到检测的目标。相比R-CNN算法，其是一个统一的框架，其速度更快，而且Yolo的训练过程也是end-to-end的。\n\n![](/img/2018-04-21-YOLOv1-3.jpg)\n\n具体来说，Yolo的CNN网络将输入的图片分割成  \\\\(S\\times S\\\\)  网格，然后**每个单元格负责去检测那些中心点落在该格子内的目标**。\n>如下图所示，可以看到狗这个目标的中心落在左下角一个单元格内，那么该单元格负责预测这个狗。\n\n![](/img/2018-04-21-YOLOv1-4.jpg)\n\n每个单元格会预测  \\\\(B\\\\)  个边界框（bounding box）以及边界框的**置信度（confidence score）**。所谓置信度其实包含两个方面:\n\n1. 这个边界框**含有目标的可能性大小**：\\\\(Pr(object)\\\\) 当该边界框是背景时（即不包含目标），此时  \\\\(Pr(object)=0\\\\)  。而当该边界框包含目标时，  \\\\(Pr(object)=1\\\\)  。\n2. 这个**边界框的准确度**：边界框的准确度可以用预测框与实际框（ground truth）的IOU（intersection over union，交并比）来表征，记为  \\\\(\\text{IOU}^{truth}\\_{pred}\\\\)  。\n\n因此置信度可以定义为  \\\\(Pr(object)*\\text{IOU}^{truth}\\_{pred}\\\\)  。很多人可能将Yolo的置信度看成边界框是否含有目标的概率，但是其实它是两个因子的乘积，预测框的准确度也反映在里面。\n\n边界框的大小与位置可以用4个值来表征：  \\\\((x, y,w,h)\\\\)  ，其中  \\\\((x,y)\\\\)  是边界框的中心坐标，而  \\\\(w\\\\)  和  \\\\(h\\\\)  是边界框的宽与高。还有一点要注意，中心坐标的预测值  \\\\((x,y)\\\\)  是相对于每个单元格左上角坐标点的偏移值，并且单位是相对于单元格大小的，单元格的坐标定义如上图所示。而边界框的  \\\\(w\\\\)  和  \\\\(h\\\\)  预测值是相对于整个图片的宽与高的比例，这样理论上4个元素的大小应该在  \\\\([0,1]\\\\)  范围。这样，每个边界框的预测值实际上包含5个元素：  \\\\((x,y,w,h,c)\\\\)  ，其中前4个表征边界框的大小与位置，而**最后一个值是置信度**。\n\n\n还有分类问题，对于每一个单元格其还要给出预测出 C 个类别概率值，其表征的是由该单元格负责预测的边界框其目标属于各个类别的概率。但是这些概率值其实是在各个边界框置信度下的条件概率，即  \\\\(Pr(class\\_{i}|object)\\\\)  。值得注意的是，不管一个单元格预测多少个边界框，其**只预测一组类别概率值**，这是Yolo算法的一个缺点，在后来的改进版本中，Yolo9000是把类别概率预测值与边界框是绑定在一起的。同时，我们可以计算出各个边界框类别置信度（class-specific confidence scores）:       \n$$Pr(class\\_{i}|object)\\*Pr(object)\\*\\text{IOU}^{truth}\\_{pred}=Pr(class\\_{i})\\*\\text{IOU}^{truth}\\_{pred}$$ \n\n   \n边界框类别置信度表征的是**该边界框中目标属于各个类别的可能性大小以及边界框匹配目标的好坏**。后面会说，一般会根据类别置信度来过滤网络的预测框。\n\n\n总结一下，每个单元格需要预测  \\\\((B\\times 5+C)\\\\)  个值（ \\\\(B\\\\)是边界框个数 ）。如果将输入图片划分为  \\\\(S\\times S\\\\)  网格，那么最终预测值为  \\\\(S\\times S\\times (B\\times 5+C)\\\\)  大小的张量。整个模型的预测值结构如下图所示。对于 PASCAL VOC 数据，其共有20个类别，如果使用  \\\\(S=7,B=2\\\\)  ，那么最终的预测结果就是  \\\\(7\\times 7\\times 30\\\\)  大小的张量（ \\\\(C\\\\)为20个分类 ）。在下面的网络结构中我们会详细讲述每个单元格的预测值的分布位置。\n\n![](/img/2018-04-21-YOLOv1-5.jpg)\n\n\n### 2.3 网络设计\n\nYolo采用卷积网络来提取特征，然后使用全连接层来得到预测值。网络结构参考GooLeNet模型，包含24个卷积层和2个全连接层，如图8所示。对于卷积层，主要使用1x1卷积来做channle reduction，然后紧跟 \\\\(3\\times 3\\\\) 卷积。对于卷积层和全连接层，采用 `Leaky ReLU` 激活函数：  \\\\(max(x, 0.1x)\\\\)  。但是最后一层却采用线性激活函数。\n\n\n![](/img/2018-04-21-YOLOv1-6.jpg)\n\n可以看到网络的最后输出为  \\\\(7\\times 7\\times 30\\\\)  大小的张量。这和前面的讨论是一致的。这个张量所代表的具体含义如图9所示。对于每一个单元格，前20个元素是类别概率值，然后2个元素是边界框置信度，两者相乘可以得到类别置信度，最后8个元素是边界框的  \\\\((x, y,w,h)\\\\)  。大家可能会感到奇怪，对于边界框为什么把置信度  \\\\(c\\\\)  和  \\\\((x, y,w,h)\\\\)  都分开排列，而不是按照  \\\\((x, y,w,h,c)\\\\)  这样排列，其实纯粹是为了计算方便，因为实际上这30个元素都是对应一个单元格，其排列是可以任意的。但是分离排布，可以方便地提取每一个部分。\n\n**代码如下：**\n\n```python\n    def _build_net(self):\n        \"\"\"build the network\"\"\"\n        if self.verbose:\n            print(\"Start to build the network ...\")\n        self.images = tf.placeholder(tf.float32, [None, 448, 448, 3])\n        net = self._conv_layer(self.images, 1, 64, 7, 2)\n        net = self._maxpool_layer(net, 1, 2, 2)\n        net = self._conv_layer(net, 2, 192, 3, 1)\n        net = self._maxpool_layer(net, 2, 2, 2)\n        net = self._conv_layer(net, 3, 128, 1, 1)\n        net = self._conv_layer(net, 4, 256, 3, 1)\n        net = self._conv_layer(net, 5, 256, 1, 1)\n        net = self._conv_layer(net, 6, 512, 3, 1)\n        net = self._maxpool_layer(net, 6, 2, 2)\n        net = self._conv_layer(net, 7, 256, 1, 1)\n        net = self._conv_layer(net, 8, 512, 3, 1)\n        net = self._conv_layer(net, 9, 256, 1, 1)\n        net = self._conv_layer(net, 10, 512, 3, 1)\n        net = self._conv_layer(net, 11, 256, 1, 1)\n        net = self._conv_layer(net, 12, 512, 3, 1)\n        net = self._conv_layer(net, 13, 256, 1, 1)\n        net = self._conv_layer(net, 14, 512, 3, 1)\n        net = self._conv_layer(net, 15, 512, 1, 1)\n        net = self._conv_layer(net, 16, 1024, 3, 1)\n        net = self._maxpool_layer(net, 16, 2, 2)\n        net = self._conv_layer(net, 17, 512, 1, 1)\n        net = self._conv_layer(net, 18, 1024, 3, 1)\n        net = self._conv_layer(net, 19, 512, 1, 1)\n        net = self._conv_layer(net, 20, 1024, 3, 1)\n        net = self._conv_layer(net, 21, 1024, 3, 1)\n        net = self._conv_layer(net, 22, 1024, 3, 2)\n        net = self._conv_layer(net, 23, 1024, 3, 1)\n        net = self._conv_layer(net, 24, 1024, 3, 1)\n        net = self._flatten(net)\n        net = self._fc_layer(net, 25, 512, activation=leak_relu)\n        net = self._fc_layer(net, 26, 4096, activation=leak_relu)\n        net = self._fc_layer(net, 27, self.S*self.S*(self.C+5*self.B))\n        self.predicts = net\n        # self.predicts.shape = (?, 1470)\n        # 1470 = 7*7*30\n```\n\n\n这里来解释一下，首先网络的预测值是一个二维张量  \\\\(P\\\\)  ，其shape为  \\\\([batch, 7\\times 7\\times 30]\\\\)  。\n\n- 采用切片，那么  \\\\(P\\_{[:,\\ 0:7\\*7\\*20]}\\\\)  就是类别概率部分\n- 而  \\\\(P\\_{[:,\\ 7\\*7\\*20:7\\*7\\*(20+2)]}\\\\)  是置信度部分\n- 最后剩余部分  \\\\(P_{[:,\\ 7\\*7\\*(20+2):]} \\\\) 是边界框的预测结果。\n\n这样，提取每个部分是非常方便的，这会方面后面的训练及预测时的计算。\n\n\n![](/img/2018-04-21-YOLOv1-7.jpg)\n\n```python\n    def _build_detector(self):\n        \"\"\"Interpret the net output and get the predicted boxes\"\"\"\n        # the width and height of orignal image\n        self.width = tf.placeholder(tf.float32, name=\"img_w\")\n        self.height = tf.placeholder(tf.float32, name=\"img_h\")\n        # get class prob, confidence, boxes from net output\n        idx1 = self.S * self.S * self.C\n        idx2 = idx1 + self.S * self.S * self.B\n        # class prediction\n        class_probs = tf.reshape(self.predicts[0, :idx1], [\n                                 self.S, self.S, self.C])\n        # confidence\n        confs = tf.reshape(self.predicts[0, idx1:idx2], [\n                           self.S, self.S, self.B])\n        # boxes -> (x, y, w, h)\n        boxes = tf.reshape(self.predicts[0, idx2:], [\n                           self.S, self.S, self.B, 4])\n\n        # convert the x, y to the coordinates relative to the top left point of the image\n        # the predictions of w, h are the square root\n        # multiply the width and height of image\n        boxes = tf.stack([(boxes[:, :, :, 0] + tf.constant(self.x_offset, dtype=tf.float32)) / self.S * self.width,\n                          (boxes[:, :, :, 1] + tf.constant(self.y_offset,\n                                                           dtype=tf.float32)) / self.S * self.height,\n                          tf.square(boxes[:, :, :, 2]) * self.width,\n                          tf.square(boxes[:, :, :, 3]) * self.height], axis=3)\n\n        # class-specific confidence scores [S, S, B, C]\n        scores = tf.expand_dims(confs, -1) * tf.expand_dims(class_probs, 2)\n\n        scores = tf.reshape(scores, [-1, self.C])  # [S*S*B, C]\n        boxes = tf.reshape(boxes, [-1, 4])  # [S*S*B, 4]\n\n        # find each box class, only select the max score\n        box_classes = tf.argmax(scores, axis=1)\n        box_class_scores = tf.reduce_max(scores, axis=1)\n\n        # filter the boxes by the score threshold\n        filter_mask = box_class_scores >= self.threshold\n        scores = tf.boolean_mask(box_class_scores, filter_mask)\n        boxes = tf.boolean_mask(boxes, filter_mask)\n        box_classes = tf.boolean_mask(box_classes, filter_mask)\n\n        # non max suppression (do not distinguish different classes)\n        # ref: https://tensorflow.google.cn/api_docs/python/tf/image/non_max_suppression\n        # box (x, y, w, h) -> box (x1, y1, x2, y2)\n        _boxes = tf.stack([boxes[:, 0] - 0.5 * boxes[:, 2], boxes[:, 1] - 0.5 * boxes[:, 3],\n                           boxes[:, 0] + 0.5 * boxes[:, 2], boxes[:, 1] + 0.5 * boxes[:, 3]], axis=1)\n        nms_indices = tf.image.non_max_suppression(_boxes, scores,\n                                                   self.max_output_size, self.iou_threshold)\n        self.scores = tf.gather(scores, nms_indices)\n        self.boxes = tf.gather(boxes, nms_indices)\n        self.box_classes = tf.gather(box_classes, nms_indices)\n```   \n\n### 2.4 网络训练\n\n在训练之前，先在ImageNet上进行了预训练，其预训练的分类模型采用GooLeNet (见前图) 中前20个卷积层，然后添加一个average-pool层和全连接层。预训练之后，在预训练得到的20层卷积层之上加上随机初始化的4个卷积层和2个全连接层。由于检测任务一般需要更高清的图片，所以将网络的输入从 \\\\(224\\times 224\\\\) 增加到了 \\\\(448\\times 448\\\\) 。整个网络的流程如下图所示：\n\n![](/img/2018-04-21-YOLOv1-8.jpg)\n\n\n下面是训练损失函数的分析，Yolo算法将目标检测看成**回归问题**，所以采用的是**均方差损失函数**。但是对不同的部分采用了不同的权重值。首先区分定位误差和分类误差。对于定位误差，即边界框坐标预测误差，采用较大的权重  \\\\(\\lambda \\_{coord}=5\\\\)  。然后其区分不包含目标的边界框与含有目标的边界框的置信度，对于前者，采用较小的权重值  \\\\(\\lambda _{noobj}=0.5\\\\)  。其它权重值均设为1。然后采用均方误差，其同等对待大小不同的边界框，但是实际上较小的边界框的坐标误差应该要比较大的边界框要更敏感。为了保证这一点，将网络的边界框的宽与高预测改为对其平方根的预测，即预测值变为  \\\\((x,y,\\sqrt{w}, \\sqrt{h})\\\\)  。\n\n另外一点时，由于每个单元格预测多个边界框。但是其对应类别只有一个。那么在训练时，如果该单元格内确实存在目标，那么只选择与ground truth的 \\\\(IOU\\\\) 最大的那个边界框来负责预测该目标，而其它边界框认为不存在目标。这样设置的一个结果将会使一个单元格对应的边界框更加专业化，其可以分别适用不同大小，不同高宽比的目标，从而提升模型性能。大家可能会想如果一个单元格内存在多个目标怎么办，其实这时候**Yolo算法就只能选择其中一个来训练**，这也是Yolo算法的缺点之一。要注意的一点时，对于不存在对应目标的边界框，其误差项就是只有置信度，坐标项误差是没法计算的。而只有当一个单元格内确实存在目标时，才计算分类误差项，否则该项也是无法计算的。\n\n**综上讨论，最终的损失函数计算如下：**\n\n![](/img/2018-04-21-YOLOv1-9.jpg)\n\n- 其中第一项是边界框中心坐标的误差项，  \\\\(1^{obj}_{ij}\\\\)  指的是第  \\\\(i\\\\)  个单元格存在目标，且该单元格中的第  \\\\(j\\\\)  个边界框负责预测该目标。\n- 第二项是边界框的高与宽的误差项。\n- 第三项是包含目标的边界框的置信度误差项。\n- 第四项是不包含目标的边界框的置信度误差项。\n- 而最后一项是包含目标的单元格的分类误差项，  \\\\(1^{obj}_{i}\\\\)  指的是第  \\\\(i\\\\)  个单元格存在目标。\n\n\n\n\n### 2.5 网络预测\n\n在说明Yolo算法的预测过程之前，这里先介绍一下非极大值抑制算法（non maximum suppression, NMS），这个算法不单单是针对Yolo算法的，而是所有的检测算法中都会用到。NMS算法主要解决的是一个目标被多次检测的问题，如图11中人脸检测，可以看到人脸被多次检测，但是其实我们希望最后仅仅输出其中一个最好的预测框，比如对于美女，只想要红色那个检测结果。那么可以采用NMS算法来实现这样的效果：首先从所有的检测框中找到置信度最大的那个框，然后挨个计算其与剩余框的IOU，如果其值大于一定阈值（重合度过高），那么就将该框剔除；然后对剩余的检测框重复上述过程，直到处理完所有的检测框。Yolo预测过程也需要用到NMS算法。\n\n ![](/img/2018-04-21-YOLOv1-10.jpg)\n\n下面就来分析Yolo的预测过程，这里我们不考虑batch，认为只是预测一张输入图片。根据前面的分析，最终的网络输出是  \\\\(7\\times 7 \\times 30\\\\)  ，但是我们可以将其分割成三个部分：类别概率部分为  \\\\([7, 7, 20]\\\\)  ，置信度部分为  \\\\([7,7,2]\\\\)  ，而边界框部分为  \\\\([7,7,2,4]\\\\)  （对于这部分不要忘记根据原始图片计算出其真实值）。然后将前两项相乘（矩阵  \\\\([7, 7, 20]\\\\)  乘以  \\\\([7,7,2]\\\\)  可以各补一个维度来完成  \\\\([7,7,1,20]\\times [7,7,2,1]\\\\)  ）可以得到类别置信度值为  \\\\([7, 7,2,20]\\\\)  ，这里总共预测了  \\\\(7\\*7\\*2=98\\\\)  个边界框。\n\n\n所有的准备数据已经得到了，那么我们先说第一种策略来得到检测框的结果，我认为这是最正常与自然的处理。        \n首先，对于每个预测框根据类别置信度选取置信度最大的那个类别作为其预测标签，经过这层处理我们得到各个预测框的预测类别及对应的置信度值，其大小都是  \\\\([7,7,2]\\\\)  。一般情况下，会设置置信度阈值，就是将置信度小于该阈值的box过滤掉，所以经过这层处理，剩余的是置信度比较高的预测框。最后再对这些预测框使用NMS算法，最后留下来的就是检测结果。一个值得注意的点是NMS是对所有预测框一视同仁，还是区分每个类别，分别使用NMS。Ng在deeplearning.ai中讲应该区分每个类别分别使用NMS，但是看了很多实现，其实还是同等对待所有的框，我觉得可能是不同类别的目标出现在相同位置这种概率很低吧。\n\n上面的预测方法应该非常简单明了，但是对于Yolo算法，其却采用了另外一个不同的处理思路（至少从C源码看是这样的），其区别就是先使用NMS，然后再确定各个box的类别。其基本过程如图12所示。对于98个boxes，首先将小于置信度阈值的值归0，然后分类别地对置信度值采用NMS，这里NMS处理结果不是剔除，而是将其置信度值归为0。最后才是确定各个box的类别，当其置信度值不为0时才做出检测结果输出。这个策略不是很直接，但是貌似Yolo源码就是这样做的。Yolo论文里面说NMS算法对Yolo的性能是影响很大的，所以可能这种策略对Yolo更好。但是我测试了普通的图片检测，两种策略结果是一样的。\n\n![](/img/2018-04-21-YOLOv1-11.jpg)\n<small><center>Yolo的预测处理流程</center></small>\n\n\n### 2.6 算法性能分析\n\n\n这里看一下Yolo算法在PASCAL VOC 2007数据集上的性能，这里Yolo与其它检测算法做了对比，包括DPM，R-CNN，Fast R-CNN以及Faster R-CNN。其对比结果如表1所示。与实时性检测方法DPM对比，可以看到Yolo算法可以在较高的mAP上达到较快的检测速度，其中Fast Yolo算法比快速DPM还快，而且mAP是远高于DPM。但是相比Faster R-CNN，Yolo的mAP稍低，但是速度更快。所以。Yolo算法算是在速度与准确度上做了折中。\n\n![](/img/2018-04-21-YOLOv1-12.jpg)\n\n现在来总结一下Yolo的优缺点。首先是优点，Yolo采用一个CNN网络来实现检测，是单管道策略，其训练与预测都是end-to-end，所以Yolo算法比较简洁且速度快。第二点由于Yolo是对整张图片做卷积，所以其在检测目标有更大的视野，它不容易对背景误判。其实我觉得全连接层也是对这个有贡献的，因为全连接起到了attention的作用。另外，Yolo的泛化能力强，在做迁移时，模型鲁棒性高。\n\n最后不得不谈一下Yolo的缺点，首先Yolo各个单元格仅仅预测两个边界框，而且属于一个类别。对于小物体，Yolo的表现会不如人意。这方面的改进可以看SSD，其采用多尺度单元格。也可以看Faster R-CNN，其采用了anchor boxes。Yolo对于在物体的宽高比方面泛化率低，就是无法定位不寻常比例的物体。当然Yolo的定位不准确也是很大的问题。\n\n\n\n\n\n\n\n## 参考\n\n1. [目标检测|YOLO原理与实现](https://zhuanlan.zhihu.com/p/32525231)\n2. [源码地址：xiaohu2015/DeepLearning_tutorials](https://github.com/xiaohu2015/DeepLearning_tutorials/blob/master/ObjectDetections/yolo/yolo_tf.py)\n\n\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>","tags":["目标检测"],"categories":["深度学习"]},{"title":"Faster R-CNN 目标检测（四）","url":"/2018/04/16/FasterRCNN/","content":"\n\n\n## 1 Faster R-CNN 简介\n经过RCNN和Fast RCNN的积淀，Ross B. Girshick在2016年提出了新的Faster RCNN，在结构上，Faster RCN已经将特征抽取(feature extraction)，proposal提取，bounding box regression(rect refine)，classification都整合在了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。\n<!-- more -->\n**示意图1：**\n\n![](/img/2018-04-16-FasterRCNN-1.jpg)\n\n**示意图2：**\n\n![](/img/2018-04-16-FasterRCNN-2.jpg)\n\n**Faster RCNN 可以分为4个主要内容：**\n\n- **Conv layers**。作为一种CNN网络目标检测方法，Faster RCNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。\n- **Region Proposal Networks**。RPN网络用于生成region proposals。该层通过softmax判断anchors属于foreground或者background，再利用bounding box regression修正anchors获得精确的proposals。\n- **Roi Pooling**。该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。\n- **Classification**。利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。\n\n\n下图展示了Python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构。\n\n![](/img/2018-04-16-FasterRCNN-3.jpg)\n\n>可以清晰的看到该网络对于一副任意大小 \\\\(P\\times Q\\\\) 的图像，首先缩放至固定大小 \\\\(M\\times N\\\\)，然后将 \\\\(M\\times N\\\\) 图像送入网络；而Conv layers中包含了13个conv层+13个relu层+4个pooling层；    \n>RPN网络首先经过 \\\\(3\\times 3\\\\) 卷积，再分别生成foreground anchors与bounding box regression偏移量，然后计算出proposals；      \n>而Roi Pooling层则利用proposals从feature maps中提取proposal feature送入后续全连接和softmax网络作classification（即分类proposal到底是什么object）。\n\n\n\n## 2 Conv layers\n\nConv layers包含了conv，pooling，relu三种层。以python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构为例，如图2，Conv layers部分共有13个conv层，13个relu层，4个pooling层。这里有一个非常容易被忽略但是又无比重要的信息，在Conv layers中：\n\n1. 所有的conv层都是：**kernel_size=3，pad=1**\n2. 所有的pooling层都是：**kernel_size=2，stride=2**\n\n为何重要？在Faster RCNN Conv layers中对所有的卷积都做了扩边处理（pad=1，即填充一圈0），导致原图变为 \\\\((M+2)\\times (N+2)\\\\) 大小，再做 \\\\(3\\times 3\\\\) 卷积后输出 \\\\(M\\times N\\\\)。正是这种设置，导致Conv layers中的conv层不改变输入和输出矩阵大小。如下图：\n\n![](/img/2018-04-16-FasterRCNN-4.jpg)\n\n类似的是，Conv layers中的pooling层kernel_size=2，stride=2。这样每个经过pooling层的MxN矩阵，都会变为 \\\\((M/2)\\times (N/2)\\\\) 大小。综上所述，在整个Conv layers中，conv和relu层不改变输入输出大小，只有pooling层使输出长宽都变为输入的 \\\\(\\frac 1 2\\\\)。\n\n那么，一个 \\\\(M\\times N\\\\) 大小的矩阵经过Conv layers固定变为 \\\\((M/16) \\times (N/16)\\\\)！这样Conv layers生成的featuure map中都可以和原图对应起来。\n\n\n\n## 3 Region Proposal Networks (RPN)\n\n\n经典的检测方法生成检测框都非常耗时，如OpenCV adaboost使用滑动窗口+图像金字塔生成检测框；或如RCNN使用**SS(Selective Search)**方法生成检测框。而Faster RCNN则抛弃了传统的滑动窗口和SS方法，直接使用**RPN生成检测框**，将Region Proposal**也交给CNN来做**，它能**和整个检测网络共享全图的卷积特征**，使得区域建议几乎不花时间，这也是Faster RCNN的巨大优势，能极大提升检测框的生成速度。\n\n>RCNN解决的是，“为什么不用CNN做classification呢？”     \n>Fast R-CNN解决的是，“为什么不一起输出bounding box和label呢？”     \n>Faster R-CNN解决的是，“为什么还要用selective search呢？”\n\nRPN的核心思想是使用CNN卷积神经网络直接产生Region Proposal，使用的方法本质上就是滑动窗口（只需在最后的卷积层上滑动一遍）。\n\nRPN的**输入是任意大小的图像，输出是一组打过分的候选框（object proposals）**。在卷积的最后一层feature map上使用固定大小的窗口（注意sliding window**只是选择位置**，除此之外没有其它作用，和后面的3*3的卷积区分）滑动，每个窗口会输出固定大小维度的特征（图中256)，每一个窗口对候选的9个box进行回归坐标和分类（这里的分类表示box中是不是一个object，而不是具体的类别）。\n\n![](/img/2018-04-16-FasterRCNN-5.jpg)\n\n<br />\n\n下图展示了RPN网络的具体结构。可以看到RPN网络实际分为2条线，\n\n1. 上面一条**通过softmax分类anchors获得foreground和background**（检测目标是foreground）\n2. 下面一条用于计算**对于anchors的bounding box regression偏移量**，以获得精确的proposal。\n\n而最后的**Proposal层**则负责**综合**foreground anchors和bounding box regression偏移量获取proposals，**同时剔除太小和超出边界的proposals**。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。\n\n![](/img/2018-04-16-FasterRCNN-6.jpg)\n\n\n\n### 3.1 anchors\n\nAnchors是一组大小固定的参考窗口：     \n\\\\(三种尺度 (128^2，256^2，512^2)\\times 三种长宽比(1:1，1:2，2:1)\\\\) ，如下图所示，表示RPN网络中对特征图滑窗时每个滑窗位置所对应的原图区域中9种可能的大小，相当于模板，对任意图像任意滑窗位置都是这9中模板。继而根据图像大小计算滑窗中心点对应原图区域的中心点，通过中心点和size就可以得到滑窗位置和原图位置的映射关系，**由此原图位置并根据与Ground Truth重复率贴上正负标签，让RPN学习该Anchors是否有物体即可**。\n \n![](/img/2018-04-16-FasterRCNN-7.jpg)\n\n所谓anchors，实际上就是一组由rpn/generate_anchors.py生成的矩形。直接运行作者demo中的generate_anchors.py可以得到以下输出：\n\n```python\n[[ -84.  -40.   99.   55.]  \n [-176.  -88.  191.  103.]  \n [-360. -184.  375.  199.]  \n [ -56.  -56.   71.   71.]  \n [-120. -120.  135.  135.]  \n [-248. -248.  263.  263.]  \n [ -36.  -80.   51.   95.]  \n [ -80. -168.   95.  183.]  \n [-168. -344.  183.  359.]]  \n```\n\n其中每行的4个值 \\\\([x1,y1,x2,y2]\\\\) 代表矩形左上和右下角点坐标。9个矩形共有3种形状，长宽比为大约为：\\\\(width:height = [1:1, 1:2, 2:1]\\\\) 三种，实际上通过anchors就引入了检测中常用到的**多尺度方法**。\n\n\n\n**注：**关于上面的anchors size，其实是根据检测图像设置的。在python demo中，会把任意大小的输入图像reshape成800x600（即第二张图中的M=800，N=600）。再回头来看anchors的大小，anchors中长宽1:2中最大为352x704，长宽2:1中最大736x384，基本是cover了800x600的各个尺度和形状。\n\n\n那么这9个anchors是做什么的呢？如下图，遍历Conv layers计算获得的**feature maps，为每一个点都配备这9种anchors作为初始的检测框**。这样做获得检测框很不准确，不用担心，后面还有2次bounding box regression可以修正检测框位置。\n\n![](/img/2018-04-16-FasterRCNN-8.jpg)\n\n\n**解释一下上面这张图的数字：**\n\n1. 在原文中使用的是ZF model中，其Conv Layers中最后的conv5层**num_output=256**，对应生成256张特征图，所以相当于feature map每个点都是**256-d**\n2. 在conv5之后，做了RPN\\_conv/3x3卷积且num_output=256，相当于每个点又融合了周围3x3的空间信息，同时256-d不变（如第四张图中的红框）\n3. 假设在conv5 feature map中每个点上有 \\\\(k\\\\) 个anchor（默认k=9），而每个anhcor要分foreground和background，所以每个点由256d feature转化为 \\\\(cls=2k\\\\) scores；而每个anchor都有 \\\\([x, y, w, h]\\\\) 对应4个偏移量，所以 \\\\(reg=4k\\\\) coordinates\n4. 补充一点，全部anchors拿去训练太多了，训练程序会选取256个合适的anchors进行训练（什么是合适的anchors下文有解释）\n\n\n![](/img/2018-04-16-FasterRCNN-9.jpg)\n\n**看完单个点的，再来看完整的：**\n\n![](/img/2018-04-16-FasterRCNN-10.jpg)\n\n**关于正负样本的划分：**考察训练集中的每张图像（含有人工标定的ground true box） 的所有anchor（\\\\(N\\times  M \\times k\\\\)）\n\n1. 对每个标定的ground true box区域，与其重叠比例最大的anchor记为 正样本 (保证每个ground true 至少对应一个正样本anchor)\n2. 对1剩余的anchor，如果其与某个标定区域重叠比例大于0.7，记为正样本（每个ground true box可能会对应多个正样本anchor。但每个正样本anchor 只可能对应一个grand true box）；如果其与任意一个标定的重叠比例都小于0.3，记为负样本。 \n3. 对1,2剩余的anchor，弃去不用。\n4. 跨越图像边界的anchor弃去不用\n\n>所谓IoU，就是预测box和真实box的覆盖率，其值等于两个box的交集除以两个box的并集。其它的anchor不参与训练。\n\n\n### 3.2 bounding box regression 原理\n\n\n\n介绍bounding box regression数学模型及原理。如图9所示绿色框为飞机的Ground Truth(GT)，红色为提取的foreground anchors，那么即便红色的框被分类器识别为飞机，但是由于红色的框定位不准，这张图相当于没有正确的检测出飞机。所以我们希望采用一种方法对红色的框进行微调，使得foreground anchors和GT更加接近。\n\n![](/img/2018-04-16-FasterRCNN-11.jpg)\n\n对于窗口一般使用四维向量 \\\\((x, y, w, h)\\\\) 表示，分别表示窗口的中心点坐标和宽高。对于下图，红色的框A代表原始的Foreground Anchors，绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G'，即：给定 \\\\(A=(A_x, A_y, A_w, A_h)\\\\)，寻找一种映射 \\\\(f\\\\)，使得 \n\n$$f(A_x, A_y, A_w, A_h)=(G'x, G'y, G'w, G'h)≈(Gx, Gy, Gw, Gh)$$\n\n![](/img/2018-04-16-FasterRCNN-12.jpg)\n\n\n那么经过何种变换才能从A变为G'呢？ 比较简单的思路就是:\n\n1. 先做平移 \\\\((\\Delta x, \\Delta y)\\\\)，\\\\(\\Delta x = A_w d_x(A), \\Delta y = A_h d_y(A)\\\\)\n\n![](/img/2018-04-16-FasterRCNN-13.jpg)\n\n\n2. 再做缩放 \\\\((S_w, S_h)\\\\)，\\\\(S_w = exp(d_w(A)), S_h = exp(d_h(A))\\\\)\n\n![](/img/2018-04-16-FasterRCNN-14.jpg)\n\n观察上面4个公式发现，需要学习的是 \\\\(d\\_{x}(A)，d\\_{y}(A)，d\\_{w}(A)，d\\_{h}(A)\\\\) 这四个变换。当输入的anchor与GT相差较小时，可以认为这种变换是一种线性变换， 那么就可以用线性回归来建模对窗口进行微调（**注意，只有当anchors和GT比较接近时，才能使用线性回归模型，否则就是复杂的非线性问题了**）。对应于Faster RCNN原文，平移量 \\\\((t_{x}, t_{y})\\\\) 与尺度因子 \\\\((t_{w}, t_{h})\\\\) 如下：\n\n![](/img/2018-04-16-FasterRCNN-15.jpg)\n\n接下来的问题就是如何通过线性回归获得 \\\\(d\\_{x}(A)，d\\_{y}(A)，d_{w}(A)，d\\_{h}(A)\\\\) 了。线性回归就是给定输入的特征向量 \\\\(X\\\\), 学习一组参数 \\\\(W\\\\), 使得经过线性回归后的值跟真实值Y（即GT）非常接近，即 \\\\(Y=WX\\\\)。对于该问题，输入 \\\\(X\\\\) 是一张经过num_output=1的 \\\\(1\\times 1\\\\) 卷积获得的feature map，定义为 \\\\(Φ\\\\)；同时还有训练传入的GT，即 \\\\((tx, ty, tw, th)\\\\)。输出是 \\\\(dx(A)，dy(A)，dw(A)，dh(A)\\\\) 四个变换。那么目标函数可以表示为：\n\n![](/img/2018-04-16-FasterRCNN-16.jpg)\n\n其中 \\\\(Φ(A)\\\\) 是对应anchor的feature map组成的特征向量，\\\\(w\\\\) 是需要学习的参数，\\\\(d(A)\\\\) 是得到的预测值（\\\\(*\\\\)表示 \\\\(x，y，w，h\\\\)，也就是每一个变换对应一个上述目标函数）。为了让预测值 \\\\((tx, ty, tw, th)\\\\) 与真实值最小，得到损失函数：\n\n![](/img/2018-04-16-FasterRCNN-17.jpg)\n\n函数优化目标为：\n\n![](/img/2018-04-16-FasterRCNN-18.jpg)\n\n需要说明，只有在GT与需要回归框位置比较接近时，才可近似认为上述线性变换成立。\n\n说完原理，对应于Faster RCNN原文，foreground anchor与ground truth之间的平移量 \\\\((t_x, t_y)\\\\) 与尺度因子 \\\\((t_w, t_h)\\\\) 如下：\n\n![](/img/2018-04-16-FasterRCNN-19.jpg)\n\n其中：\n\n$$x,y,w,h\\ 为\\ box\\ 的中心坐标，宽，高$$\n\n$$\\begin{cases}\nx: predicted\\ box\\ (就是Proposal)\\\\\\\\\nx_{a}: anchor\\ box\\ (N\\times M \\times k 个)\\\\\\\\\nx^{*}: Ground\\ Truth\\ (正确标定的GT)\\\\\n\\end{cases}\n$$\n\n![](/img/2018-04-16-FasterRCNN-20.jpg)\n\n对于训练bouding box regression网络回归分支，输入是cnn feature \\\\(Φ\\\\)，监督信号是Anchor与GT的差距 \\\\((t_x, t_y, t_w, t_h)\\\\)，即训练目标是：输入 Φ的情况下使网络输出与监督信号尽可能接近。\n\n那么当bouding box regression工作时，再输入Φ时，回归网络分支的输出就是每个Anchor的平移量和变换尺度 \\\\((t_x, t_y, t_w, t_h)\\\\)，显然即可用来修正Anchor位置了。\n\n\n\n### 3.3 Proposal Layer\n\n\nProposal Layer负责综合所有 \\\\([d_{x}(A)，d_{y}(A)，d_{w}(A)，d_{h}(A)]\\\\) 变换量和foreground anchors，计算出精准的proposal，送入后续RoI Pooling Layer。\n\nProposal Layer有3个输入：fg/bg anchors分类器结果rpn_cls_prob_reshape，对应的bbox reg的[dx(A)，dy(A)，dw(A)，dh(A)]变换量rpn_bbox_pred，以及im_info；另外还有参数feat_stride=16，这和图4是对应的。\n\n首先解释**im_info**。对于一副任意大小PxQ图像，传入Faster RCNN前首先reshape到固定 \\\\(M\\times N\\\\)，\\\\(im\\\\_info=[M, N, scale\\\\_factor]\\\\)则保存了此次缩放的所有信息。然后经过Conv Layers，经过4次pooling变为 \\\\(W\\times H=(M/16)\\times (N/16)\\\\) 大小，其中feature_stride=16则保存了该信息。所有这些数值都是为了将proposal映射回原图而设置的，如下图，毕竟检测就是为了在原图上画一个框而已。\n\n![](/img/2018-04-16-FasterRCNN-21.jpg)\n\nProposal Layer forward 按照以下顺序依次处理：\n\n1. 再次生成anchors，并对所有的anchors做bbox reg位置回归（注意这里的anchors生成顺序和之前是即完全一致的）\n2. 按照输入的foreground softmax scores由大到小排序anchors，提取前pre_nms_topN(e.g. 6000)个anchors。即提取修正位置后的foreground anchors\n3. 利用feat_stride和im_info将anchors映射回原图，判断fg anchors是否大范围超过边界，剔除严重超出边界fg anchors。\n4. 进行nms（**nonmaximum suppression，非极大值抑制**）\n5. 再次按照nms后的foreground softmax scores由大到小排序fg anchors，提取前post_nms_topN(e.g. 300)结果作为proposal输出。\n\n\n之后输出proposal=[x1, y1, x2, y2]，注意，由于在第三步中将anchors映射回原图判断是否超出边界，所以这里输出的proposal是对应MxN输入图像尺度的，这点在后续网络中有用。另外我认为，严格意义上的检测应该到此就结束了，后续部分应该属于识别了~\n\nRPN网络结构就介绍到这里，总结起来就是：\n\n1. 生成 anchors\n2. softmax 分类器提取 fg anchors\n3. bbox reg 回归 fg anchors\n4. Proposal Layer 生成 proposals\n\n## 4 RoI pooling\n对于传统的CNN（如AlexNet，VGG），当网络训练好后输入的图像尺寸必须是固定值，同时网络输出也是固定大小的vector or matrix。如果输入图像大小不定，这个问题就变得比较麻烦。有2种解决办法：\n\n1. 从图像中crop一部分传入网络\n2. 将图像warp成需要的大小后传入网络\n\n\n![](/img/2018-04-16-FasterRCNN-22.jpg)\n\n\n>传统办法的示意图如上图，可以看到无论采取那种办法都不好，要么crop后破坏了图像的完整结构，要么warp破坏了图像原始形状信息。回忆RPN网络生成的proposals的方法：对foreground anchors进行bound box regression，那么这样获得的proposals也是大小形状各不相同，即也存在上述问题。所以Faster RCNN中提出了RoI Pooling解决这个问题（RoI Pooling是从SPP发展而来）\n\nROI Pooling的过程就是将一个个大小不同的box矩形框，都映射成大小为\\\\(w\\times h\\\\)的矩形框；\n\n![](/img/2018-04-16-FasterRCNN-23.jpg)\n\nRoI Pooling层则负责收集proposal，并计算出proposal feature maps，送入后续网络。Rol pooling层有2个输入：\n\n1. 原始的feature maps（指的是进入RPN层之前的那个Conv层的Feature Map，通常我们称之为 share_conv）\n2. RPN输出的proposal boxes（大小各不相同），其中值得注意的是：坐标的参考系不是针对feature map这张图的，而是针对原图的（神经网络最开始的输入）\n\n\n在之前有明确提到：\\\\(proposal=[x1, y1, x2, y2]\\\\) 是对应 \\\\(M\\times N\\\\) 尺度的，所以首先使用spatial_scale参数将其映射回 \\\\(M/16 \\times N/16\\\\) 大小的feature maps尺度（这里来回多次映射，是有点绕）；\n\n**之后将每个proposal水平和竖直都分为7份，对每一份都进行max pooling处理。这样处理后，即使大小不同的proposal，输出结果都是7x7大小，实现了fixed-length output。**\n\n![](/img/2018-04-16-FasterRCNN-24.jpg)\n\n\n## 5 Classification\n\nClassification部分利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出cls_prob概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量bbox_pred，用于回归更加精确的目标检测框。\n\n![](/img/2018-04-16-FasterRCNN-25.jpg)\n\n## 6 Faster RCNN 训练\n\nFaster CNN的训练，是在已经训练好的model（如VGG_CNN_M_1024，VGG，ZF）的基础上继续进行训练。实际中训练过程分为6个步骤：\n\n1. 在已经训练好的model上，训练RPN网络\n2. 利用步骤1中训练好的RPN网络，收集proposals\n3. 第一次训练Fast RCNN网络\n4. 第二训练RPN网络\n5. 再次利用步骤4中训练好的RPN网络，收集proposals\n6. 第二次训练Fast RCNN网络\n\n\n>可以看到训练过程类似于一种“迭代”的过程，不过只循环了2次。至于只循环了2次的原因是应为作者提到：\"A similar alternating training can be run for more iterations, but we have observed negligible improvements\"，即循环更多次没有提升了。接下来本章以上述6个步骤讲解训练过程。\n\nRPN通过**反向传播**（BP，back-propagation）和**随机梯度下降**（SGD，stochastic gradient descent）进行**端到端**（end-to-end）训练。\n\n每一个mini-batch包含从一张图像中随机提取的256个anchor（注意，不是所有的anchor都用来训练），训练的过程对正负样本进行抽样，前景样本和背景样本均取128个，**保证正负比例为1:1**。如果一个图像中的正样本数小于128，则多用一些负样本以满足有256个Proposal可以用于训练。\n\n\n\n整体损失函数具体为：\n\n$$L(\\{p\\_{i}\\},\\{t\\_{i}\\})={\\frac {1} {N\\_{cls}}}\\sum\\_{i} L\\_{cls}(p\\_{i},p\\_{i}^{\\*}) + \\lambda {\\frac {1}{N\\_{reg}}} \\sum\\_{i}p\\_{i}^{\\*} L\\_{reg}(t\\_i,t\\_i^{\\*})$$\n\n**其中i是mini-batch中anchor的索引**\n\n\\\\(p_i\\\\) 为 anchor 预测为目标的概率（foreground softmax predict概率）；\n\n\\\\(p_i^{\\*}\\\\)代表对应的GT predict概率：\n\n- 即当第i个anchor与GT间 \\\\(IoU>0.7\\\\)，认为是该anchor是foreground，\\\\(p_i^{\\*}=1\\\\)；    \n- 反之 \\\\(IoU<0.3\\\\) 时，认为是该anchor是background，\\\\(p_i^{\\*}=0\\\\)；     \n- 至于那些 \\\\(0.3<IoU<0.7\\\\) 的anchor则不参与训练。\n\n\n$$p_i^*=\\begin{cases}\n0\\ \\ negative\\ label\\\\\\\\\n1\\ \\ positive\\ label\\\\\n\\end{cases}\n$$\n\n\\\\(L\\_{cls}(p\\_i,p\\_i^*)\\\\) 是两个类别（目标 vs. 非目标）的对数损失：\n\n$$ L\\_{cls}(p\\_i,p\\_i^{\\*})=-\\log [p\\_{i}^{\\*}p\\_i+(1-p\\_i^{\\*})(1-p\\_i)]$$\n\n<br />\n\n\\\\(t\\_i=\\\\{t\\_x,t\\_y,t\\_w,t\\_h\\\\}\\\\) 是一个向量，表示预测的 bounding box 包围盒的4个参数化坐标；\n\n\\\\(t\\_i^*\\\\) 是与 positive anchor 对应的 ground truth （GT box）包围盒的坐标向量；\n\n\n\\\\(L\\_{reg}(t\\_i,t\\_i^\\*)\\\\) 是回归损失，用 \\\\(L\\_{reg}(t\\_i,t\\_i^\\*)=R(t\\_i-t\\_i^*)\\\\) 来计算，\\\\(R\\\\) 是 \\\\(smooth L1\\\\) 函数：\n\n$$L\\_{reg}(t\\_i,t\\_i^\\*)={\\sum\\_{i \\in \\{x,y,w,h\\}} smooth\\_{L1}(t\\_i-t\\_i^\\*)}$$\n\n$$\nsmooth\\_{L\\_{1}}(x) =\n\\begin{cases}\n0.5x^2, & \\text{if |x| < 1}\\\\\\\\\n|x|-0.5, & \\text{otherwise}\n\\end{cases}\n$$\n\nsmooth L1 函数图像为：\n\n![](/img/2018-04-16-FasterRCNN-26.jpg)\n\n**注意：**在该loss中乘了 \\\\(pi^\\*\\\\)，相当于只关心foreground anchors的回归，因为负样本时 \\\\(p_i^*=0\\\\) 该项被消去（其实在回归中也完全没必要去关心background）。\n\n<br />\n\n可以看到，整个Loss分为2部分：\n\n1. **cls loss**，即rpn_cls_loss层计算的softmax loss，用于分类anchors为forground与background的网络训练\n2. **reg loss**，即rpn_loss_bbox层计算的soomth L1 loss，用于bounding box regression网络训练。\n\n由于在实际过程中，\\\\(N\\_{cls}\\\\) 和 \\\\(N\\_{reg}\\\\) 差距过大，用参数 \\\\(\\lambda\\\\) 平衡二者（如 \\\\(N\\_{cls}=256\\\\)，\\\\(N_{reg}=2400\\\\) 时设置 \\\\(λ=10\\\\)），使总的网络Loss计算过程中能够均匀考虑2种Loss。\n\n\n\n\n\n\n\n\n## 参考及部分转载\n\n1. [一文读懂Faster R-CNN](https://zhuanlan.zhihu.com/p/31426458)\n2. [Faster R-CNN](https://zhuanlan.zhihu.com/p/24916624)\n3. [Faster RCNN解析](http://www.360doc.com/content/17/0303/14/10408243_633634497.shtml)\n\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n","tags":["深度学习","目标检测"],"categories":["计算机视觉"]},{"title":"机器学习中的熵、条件熵、相对熵和交叉熵","url":"/2018/04/13/entropy/","content":"\n\n## 1 信息熵\n熵 (entropy) 这一词最初来源于热力学。1948年，克劳德·爱尔伍德·香农将热力学中的熵引入信息论，所以也被称为香农熵 (Shannon entropy)，信息熵 (information entropy)。\n\n首先，我们先来理解一下信息这个概念。信息是一个很抽象的概念，泛指人类社会传播的一切内容。那信息可以被量化么？可以的！香农提出的“信息熵”概念解决了这一问题。\n\n一条信息的**信息量**大小和它的**不确定性**有直接的关系。我们需要搞清楚一件非常非常不确定的事，或者是我们一无所知的事，就需要了解大量的信息。相反，如果我们对某件事已经有了较多的了解，我们就不需要太多的信息就能把它搞清楚。所以，从这个角度，我们可以认为，**信息量的度量就等于不确定性的多少**。\n\n>**比如**，有人说广东下雪了。对于这句话，我们是十分不确定的。因为广东几十年来下雪的次数寥寥无几。为了搞清楚，我们就要去看天气预报，新闻，询问在广东的朋友，而这就需要大量的信息，信息熵很高。再比如，中国男足进军2022年卡塔尔世界杯决赛圈。对于这句话，因为确定性很高，几乎不需要引入信息，信息熵很低。\n\n\n<!-- more -->\n考虑一个**离散的随机变量 \\\\(x\\\\)**，由上面两个例子可知，信息的量度应该依赖于概率分布 \\\\(p(x)\\\\)，因此我们想要寻找一个函数 \\\\(I(x)\\\\)，它是概率 \\\\(p(x)\\\\) 的单调函数，表达了信息的内容。怎么寻找呢？如果我们有两个不相关的事件 \\\\(x\\\\) 和 \\\\(y\\\\)，那么观察两个事件同时发生时获得的信息量应该等于观察到事件各自发生时获得的信息之和，即：\\\\(I(x,y)=I(x)+I(y)\\\\)。  \n\n因为两个事件是**独立不相关**的，因此 \\\\(p(x,y)=p(x)p(y)\\\\)。根据这两个关系，很容易看出 \\\\(I(x)\\\\) 一定与 \\\\(p(x)\\\\) 的对数有关 (因为对数的运算法则是 \\\\(\\log_{a}(mn)=\\log_{a}{m}+\\log_{a}{n}\\\\)因此，我们有：\n\n$$I(x)=−\\log {p(x)}$$\n\n其中负号是用来保证信息量是正数或者零。而 \\\\(\\log\\\\) 函数基的选择是任意的（信息论中基常常选择为2，因此信息的单位为**比特bits**；而机器学习中基常常选择为自然常数，因此单位常常被称为**奈特nats**）。\\\\(I(x)\\\\) 也被称为随机变量 \\\\(x\\\\) 的自信息 (self-information)，描述的是**随机变量的某个事件发生所带来的信息量**。图像如图：\n\n![](/img/2018-04-13-entropy-1.jpg)\n\n最后，我们正式引出**信息熵**。 现在假设一个发送者想传送一个随机变量的值给接收者。那么在这个过程中，他们传输的平均信息量可以通过求 \\\\(I(x)=−\\log p(x)\\\\) 关于概率分布 \\\\(p(x)\\\\) 的期望得到，即：\n\n\n\n$$H(X)=-\\sum_{x}p(x)\\log p(x)=-\\sum_{i=1}^{n}p(x_{i})\\log p(x_{i})=-\\int_x p(x)\\log p(x)dx$$\n\n\\\\(H(X)\\\\) 就被称为随机变量 \\\\(x\\\\) 的熵,它是表示**随机变量不确定的度量**，**是对所有可能发生的事件产生的信息量的期望**。      \n从公式可得，随机变量的取值个数越多，状态数也就越多，信息熵就越大，混乱程度就越大。(变量的个数越多，分到的概率越小，概率越小，自信息越大)     \n当随机分布为均匀分布时，熵最大，且 \\\\(0≤H(X)≤logn\\\\)。稍后证明。\n\n\n\n将一维随机变量分布推广到多维随机变量分布，则其**联合熵**(Joint entropy) 为：\n\n\n\n$$H(X,Y)=-\\sum_{x,y} \\log p(x,y) = -\\sum_{i=1}^{n} \\sum_{j=1}^{m} p(x_i,y_i) \\log p(x_i,y_i)$$\n\n\n\n（**注意**：熵只依赖于随机变量的分布,与随机变量取值无关，所以也可以将 \\\\(X\\\\) 的熵记作 \\\\(H(p)\\\\)；令 \\\\(0\\log 0=0\\\\)(因为某个取值概率可能为0)）\n\n\n那么这些定义有着什么样的性质呢？     \n考虑一个随机变量 \\\\(x\\\\)。这个随机变量有4种可能的状态，每个状态都是等可能的。为了把 \\\\(x\\\\) 的值传给接收者，我们需要传输2比特的消息。     \n \n$$H(X)=−4\\times{\\frac {1}{4}}\\times\\log_{2} {\\frac {1}{4}}=2\\ bits$$     \n\n现在考虑一个具有4种可能的状态 \\\\(\\\\{a,b,c,d\\\\}\\\\) 的随机变量，每个状态各自的概率为 \\\\(\\\\{\\frac {1}{2},\\frac {1}{4},\\frac {1}{8},\\frac {1}{8}\\\\}\\\\)。这种情形下的熵为：\n\n\n$$H(X)=-\\frac{1}{2} \\log_2{\\frac{1}{2}}-\\frac{1}{4} \\log_2{\\frac{1}{4}}-\\frac{1}{8} \\log_2{\\frac{1}{8}}-\\frac{1}{8} \\log_2{\\frac{1}{8}}=1.75 \\ bits$$\n\n\n我们可以看到，**非均匀分布比均匀分布的熵要小**。   \n现在让我们考虑如何**把变量状态的类别传递给接收者**。与之前一样，我们可以使用一个2比特的数字来完成这件事情。然而，我们可以利用非均匀分布这个特点，使用**更短的编码来描述更可能的事件**，使用**更长的编码来描述不太可能**的事件。我们希望这样做能够得到一个更短的平均编码长度。我们可以使用下面的编码串（哈夫曼编码）：0、10、110、111来表示状态 \\\\(\\\\{a,b,c,d\\\\}\\\\)。传输的编码的平均长度就是：\n\n\n\n$$average\\ code\\ length= {\\frac {1}{2}}\\times{1} +{\\frac {1}{4}}\\times 2+ 2 \\times {\\frac {1}{8}} \\times 3 = 1.75\\ bits $$\n\n这个值与上方的随机变量的熵相等。**熵和最短编码长度的这种关系是一种普遍的情形**。Shannon 编码定理表明**熵是传输一个随机变量状态值所需的比特位下界**（**最短平均编码长度**）。因此，信息熵可以应用在数据压缩方面。\n\n### 1.1 小结\n\n1. 熵只依赖于随机变量的分布,与随机变量取值无关；\n2. 定义0log0=0(因为可能出现某个取值概率为0的情况)；\n3. 熵越大,随机变量的不确定性就越大,分布越混乱，随机变量状态数越多。\n\n<br />\n### 1.2 证明 0 ≤ H(X) ≤ logn\n证明 \\\\(0≤H(X)≤logn\\\\) 利用拉格朗日乘子法证明：   \n因为 \\\\(p(1)+p(2)+⋯+p(n)=1\\\\) 所以有：  \n   \n**目标函数**：\n\n$$f(p(1),p(2),…,p(n))=\\\\{-(p(1)logp(1)+p(2)logp(2)+⋯+p(n)logp(n))\\\\}$$      \n\n**约束条件**：\n\n$$g(p(1),p(2),…,p(n),λ)=p(1)+p(2)+⋯+p(n)−1=0$$\n\n**定义拉格朗日函数**： \n\n$$L(p(1),p(2),…,p(n),λ)=\\{−(p(1)logp(1)+p(2)logp(2)+⋯+p(n)logp(n))+λ(p(1)+p(2)+⋯+p(n)−1))\\}$$\n     \n函数 \\\\( L(p(1),p(2),…,p(n),λ)\\\\) 分别对 \\\\(p(1),p(2),p(n),λ\\\\) 求偏导数，       \n\n**令偏导数为**： \n<center>\n$$λ−log(e\\times p(1))=0$$     $$λ−log(e\\times p(2))=0$$       $$\\cdots$$        $$λ−log(e\\times p(n))=0$$        $$p(1)+p(2)+⋯+p(n)−1=0$$ \n</center>\n\n求出 \\\\(p(1),p(2),…,p(n)\\\\) 值：      \n解方程得，\\\\(p(1)=p(2)=⋯=p(n)={1 \\over n}\\\\)代入 \\\\(f(p(1),p(2),…,p(n))\\\\) \n\n**得到目标函数的极值为**:\n\n$$f({1 \\over n},{1 \\over n},\\cdots\n,{1 \\over n})=-({1 \\over n}\\log{1 \\over n}+{1 \\over n}\\log{1 \\over n}+\\cdots+{1 \\over n}\\log{1 \\over n})=-\\log{1 \\over n}=\\log n$$\n\n由此可证 \\\\(\\log n\\\\) 为最大值。\n\n## 2 条件熵\n\n>Conditional entropy\n\n\n条件熵 \\\\(H(Y|X)\\\\) 表示在已知随机变量 \\\\(X\\\\) 的条件下随机变量 \\\\(Y\\\\) 的不确定性。条件熵 \\\\(H(Y|X)\\\\) 定义为 \\\\(X\\\\) 给定条件下 \\\\(Y\\\\) 的条件概率分布的熵对 \\\\(X\\\\) 的数学期望：\n\n<center>\n$$H(Y|X) ={ \\sum_{x} p(x)H(Y|X=x)}$$ $$={-\\sum_{x}p(x) \\sum_{y} p(y|x) \\log p(y|x)}$$ $$={-\\sum_{x} \\sum_{y} p(x,y) \\log p(y|x)}$$ $$={-\\sum_{x,y}p(x,y) \\log p(y|x)}$$ $$={-\\sum_{x,y}p(x,y) \\log p(y|x)}$$\n</center>\n\n条件熵 \\\\(H(Y\\mid X)\\\\) 相当于联合熵 \\\\(H(X,Y)\\\\) 减去单独的熵 \\\\(H(X)\\\\) ，即 \\\\(H(Y\\mid X)=H(X,Y)−H(X)\\\\) ，证明如下： \n\n<center style=\"text-align:left\">\n$$H(X,Y) ={-\\sum_{x,y} p(x,y)\\log p(x,y)}$$ $$={-\\sum_{x,y} p(x,y) \\log (p(y|x)p(x))}$$ $$={-\\sum_{x,y} p(x,y) \\log p(y|x)-\\sum_{x,y}p(x,y) \\log p(x)}$$ $$={H(Y|X)-\\sum_{x,y} p(x,y) \\log p(x)}$$ $$= {H(Y|X)-\\sum_{x} \\sum_{y} p(x,y) \\log p(x)}$$ $$={H(Y|X)-\\sum_{x} \\log p(x) \\sum_{y} p(x,y)}$$ $$={H(Y|X)-\\sum_{x} \\log p(x) p(x)}$$ $$= {H(Y|X)-\\sum_{x} p(x)\\log p(x)}$$ $$={H(Y|X)+H(X)}$$\n</center>\n\n同理可得： \\\\(H(X,Y)=H(X\\mid Y)+H(Y)\\\\) 即 \\\\(H(X\\mid Y)=H(X,Y)-H(Y)\\\\) \n\n>举个例子，比如环境温度是低还是高，和我穿短袖还是外套这两个事件可以组成联合概率分布 \\\\(H(X,Y)\\\\) ，因为两个事件加起来的信息量肯定是大于单一事件的信息量的。假设 \\\\(H(X)\\\\) 对应着今天环境温度的信息量，由于今天环境温度和今天我穿什么衣服这两个事件并不是独立分布的，所以在已知今天环境温度的情况下，我穿什么衣服的信息量或者说不确定性是被减少了。当已知 \\\\(H(X)\\\\) 这个信息量的时候， \\\\(H(X,Y)\\\\) 剩下的信息量就是条件熵： \\\\(H(Y\\mid X)=H(X,Y)−H(X)\\\\) \n\n因此，可以这样理解，描述 \\\\(X\\\\) 和 \\\\(Y\\\\) 所需的信息是描述 \\\\(X\\\\) 自己所需的信息,加上给定 \\\\(X\\\\) 的条件下具体化 \\\\(Y\\\\) 所需的额外信息。关于条件熵的例子可以看这篇文章，讲得很详细。[(链接)](https://zhuanlan.zhihu.com/p/26551798)\n\n\n## 3 相对熵 / KL散度\n\n>相对熵 (Relative entropy) / KL散度 (Kullback–Leibler divergence)\n\n设 \\\\(p(x),q(x)\\\\) 是 离散随机变量 \\\\(X\\\\) 中取值的两个概率分布，则 \\\\(p\\\\) 对 \\\\(q\\\\) 的相对熵是：\n\n$$D_{KL}(p||q)=\\sum_{x}p(x)\\log {\\frac {p(x)}{q(x)}}=\\sum_{x}p(x)(\\log p(x)-\\log q(x))=E_{p(x)}\\log {\\frac {p(x)}{q(x)}}$$\n\n\n在一定程度上，熵可以度量两个随机变量的距离。\n\n可是熵值并没有给出压缩数据到最小熵值的方法，即如何编码数据才能达到最优（存储空间最优）。     \n熵的主要作用是告诉我们最优编码信息方案的理论下界（存储空间），以及度量数据的信息量的一种方式。     \n理解了熵，我们就知道有多少信息蕴含在数据之中，现在我们就可以计算当我们**用一个带参数的概率分布来近似替代原始数据分布的时候，到底损失了多少信息**。\n\n**K-L散度度量信息损失：**\n\n$$\\sum_{x}p(x)(\\log p(x)-\\log q(x))$$\n\n典型情况下，P表示数据的真实分布，Q表示数据的理论分布，模型分布，或P的近似分布。\n\n### 3.1 性质\n\n- 如果 \\\\(p(x)\\\\) 和 \\\\(q(x)\\\\) 两个分布相同，那么相对熵等于0\n- **非对称性：** \\\\(D_{KL} (p\\|\\|q) ≠ D_{KL}(q\\|\\|p)\\\\)，相对熵具有不对称性。大家可以举个简单例子算一下。\n- **非负性：**\\\\(D_{KL}(p\\|\\|q)≥0\\\\)\n\n### 3.2 应用\n\n相对熵是比较两个概率分布的距离（相似度），用一个分布来近似另一个分布时计算信息损失量，因此可以用于文本相似度的计算；还可以用于权重指标的分配。\n\n### 3.3 总结     \n\n相对熵可以用来衡量两个概率分布之间的差异，上面公式的意义就是求 \\\\(p\\\\) 与 \\\\(q\\\\) 之间的对数差在 \\\\(p\\\\) 上的期望值。\n\n**优秀文章：**\n\n1. [如何理解K-L散度（相对熵）](https://www.jianshu.com/p/43318a3dc715?from=timeline&isappinstalled=0)\n2. [如何通俗的解释交叉熵与相对熵?](https://www.zhihu.com/question/41252833)\n\n## 4 交叉熵\n\n> Cross entropy\n\n\n\n\n\n\n\n\n\n## 参考：\n\n1. https://blog.csdn.net/hsj1213522415/article/details/56673389\n2. https://www.jianshu.com/p/43318a3dc715?from=timeline&isappinstalled=0\n3. https://zhuanlan.zhihu.com/p/35379531\n4. https://zhuanlan.zhihu.com/p/35423404\n\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n\n\n\n\n\n","tags":["机器学习"],"categories":["机器学习"]},{"title":"Fast R-CNN 目标检测（三）","url":"/2018/04/12/FastRCNN/","content":"\n\n## Fast R-CNN简介\n在之前的两个文章中，我们分别介绍了[R-CNN](http://mp.weixin.qq.com/s?__biz=MzUyMjE2MTE0Mw==&mid=2247484206&idx=1&sn=f9084165a4673affd8e23ac97f707eb8&chksm=f9d15db6cea6d4a04a777d45ae3e3f1a3ef6f657352c98db9664084fe4b4f802273dde7ac943&scene=21#wechat_redirect)与SPP-Net，于是在2015年RBG（Ross B. Girshick）等结合了SPP-Net的共享卷积计算思想，对R-CNN做出改进，于是就有了Fast R-CNN。首先简单介绍下Fast R-CNN。   \n\n\n它由以下几个部分组成：                               \n\n1. ss算法                                                 \n2. CNN网络                                           \n3. SoftMax                                              \n4. bounding box\n\n<!-- more -->\n\n训练过程如下：\n\n1. 用selective search在一张图片中生成约2000个object proposal，即感兴趣区域RoI。\n2. 把它们整体输入到全卷积的网络中，用几个卷积层（conv）和最大池化层处理整个图像以产生conv特征图，在最后一个卷积层上对每个ROI求映射关系，并用一个RoI pooling layer来统一到相同的大小，得到特征向量 feature vector。\n3. 每个特征向量被输送到分支成两个同级输出层的全连接（fc）层序列中：其中一层进行分类，对目标关于K个对象类（包括全部背景background类）产生softmax概率估计，即输出每一个RoI的概率分布；另一层进行bbox regression，输出K个对象类中每一个类的四个实数值。每4个值编码K个类中的每个类的精确边界盒（bounding-box）位置，即输出每一个种类的的边界盒回归偏差。整个结构是使用多任务损失的端到端训练（trained end-to-end with a multi-task loss）。\n\n![](/img/2018-04-12-FastRCNN-1.jpg)\n<center>Fast R-CNN结构</center>\n\n<br />\n\n\n图像归一化为224×224直接送入网络。\n前五阶段是基础的conv+relu+pooling形式，在第五阶段结尾，输入P个候选区域（图像序号×1+几何位置×4，序号用于训练）。\n\n![](/img/2018-04-12-FastRCNN-2.jpg)\n\n<br />\n\n相比于之前两种算法，Fast R-CNN提出了： \n\n- 多任务损失函数（Multi-task loss）     \n- 感兴趣区域池化（RoI pooling layer）\n\n首先在SPP-Net与R-CCN一直使用的SVM分类器被换成了**SoftMax**，SPP-Net中的SPP换成了**RoI pooling (Region of Interest)**，多任务损失函数的引入整合了分类网络的损失函数与bounding box回归模型的损失函数，使任务不需要分阶段训练，区域建议依然使用ss算法生成，并在卷积后的特征图上提取（充分共享卷积计算），初始模型从AlexNet换成了**VGG16**。 \n\n\n\n\n\n## 兴趣区域池化 RoI\nroi_pool层将每个候选区域均匀分成M×N块，对每块进行max pooling。将特征图上大小不一的候选区域转变为大小统一的数据，送入下一层。\n\n![](/img/2018-04-12-FastRCNN-3.jpg)\n\n<br />\n\n![](/img/2018-04-12-FastRCNN-4.jpg)\n\n上面这张图说明了SPP与RoI pooling的区别，其实RoI pooling是SPP的一种简化，原本SPP是一种多尺度的池化操作，最后将三个尺度的特征做串接作为全连接层的输入，而RoI pooling只选择了其中一种尺度，将ss算法的建议框做坐标变化后的尺寸的长和宽，平均分为w份和h份，在每一份中使用最大池化，最后产生 \\\\(w \\times h\\\\) 个bin，这样做有下面几个好处： \n\n1. 统一输出维度，这个是必须的。 \n2. 相比于SPP-Net，RoI pooling的维度更少，假设RoI pooling选择了 \\\\(4\\times4\\\\) 的话，那么维度就可以从21个bin降低为16个，虽然这样看来降低的并不多，但是不要忘了特征还有厚度，如果厚度是256的话，那么降维就比较可观了。 \n3. RoI pooling不再是多尺度的池化，这样一来梯度回传就会更方便，有利于Fast R-CNN实现end-to-end的训练。\n\n\n## 兴趣区域池化的梯度回传\n\n在上说提到了，RoI pooling是单层的SPP，也就是只用一层金字塔并在区域内做Max pooling，所以如何说在卷积层上提取特征的时候，特征的位置没有出现重叠，RoI pooling就是一个Max pooling，梯度回传也是一样的，而出现位置重叠的时候，梯度回传才会发生变化。 \n\n那么先解释一下什么是重叠：    \n我们知道Fast R-CNN的区域建议同样是ss算法生成的，那么一幅图片在生成多个建议框时（假设是2个）可能会出现一些像素重叠的情况，就像下面这样：\n\n![](/img/2018-04-12-FastRCNN-5.jpg)\n\n而这种情况就没有重叠：\n\n![](/img/2018-04-12-FastRCNN-6.jpg)\n\n\n显然，重叠的区域经过相同的坐标变换之后在卷积特征图上同样是有重叠的，那么这部分重叠的像素梯度应该如何让计算呢？     \n是多个区域的偏导之和： \n\n![](/img/2018-04-12-FastRCNN-7.jpg)\n\n上图中有\\\\(r_{0}\\\\)与\\\\(r_1\\\\)两个区域，每个区域都通过RoI pooling之后生成4个bin，x23的意思是第23个像素，那么计算x23位置的梯度就可以根据上图中左侧的公式，其中r是包含有这一点的区域，j是某个区域内的所有位置。 \n但是x23的梯度计算显然不需要r0，r1内的所有位置的梯度信息，它只需要包含x23这一点的，或者说是x23这一点有贡献的点的梯度，所以这里需要一个阈值函数 \\\\(i\\*(r,j)\\\\)，它的作用就是如果需要RoI pooling后的这一点的梯度，那么 \\\\(i\\*(r,j)=1\\\\)，否则 \\\\(i*(r,j)=0\\\\)。\n\n\n\n这样一来，RoI pooling层的梯度回传只需要在Max pooling上简单修改即可。\n\n\n\n## 多任务损失函数\n\nMulti-task loss是Fast R-CNN最重要的改进了，它将分类模型的损失函数与bounding box模型的损失函数加到了一起，这样一来就不再需要分阶段的训练了，而是实现了end-to-end。\n\n\n**分类模型的loss：**\n\n$$L_{cls}(p,u) = -\\log{p_{u}}$$\n\n其中p是每个RoI的概率分布：\\\\(p = (p_{0},{...},p_{K})\\\\)\n\n而\\\\(u\\\\)是Ground truth的类别，显然u的范围为\\\\((0,…,k)\\\\) \n显然，这就是损失函数一个交叉熵，只是它简写了，或者说换了一种形式。\n\n**Bounding box回归模型的loss：**    \n\n这个loss和R-CNN中的Bounding box的loss没啥区别，都是在用实际的边界框信息与ss算法给出的边界框信息构建一个L1距离。如下： \n\n$$L_{loc}(t^{u},v)=\\sum_{i\\in \\{x,y,w,h\\}} smooth_{L_{1}}(t_i^u - v_i)$$\n\n其中平滑方程的具体形式如下：\n\n$$\nsmooth_{L_{1}}(x) =\n\\begin{cases}\n0.5x^2, & \\text{if |x| < 1}  \\\\\n|x|-0.5, & \\text{otherwise}\n\\end{cases}\n$$\n\n\n而平滑方程里面的东西，就和R-CNN一样了。最后，组合的多任务损失函数为：\n\n$$L(p,u,t^u,v) = L_{cls}(p,u) + \\lambda [u \\ge 1]L_{loc}(t^u,v)$$\n\n其中 \\\\([u \\ge 1]\\\\) 是一个指示函数，作用就是背景类不需要bounding box修正，也就没有回归loss。相当于：\n\n$$L=\n\\begin{cases}\nL_{cls}+\\lambda L_{loc}, & \\text{u 为前景}  \\\\\\\\\nL_{cls}, & \\text{u 为背景}\n\\end{cases}\n$$\n\n第五阶段的特征输入到两个并行的全连层中（称为multi-task）。\n\n![](/img/2018-04-12-FastRCNN-8.jpg)\n\n**cls_score层**用于分类，输出 \\\\(K+1\\\\) 维数组\\\\(p\\\\)，表示属于K类和背景的概率。 \n\n**bbox_prdict层**用于调整候选区域位置，输出 \\\\(4\\times K\\\\)维数组\\\\(t\\\\)，表示分别属于K类时，应该平移缩放的参数。\n\n\n\n\n## Fast R-CNN训练与测试\n### 参数初始化\n网络除去末尾部分如下图，在ImageNet上训练1000类分类器。结果参数作为相应层的初始化参数。 \n\n![](/img/2018-04-12-FastRCNN-9.jpg)\n\n其余参数随机初始化。\n\n\n### 分层数据\n\n在调优训练时，每一个mini-batch中首先加入N张完整图片，而后加入从N张图片中选取的R个候选框。这R个候选框可以复用N张图片前5个阶段的网络特征。 \n实际选择N=2， R=128。\n\n### 训练数据构成\n\nN张完整图片以50%概率水平翻转。 \nR个候选框的构成方式如下：\n\n| 类别        | 比例    |  方式  | \n| --------   | -----:   | :----: |\n| 前景        | 25%      |   与某个真值重叠在[0.5,1]的候选框    |\n| 背景        | 75%      |   与真值重叠的最大值在[0.1,0.5)的候选框    |\n\n\n### 训练与测试\n![](/img/2018-04-12-FastRCNN-10.jpg)\n\n上面这张图解释了Fast R-CNN的训练与测试过程，前面两部分说明了RoI pooling层的梯度回传与多任务损失函数的构建，所以Fast R-CNN的梯度可以一直传到卷积层，实现end-to-end的训练。 \n此外，为了在训练事得到更好的效果，作者提出了一种分级抽样法，如果batch-size为128的话，那么这128个RoI由2张图片，各生产64个区域。 \n\n![](/img/2018-04-12-FastRCNN-11.jpg)\n\n而Fast R-CNN的测试过程和之前没什么区别。\n\n## Fast R-CNN性能评价\n\n![](/img/2018-04-12-FastRCNN-12.jpg)\n\n上面这张图对比了R-CNN，SPP-Net与Fast R-CNN的训练时间，单张图片的测试时间与mAP，可以看到由于Fast R-CNN可以end-to-end的训练，它的mAP比R-CNN还要高一些，这样就不会出现像SPP-Net那样mAP降低的情况，而在训练时间与测试时间上，又一次有了较大进步。 \n那么为什么Fast R-CNN比SPP-Net更快呢，最重要的原因就是end-to-end的训练，这样训练不再是分阶段的。\n\n\n\n## Fast R-CNN的问题\n\n虽然上面那张图上写的，Fast R-CNN的单图测试时间为0.32s，但是其实这样说并不准确，0.32为了和R-CNN的47.0s做对比。是的Fast R-CNN依然没有脱离ss算法，但是ss算法跑一张图的时间，大概是2s，所以讲道理的话，Fast R-CNN依然是达不到实时检测的要求的，好在ss算法在Faster R-CNN中被换成RPN（区域建议网络），这个我们后面再说。\n\n<br />\n\n\n## 转载及参考\n1. [Object Detection系列（三） Fast R-CNN](https://mp.weixin.qq.com/s?__biz=MzUyMjE2MTE0Mw==&mid=2247485064&idx=2&sn=5a99a25c8f3b6c70d440df1225923948&chksm=f9d15810cea6d106afb9efef986e9ced94e7729e3e82840b87d8bcef248a83873f89fc34f478&scene=21#wechat_redirect)\n\n2. [【目标检测】Fast RCNN算法详解](https://blog.csdn.net/shenxiaolu1984/article/details/51036677)\n\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>","tags":["深度学习","目标检测"],"categories":["计算机视觉"]},{"title":"MathJax 使用指南","url":"/2018/04/11/MathJax/","content":"\n## 1 使用Google Chart的服务器\n`<img src=\"http://chart.googleapis.com/chart?cht=tx&chl= 在此插入Latex公式\" style=\"border:none;\">`     \n例子：\n`<img src=\"http://chart.googleapis.com/chart?cht=tx&chl=\\Large x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\" style=\"border:none;\">\n`      \n显示结果:\n\n<img src=\"http://chart.googleapis.com/chart?cht=tx&chl=\\Large x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\" style=\"border:none;\">\n\n<!-- more -->\n\n## 2 使用MathJax引擎\n\n在Markdown中添加MathJax引擎也很简单，\n    \n```\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n```\n\n然后，再使用Tex写公式。 \n  \n- `$$公式$$` 表示行间公式     \n- 本来Tex中使用 `\\(公式\\)` 表示行内公式，但因为Markdown中 `\\` 是转义字符，所以在Markdown中输入行内公式使用 `\\\\(公式\\\\)`\n\n如下代码：\n    \n```\n$$x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}$$      # 行间公式\n\n\\\\(x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\\\\)    # 行内公式\n$h(x) = \\theta_0 + \\theta_1 x$            # 行内公式\n```\n分别显示结果（行间公式）：\n\n$$x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}$$ \n\n行内公式：\n\\\\(x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\\\\)\n\n\n\n\n## 3 mathjax 常用符号\n\n#### 绝对值符号 \n\n`\\vert x \\vert`: \\\\(\\vert x \\vert\\\\) \n\n#### 方程组\n\n```\n$$\\begin{cases}\na_1x+b_1y+c_1z=d_1\\\\\na_2x+b_2y+c_2z=d_2\\\\\na_3x+b_3y+c_3z=d_3\\\\\n\\end{cases}\n$$\n```\n\n$$\\\\begin{cases}\na\\_1x+b\\_1y+c\\_1z=d\\_1\\\\\\\\\na\\_2x+b\\_2y+c\\_2z=d\\_2\\\\\\\\\na\\_3x+b\\_3y+c\\_3z=d\\_3\\\\\\\\\n\\\\end{cases}\n$$\n\n#### 特殊符号\n\n\n|id|name|\n|:-|:-|\n| \\\\(\\infty\\\\) |\\\\infty|\n| \\\\(\\cup\\\\) |\\\\cup|\n| \\\\(\\cap\\\\) |\\\\cap|\n| \\\\(\\subset\\\\) |\\\\subset|\n| \\\\(\t\\subseteq\\\\) |\\\\subseteq|\n| \\\\(\\in\\\\) | \\\\in |\n| \\\\(\\varnothing\\\\) |\\\\varnothing|\n|3|A3|\n|3|A3|\n|3|A3|\n\n\n## 4 参考：\n\n1. [Markdown中插入数学公式的方法](https://blog.csdn.net/xiahouzuoxin/article/details/26478179)\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n","tags":["mathjax"],"categories":["论文"]},{"title":"综述|基于深度学习的目标检测","url":"/2018/04/11/ObjectDetection/","content":"\n\n## 概述\n**图像分类**，**检测**及**分割**是计算机视觉领域的三大任务。   \n**图像分类模型**是将图像划分为单个类别，通常对应于图像中最突出的物体。   \n但是现实世界的很多图片通常包含不只一个物体，此时如果使用图像分类模型为图像分配一个单一标签其实是非常粗糙的，并不准确。    \n对于这样的情况，就需要**目标检测模型**，目标检测模型可以<u>识别一张图片的多个物体，并可以定位出不同物体（给出边界框）</u>。目标检测在很多场景有用，如无人驾驶和安防系统。\n\n![](/img/2018-04-11-ObjectDetection-1.jpg)\n\n目前主流的目标检测算法主要是基于深度学习模型，其可以分成两大类：\n\n1. **two-stage检测算法**，其将检测问题划分为两个阶段，\n\t\n\t首先产生**候选区域（region proposals）**   \n\t然后**对候选区域分类**（一般还需要对位置精修） \n\t \n\t这类算法的典型代表是基于 region proposal 的 R-CNN 系算法，如 R-CNN，Fast R-CNN，Faster R-CNN 等；\n2. **one-stage检测算法**，其不需要 region proposal 阶段，直接产生物体的类别概率和位置坐标值，比较典型的算法如YOLO和SSD。\n\n<!-- more -->\n\n目标检测模型的主要性能指标是**检测准确度**和**速度**，对于准确度，目标检测要考虑物体的定位准确性，而不单单是分类准确度。    \n一般情况下，**two-stage**算法在**准确度**上有优势，而**one-stage**算法在**速度**上有优势。不过，随着研究的发展，两类算法都在两个方面做改进。  \n\nGoogle在2017年开源了<u>TensorFlow Object Detection API</u>，并对主流的 **Faster R-CNN**，**R-FCN** 及 **SSD** 三个算法在MS COCO数据集上的性能做了细致对比（见Huang et al. 2017），如下图所示：\n\n![](/img/2018-04-11-ObjectDetection-2.jpg)\n\n![](/img/2018-04-11-ObjectDetection-3.jpg)\n\nFacebook的FAIR也开源了基于Caffe2的目标检测平台[Detectron](https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/Detectron)，其实现了最新的 Mask R-CNN，RetinaNet 等检测算法，并且给出了这些算法的[Baseline Results](https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/Detectron/blob/master/MODEL_ZOO.md) 。   \n\n**不得不说，准确度（accuracy）和速度（speed）是一对矛盾体，如何更好地平衡它们一直是目标检测算法研究的一个重要方向。**\n\n\n在这篇长文中，我们将对最新的目标检测算法做一个综述。在介绍目标检测算法之前，先简单介绍目标检测领域常用的数据集以及性能指标。\n\n## 数据集和性能指标\n\n\n目标检测常用的数据集包括 **PASCAL VOC**，**ImageNet**，**MS COCO** 等数据集，这些数据集用于研究者测试算法性能或者用于竞赛。目标检测的性能指标要考虑检测物体的位置以及预测类别的准确性，下面我们会说到一些常用的性能评估指标。\n\n### 数据集\n\n**PASCAL VOC**：（[The PASCAL Visual Object Classification](https://link.zhihu.com/?target=http%3A//host.robots.ox.ac.uk/pascal/VOC/)）是目标检测，分类，分割等领域一个有名的数据集。从2005到2012年，共举办了8个不同的挑战赛。PASCAL VOC包含约10,000张带有边界框的图片用于训练和验证。但是，PASCAL VOC数据集仅包含<u>20个类别，因此其被看成目标检测问题的一个基准数据集。</u>\n\n**ImageNet**：[ImageNet](https://link.zhihu.com/?target=http%3A//www.image-net.org/)在2013年放出了包含边界框的目标检测数据集。训练数据集包含500,000张图片，属于200类物体。<u>由于数据集太大，训练所需计算量很大，因而很少使用。同时，由于类别数也比较多，目标检测的难度也相当大。</u>2014 ImageNet数据集和2012 PASCAL VOC数据集的对比在[这里](https://link.zhihu.com/?target=http%3A//image-net.org/challenges/LSVRC/2014/)。\n\n**MS COCO**：另外一个有名的数据集是Microsoft公司（见[T.-Y.Lin and al. 2015](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1405.0312.pdf))建立的MS COCO（[Common Objects in COntext](https://link.zhihu.com/?target=http%3A//cocodataset.org/%23home)）数据集。这个数据集<u>用于多种竞赛：图像标题生成，目标检测，关键点检测和物体分割。</u>对于目标检测任务，COCO共包含80个类别，每年大赛的训练和验证数据集包含超过120,000个图片，超过40,000个测试图片。测试集最近被划分为两类，一类是 test-dev 数据集用于研究者，一类是 test-challenge 数据集用于竞赛者。测试集的标签数据没有公开，以避免在测试集上过拟合。在[COCO 2017 Detection Challenge](https://link.zhihu.com/?target=http%3A//cocodataset.org/%23detections-challenge2017)中，旷视科技团队凭借提出的[Light-Head R-CNN](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1711.07264.pdf)模型夺得冠军（AP为0.526 ），看来还是two-stage算法准确度更胜一筹。\n\n\n\n\n<div align=center>\n![](/img/2018-04-11-ObjectDetection-4.jpg)\n<center><small><font color=gray> 2015 COCO数据集的分割实例. 来源: T.-Y.Lin and al. (2015)   </font></small></center>\n</div>\n\n\n<div align=center>\n![](/img/2018-04-11-ObjectDetection-5.jpg)\n<center><small><font color=gray>  目标检测的主流数据集. 来源: https://tryolabs.com/blog/  </font></small></center>\n</div>\n\n\n<br /> \n\n### 性能指标\n\n目标检测问题**同时是一个回归和分类问题**。首先，为了评估定位精度，需要计算 `IoU (Intersection over Union)`，介于0到1之间，其表示**预测框与真实框（ground-truth box）之间的重叠程度**。IoU越高，预测框的位置越准确。因而，在评估预测框时，通常会设置一个**IoU阈值**（如0.5），只有当预测框与真实框的IoU值大于这个阈值时，该预测框才被认定为真阳性`(True Positive, TP)`，反之就是假阳性`(False Positive，FP)`。\n\n对于二分类，`AP (Average Precision)`是一个重要的指标，这是**信息检索**中的一个概念，基于precision-recall曲线计算出来，详情见[这里](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/w/index.php%3Ftitle%3DInformation_retrieval%26oldid%3D793358396%23Average_precision)。对于目标检测，首先要单独**计算各个类别的AP值**，这是评估检测效果的重要指标。取**各个类别的AP的平均值**，就得到一个综合指标 `mAP (Mean Average Precision)`，<u>mAP指标可以避免某些类别比较极端化而弱化其它类别的性能这个问题。</u>\n\n对于目标检测，mAP一般在某个固定的IoU上计算，但是不同的IoU值会改变TP和FP的比例，从而造成mAP的差异。COCO数据集提供了[官方的评估指标](https://link.zhihu.com/?target=https%3A//github.com/cocodataset/cocoapi)，它的AP是计算一系列IoU下（0.5:0.05:0.9，见[说明](https://link.zhihu.com/?target=http%3A//cocodataset.org/%23detection-eval)）AP的平均值，这样可以消除IoU导致的AP波动。其实对于PASCAL VOC数据集也是这样，Facebook的Detectron上的有比较清晰的[实现](https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/Detectron/blob/05d04d3a024f0991339de45872d02f2f50669b3d/lib/datasets/voc_eval.py%23L54)。\n\n除了检测准确度，目标检测算法的另外一个重要性能指标是**速度**，只有速度快，才能实现实时检测，这对一些应用场景极其重要。评估速度的常用指标是**每秒帧率** `(Frame Per Second，FPS)`，**即每秒内可以处理的图片数量**。当然要对比FPS，你需要在同一硬件上进行。另外也可以使用处理一张图片所需时间来评估检测速度，时间越短，速度越快。\n\n## 检测算法\n\n![](/img/2018-04-11-ObjectDetection-6.jpg)\n\n### R-CNN\n\n\nR-CNN（[R. Girshick et al., 2014](https://link.zhihu.com/?target=http%3A//islab.ulsan.ac.kr/files/announcement/513/rcnn_pami.pdf)）是基于region proposal方法的目标检测算法系列开山之作，其先进行区域搜索，然后再对候选区域进行分类。在R-CNN中，选用 Selective search 方法（[J.R.R. Uijlings and al. 2012](https://link.zhihu.com/?target=http%3A//www.huppelen.nl/publications/selectiveSearchDraft.pdf)）来生成候选区域，这是一种启发式搜索算法。它先通过**简单的区域划分**算法将**图片划分成很多小区域**，然后通过**层级分组**方法**按照一定相似度合并它们**，最后的**剩下的就是候选区域** `region proposals`，它们可能包含一个物体。\n\n![](/img/2018-04-11-ObjectDetection-7.jpg)   \n<center>Selective Search方法：上面是分割结果，下面是候选框. 来源: J.R.R. Uijlings and al. (2012)</center>\n<br /> \n\n对于一张图片，R-CNN基于selective search方法大约生成2000个候选区域，然后每个候选区域被 **resize** 成固定大小（ \\\\(227\\times227\\\\) ）并**送入一个CNN模型**中，最后得到一个**4096-d的特征向量**。然后这个特征向量被送入一个**多类别SVM分类器**中，预测出候选区域中所含物体的属于每个类的概率值。**每个类别训练一个SVM分类器**，从特征向量中推断其属于该类别的概率大小。为了提升定位准确性，R-CNN最后又训练了一个**边界框回归模型**。训练样本为（\\\\(P,G\\\\)），其中 \\\\(P=\\left(P_{x},P_{y},P_{w},P_{h}\\right)\\\\) 为候选区域，而 \\\\(G=\\left(G_{x},G_{y},G_{w},G_{h}\\right)\\\\) 为真实框，\\\\(G\\\\) 是与 \\\\(P\\\\) 的IoU最大的真实框（只使用IoU大于0.6的样本），回归器的目标值定义为：\n\n$$t_{x} = (G_{x} - P_x) / P_w, t_y = (G_y - P_y) / P_h$$\n\n$$t_w = \\log(\\frac{G_w}{P_w}), t_h = \\log(\\frac{G_h}{P_h})$$\n\n在做预测时，利用上述公式可以**反求出预测框的修正位置**。R-CNN**对每个类别都训练了单独的回归器**，采用**最小均方差损失函数**进行训练。\n\n\nR-CNN模型的训练是**多管道**的，CNN模型首先使用 2012 ImageNet 中的图像分类竞赛数据集进行预训练。然后在检测数据集上对CNN模型进行 finetuning（微调），其中那些与真实框的IoU大于0.5的候选区域作为**正样本**，剩余的候选区域是**负样本**（背景）。共训练两个版本，第一版本使用 2012 PASCAL VOC 数据集，第二个版本使用 2013 ImageNet 中的目标检测数据集。最后，**对数据集中的各个类别训练SVM分类器**（注意SVM训练样本与CNN模型的funetuning不太一样，只有IoU小于0.3的才被看成负样本）。\n\n总体来看，R-CNN是非常直观的，就是把检测问题转化为了分类问题，并且采用了CNN模型进行分类，但是效果却很好。最好的R-CNN模型在2012 PASCAL VOC数据集的mAP为62.4%（比第二名高出了22个百分点），在2013 ImageNet上的mAP为31.4%（比第二名高出7.1个百分点）。\n\n\n\n<div align=center>\n![](/img/2018-04-11-ObjectDetection-8.jpg)\n<center><small><font color=gray> R-CNN模型结构图   </font></small></center>\n</div>\n\n<br /> \n\n### SPP-net\n\nSPP-net（[Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition, He et al. 2014](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1406.4729.pdf)）提出的起因是**解决图像分类中要求输入图片固定大小的问题**，但是SPP-net中所提出的**空间金字塔池化层** `Spatial Pyramid Pooling Layer, SPP` 可以和R-CNN结合在一起并提升其性能。采用深度学习模型解决图像分类问题时，往往需要图像的大小固定（比如 \\\\(224\\times224\\\\)），这并不是CNN层的硬性要求，主要原因在于CNN层提取的特征图最后要送入全连接层（如softmax层），对于变大小图片，CNN层得到的特征图大小也是变化的，但是全连接层需要固定大小的输入，所以必须要将图片通过 **resize**, **crop(剪切)** 或 **wrap(扭曲)** 等方式固定大小（训练和测试时都需要）。但是实际上真实的图片的大小是各种各样的，一旦固定大小可能会造成图像损失，从而影响识别精度。为了解决这个问题，SSP-net在**CNN层与全连接层之间插入了空间金字塔池化层**来解决这个矛盾。\n\n![](/img/2018-04-11-ObjectDetection-9.jpg)\n<center>SPP-net与普通网络的结构对比</center>\n<br /> \n\nSPP 层原理如下所所示，假定 CNN 层得到的特征图大小为 \\\\(a\\times{a}\\\\)（比如 \\\\(13\\times{13}\\\\)，随输入图片大小而变化），设定的金字塔尺度为 \\\\(n\\times{n}\\\\) bins（对于不同大小图片是固定的），那么 SPP 层采用一种滑动窗口池化，窗口大小 \\\\(win\\\\_size = [a / n]\\\\) ，步为 \\\\(stride = [a/n]\\\\) ，采用 max pooling，本质上将特征图均分为 \\\\(n\\times{n}\\\\) 个子区域，然后对各个子区域 max pooling，这样不论输入图片大小，经过SPP层之后得到是固定大小的特征。**一般设置多个金字塔级别**，文中使用了 \\\\(4\\times{4}\\\\), \\\\(2\\times{2}\\\\) 和 \\\\(1\\times{1}\\\\) 三个尺度。每个金字塔都得一个特征，**将它们连接在一起送入后面的全连接层即可**，这样就解决了变大小图片输入的问题了。SPP-net 在 ImageNet ILSVRC 2014 图像分类大赛中夺得了第三名。\n\n![](/img/2018-04-11-ObjectDetection-10.jpg)\n<center>SPP-net中的空间金字塔池化层</center>\n<br /> \n\n那么 SPP-net 和 R-CNN 有什么关系呢？在 R-CNN 中，由于每个候选区域大小是不同，所以需要先 resize 成固定大小才能送入 CNN 网络，SPP-net 正好可以解决这个问题。继续上前一步，就是 R-CNN 每次都要挨个使用CNN模型计算各个候选区域的特征，这是极其费时的，**不如直接将整张图片送入CNN网络，然后抽取候选区域的对应的特征区域**，采用 SPP 层，这样可以大大减少计算量，并提升速度。基于 SPP 层的 R-CNN 模型在准确度上提升不是很大，但是速度却比原始 R-CNN 模型快24-102倍。这也正是接下来 Fast R-CNN 所改进的方向。\n\n![](/img/2018-04-11-ObjectDetection-11.jpg)\n<center>SPP层用于R-CNN模型</center>\n<br /> \n\n### Fast R-CNN\n\nFast R-CNN（[Fast Region-based Convolutional Network, R. Girshick 2015](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1504.08083.pdf)）的提出主要是为了减少候选区域使用 CNN 模型提取特征向量所消耗的时间，其主要借鉴了SPP-net的思想。在 R-CNN 中，每个候选区域都要单独送入 CNN 模型计算特征向量，这是非常费时的，而对于 Fast R-CNN，其 CNN 模型的输入是整张图片，然后结合 `RoIs（Region of Interests）pooling` 和 `Selective Search` 方法从 CNN 得到的特征图中提取各个候选区域的所对应的特征。对于每个候选区域，使用 `RoI pooling` 层来从 CNN 特征图中得到一个固定长和宽的特征图（长和宽是超参数，文中选用 \\\\(7\\times7\\\\) ），RoI pooling 的原理很简单，其根据候选区域按比例从 CNN 特征图中找到对应的特征区域，然后将其分割成几个子区域（根据要输出的特征图的大小），然后在每个子区域应用 max pooling，从而得到固定大小的特征图，这个过程是可导的（见RoI pooling层的[Caffe官方实现](https://link.zhihu.com/?target=https%3A//github.com/rbgirshick/caffe-fast-rcnn/blob/bcd9b4eadc7d8fbc433aeefd564e82ec63aaf69c/src/caffe/layers/roi_pooling_layer.cpp)，RoI pooling层在大部分深度学习框架中是没有官方实现的，需要自己扩展，Tensorflow上的开源实现可以参考[deepsense-ai/roi-pooling](https://link.zhihu.com/?target=https%3A//github.com/deepsense-ai/roi-pooling)，但是在TensorFlow中可以基于一种 crop+resize+pooling 的方式来简化实现，可以参考一下[Luminoth上的实现](https://link.zhihu.com/?target=https%3A//github.com/tryolabs/luminoth/blob/master/luminoth/models/fasterrcnn/roi_pool.py)），RoI pooling 层相比 SPP 层看起来主要是只使用一个金字塔级别。然后 RoI pooling 层得到的特征图送入几个全连接层中，并产生新的特征向量，这些特征向量分别用于一个 **softmax 分类器**（**预测类别**）和一个**线性回归器**上（**用于调整边界框位置**）来进行检测。在实现上是使用两个不同的全连接层，第一个全连接层有 \\\\(N + 1\\\\) 个输出（是 \\\\(N\\\\) 类别总数， \\\\(1\\\\) 是背景），表示各个类别的概率值；第二个全连接层有 \\\\(4N\\\\) 个输出，表示坐标回归值 \\\\((t_{x},t_{y},t_{w},t_{h})\\\\) ，这个与R-CNN是一样的，每个类别都预测4个位置坐标值。\n\n![](/img/2018-04-11-ObjectDetection-12.jpg)\n<center>Fast R-CNN的分类与回归预测. 来源：https://tryolabs.com/blog/</center>\n<br /> \n\n![](/img/2018-04-11-ObjectDetection-13.jpg)\n<center>RoI pooling 原理图，特征图大小为8x8，候选区域为5x7，输出2x2. 来源：https://blog.deepsense.ai/region-of-interest-pooling-explained/</center>\n<br /> \n\nFast R-CNN 与 R-CNN 的另外的一个主要区别点是采用了 softmax 分类器而不是SVM分类器，而且训练过程是单管道的，因为 Fast R-CNN 将分类误差和定位误差合并在一起训练，定位误差采用 smooth L1 而不是 R-CNN 中的 L2。这里说点题外话，就是 R-CNN 训练是多管道的，除了对 CNN 模型预训练，R-CNN 还先对 CNN 模型 funetuning，使用的是 softmax 分类器，但是最后却又训练 SVM 分类器（原因可以见原论文），直觉上感觉有点多此一举，所以现在 Fast R-CNN 直接采用 softmax 分类器了。Fast R-CNN 训练采用mini-batch sampling，每个 mini-batch 大小为 \\\\(128\\\\) ，从 \\\\(N = 2\\\\) 个图片中构建，其中 25% 来自正样本（IoU>=0.5）,75%从负样本中抽样得到（背景，IoU \\\\(\\in [0.1,0.5)\\\\) ），这里的 IoU 阈值属于超参数。在图像分类中，当我们说 batch_size=32 时，是指的是32个图片，在 Fast R-CNN 中并不是这样，因为一个图片含有很多 RoIs，每个 batch 使用的图片非常少（内存限制），所以有时候你会看到 Fast R-CNN 训练时直接从一个图片中构建 batch，这实现起来更容易一些。\n\n\n$$L(p,u,t^u,v) = L_{cls}(p,u) + \\lambda [u > 0]L_{loc}(t^u,v)$$\n\n最好的 Fast R-CNN 模型在 2007 PASCAL VOC 测试集上的 mAp 为70%，在 2010 PASCAL VOC 测试集上的 mAP 为68.8%，而在 2012 PASCAL VOC 测试集上的 mAP 为68.4%，准确度相比 R-CNN 略有提升，其实主要是速度更快。\n\n![](/img/2018-04-11-ObjectDetection-14.jpg)\n<center>Fast R-CNN模型结构图</center>\n<br /> \n  \n\n\n\n\n### Faster R-CNN\n\n对于 Fast R-CNN，其仍然需要 selective search 方法来生产候选区域，这是非常费时的。为了解决这个问题，Faster R-CNN模型引入了 `RPN (Region Proposal Network)` 直接产生候选区域。Faster R-CNN 可以看成是 RPN 和 Fast R-CNN 模型的组合体，即 `Faster R-CNN = RPN + Fast R-CNN`。\n\n\n\n对于 RPN 网络，先采用一个 CNN 模型（一般称为特征提取器）接收整张图片并提取特征图。然后在这个特征图上采用一个 \\\\(N\\times N\\\\) （文中是 \\\\(3\\times 3\\\\) ）的滑动窗口，对于每个滑窗位置都映射一个低维度的特征（如 256-d）。然后这个特征分别送入两个全连接层，一个用于分类预测，另外一个用于回归。对于每个窗口位置一般设置 \\\\(k\\\\) 个不同大小或比例的先验框（anchors, default bounding boxes），这意味着每个位置预测 \\\\(k\\\\) 个候选区域（region proposals）。对于分类层，其输出大小是 \\\\(2k\\\\) ，表示各个候选区域包含物体或者是背景的概率值，而回归层输出 \\\\(4k\\\\) 个坐标值，表示各个候选区域的位置（相对各个先验框）。对于每个滑窗位置，这两个全连接层是共享的。因此，RPN可以采用卷积层来实现：首先是一个 \\\\(n\\times n\\\\) 卷积得到低维特征，然后是两个 \\\\(1\\times 1\\\\) 的卷积，分别用于分类与回归。\n\n\n![](/img/2018-04-11-ObjectDetection-15.jpg)\n<center>RPN架构图</center>\n<br />\n\n可以看到 RPN 采用的是二分类，仅区分背景与物体，但是不预测物体的类别，即 class-agnostic。由于要同时预测坐标值，在训练时，要先将先验框与 ground-truth box 进行匹配，原则为：（1）与某个 ground-truth box 的 IoU 最高的先验框；（2）与某个 ground-truth box 的 IoU 值大于0.7的先验框，只要满足一个，先验框就可以匹配一个 ground-truth，这样该先验框就是正样本（属于物体），并以这个 ground-truth 为回归目标。对于那些与任何一个 ground-truth box 的 IoU 值都低于0.3的先验框，其认为是负样本。RPN 网络是可以单独训练的，并且单独训练出来的 RPN 模型给出很多 region proposals。由于先验框数量庞大，RPN 预测的候选区域很多是重叠的，要先进行 NMS(non-maximum suppression，IoU 阈值设为0.7）操作来减少候选区域的数量，然后按照置信度降序排列，选择 top-N 个 region proposals 来用于训练 Fast R-CNN 模型。RPN的作用就是代替了 Selective search 的作用，但是速度更快，因此 Faster R-CNN 无论是训练还是预测都可以加速。\n\n\n\nFaster R-CNN 模型采用一种4步迭代的训练策略：（1）首先在ImageNet上预训练 RPN，并在 PASCAL VOC 数据集上finetuning；（2）使用训练的PRN产生的 region proposals 单独训练一个 Fast R-CNN 模型，这个模型也先在 ImageNet 上预训练；（3）用 Fast R-CNN 的 CNN 模型部分（特征提取器）初始化RPN，然后对 RPN 中剩余层进行 finetuning，此时 Fast R-CNN 与 RPN 的特征提取器是共享的；（4）固定特征提取器，对 Fast R-CNN 剩余层进行 finetuning。这样经过多次迭代，Fast R-CNN 可以与 RPN 有机融合在一起，形成一个统一的网络。其实还有另外一中近似联合训练策略，将 RPN 的2个 loss 和 Fast R-CNN 的2个 loss 结合在一起，然后共同训练。注意这个过程，Fast R-CNN的 loss 不对 RPN 产生的 region proposals 反向传播，所以这是一种近似（如果考虑这个反向传播，那就是非近似联合训练）。应该来说，联合训练速度更快，并且可以训练出同样的性能。\n\n最好的 Faster R-CNN 模型在 2007 PASCAL VOC 测试集上的 mAP 为78.8% ，而在 2012 PASCAL VOC 测试集上的 mAP 为75.9%。论文中还在 COCO 数据集上进行了测试。Faster R-CNN 中的某个模型可以比采用 selective search 方法的 Fast R-CNN 模型快34倍。可以看到，采用了 RPN 之后，无论是准确度还是速度，Faster R-CNN 模型均有很大的提升。Faster R-CNN 采用RPN代替启发式 region proposal 的方法，这是一个重大变革，后面的 two-stage 方法的研究基本上都采用这种基本框架，而且和后面算法相比，Faster R-CNN 在准确度仍然占据上风。\n\n![](/img/2018-04-11-ObjectDetection-16.jpg)\n<center>Faster R-CNN模型结构图</center>\n\n转载自   \nhttps://zhuanlan.zhihu.com/p/34325398/\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n","tags":["深度学习","目标检测"],"categories":["计算机视觉"]},{"title":"SPP-Net 目标检测（二）","url":"/2018/04/11/SPPNet/","content":"\n## SPP-Net简介\n在上一篇R-CNN的文章中，详细介绍了R-CNN算法，同时也说明了R-CNN的致命缺陷，**超长的训练时间**（84h）和**测试时间**（47s），造成这个问题的主要原因就是**重复性的卷积计算**，在R-CNN中，输入到CNN网络中的图片是ss算法提取到的区域，每一张待检测图都会产生1000-2000个区域，**这也就意味着卷积计算要重复1000-2000次**，但是**由于ss算法提取到的区域本身就有很多重叠，所以这种重复计算是非常没有必要的**。 \n       \n那么能不能只通过一次卷积计算就完成整张图像的特征提取工作呢？这就是SPP-Net的主要贡献，也是在R-CNN之后的很多网络结构的统一目标——**如何共享卷积计算**。\n\nSPP-Net主要改进有下面两个： \n\n1. 共享卷积计算     \n2. 空间金字塔池化\n\n在SPP-Net中同样由这几个部分组成： \n\n1. ss算法           \n2. CNN网络       \n3. SVM分类器   \n4. bounding box\n\n<!-- more -->\n\nss算法的区域建议框同样**在原图上生成**，**但是却在Conv5上提取**，当然由于尺寸的变化，在Conv5层上提取时要经过尺度变换，这是它R-CNN最大的不同，也是SPP-Net能够大幅缩短时长的原因。因为它充分利用了卷积计算，也就是**每张图片只卷积一次**，但是这种改进带来了一个新的问题，由于ss算法生成的推荐框尺度是不一致的，所以在cov5上提取到的特征尺度也是不一致的，这样是没有办法做全尺寸卷积的（Alexnet）。 \n\n所以SPP-Net需要一种算法，这种算法能够**把不一致的输入产生统一的输出**，这就**SPP**，即**空间金字塔池化**，由它替换R-CNN中的pooling层，除此之外，它和R-CNN就一样了。\n\n## 如何共享卷积计算\n\n![](/img/2018-04-11-SPPNet-1.jpg)\n\n在上面这个图中，说明了R-CNN与SPP-Net的区别，R-CNN的卷积神经网络的输入是**ss生成的建议区域（经过尺寸的归一化）**，而SPP-Net的中的卷积神经网络的**输入是整幅图**，经过卷积特征提取后，在**Conv5上做建议区域的提取**。这里有一个问题是一张图经过卷积之后图像的尺寸会发生变化，那么**在原图上生成的ss区域，没有办法直接扣在Conv5层上**，所以需要**做一下坐标变换，使之适应Conv5层的宽高尺寸**。\n\n\n## 坐标变换\n\n在CNN中特征的宽和高发生变化是因为步长的选取，当步长选择为2时，图像的宽高尺寸会变为原来的一半，那么对于在建议区域内的一个点(x,y)，对应的Conv5层上的位置(x’,y’)，应该满足如下关系： \n\n$$(x,y) = (S * x',S*y')$$\n  \n<center>其中S为所有层的步长的乘积</center>     \n而又由于卷积过程中的padding问题，Conv5上的特征会更靠近图像的中心，个人认为这也是为什么左上角的点要做像素加1，右下角的点做像素减1：      \n\n左上：\\\\(x' = \\frac {x}{S} + 1\\\\)     \n右下：\\\\(x' = \\frac {x}{S} - 1\\\\)\n\n\n## 空间金字塔池化\n经过坐标变化之后，在原图上生成的区域建议框就可以映射在Conv5上，但是这样一来就有出现了新的问题，提取到的特征由于尺寸不一致，没办法送到全连接层，解决方法在上面就提到了——SPP：\n\n![](/img/2018-04-11-SPPNet-2.jpg)\n\n上面这张图解释了SPP的原理，那么对于任意尺寸的输入，SPP可以将输入特征平均分为16份，4份和1份，并在每一份（Bin）上做Max pooling，同时特征的厚度保持不变，最后将这些特征串接作为全连接层的输入，如上图所示，假设特征的厚度为 \\\\(256\\\\)，那么SPP后的特征长度（一维特征）就是 \\\\((16+4+1)*256\\\\)，于是维度就统一了。\n\n## SPP-Net训练与测试\n\n![](/img/2018-04-11-SPPNet-3.jpg)\n\n**SPP-Net的训练过程：**\n \n首先拿到在ImageNet预训练的AlexNet模型，用AlexNet计算Conv5层特征，根据ss生成的区域建议，从Conv5上提取到对应的SPP特征，用提取到的特征finetune全连接层（把AlexNet当做分类模型来训练）。  \nAlexNet训练好之后，用fc7层的特征训练SVM分类器，用SPP特征训练bounding box（这里和R-CNN一样了）。 \n\n**SPP-Net的测试过程：**\n\n首先在一张图片上用训练好的AlexNet网络提取整张图片的Conv5和fc7层特征，同时在图片上用ss算法生成1000-2000个区域建议，将区域建议框坐标变换之后在Conv5上提取SPP特征，fc7层特征送入SVM做类别的预测，SPP特征送入bounding box做边界框的修正。\n\n## SPP-Net性能评价\n\n![](/img/2018-04-11-SPPNet-4.jpg)\n\n上面这张图说明了下SPP-Net与R-CNN的性能对比，其中训练时间SPP-Net需要25个小时，而R-CNN需要84小时；单张图片的测试时间SPP-Net只需要2.3s，而R-NN需要47s，这就是共享卷积计算带来的速度上的提升，也是SPP-Net最重要的贡献；最后一个指标，SPP-Net的mAP相比R-CNN反而更低了，这是因为SPP-Net的结构无法fintune卷积层。\n\n\n## SPP-Net的问题\n\n最后，通过上面的性能评价可以看到，SPP-Net在速度上有大幅的提升，其所提出的共享卷积计算的思想在后续的Fast R-CNN与Faster R-CNN中都在沿用，但是从SPP-Net的训练过程可以看出，它是无法finetune卷积层的，这个问题在Fast RCNN中通过多任务损失函数与Roi Pooling提出得以解决。 \n\nSPP-Net的训练过程依然是一个多阶段的训练，这一点和R-CNN一样，并没有改进。 \n由于是多阶段训练，过程中需要存储大量特征。\n\n<br/>\n<br/>\n\n```\n转载自：\nhttp://mp.weixin.qq.com/s?__biz=MzUyMjE2MTE0Mw==&mid=2247484830&idx=1&sn=6f916032c629f980a0ea9230853a6a45&chksm=f9d15b06cea6d2100f395c2cc6cc56c0401792968d810f38fdd52e00cff1abe8a6cf3cbc27ad&scene=21#wechat_redirect\n```\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>","tags":["深度学习","目标检测"],"categories":["计算机视觉"]},{"title":"R-CNN 目标检测（一）","url":"/2018/04/11/RCNN/","content":"\n\n## R-CNN 结构\n对于R-CNN模型，个人是这样理解，它其实是将4个应用于不同任务的已有的算法很好的结合了起来，最终在目标检测任务中取得了不错的效果，这种结合更像是偏向于工程的方法，而不是在算法上的一种突破，当然在后续的Fast-RCNN与Faster-RCNN中模型逐步完善并整合成为一个模型，但是在R-CNN中是没有的。    \n\n\n\nR-CNN由4个部分构成:         \n\n1. 区域建议算法（ss） \n2. 特征提取算法（AlexNet） \n3. 线性分类器（线性SVM） \n4. 边界框修正回归模型（Bounding box）\n\n<!-- more -->\n\n简单来说，RCNN使用以下四步实现目标检测： \n\n1. 在图像中确定约1000-2000个候选框 \n2. 对于每个候选框内图像块，使用深度网络提取特征 \n3. 对候选框中提取出的特征，使用分类器判别是否属于一个特定类 \n4. 对于属于某一特征的候选框，用回归器进一步调整其位置 \n\n![](/img/2018-04-11-RCNN-1.jpg)\n\n\n### 区域建议算法 Region Proposal\n区域建议（Region Proposal）算法在CNN之前就已经有了，而且算法不止一种，`ss (selective search)` 算法是比较著名的一个，此外还有EdgeBox，MSER，MCG等等算法，CS231n中对这几种算法做了一个简单的介绍，感兴趣的话可以移步到CS231n第16课时。\n\n那么ss算法在R-CNN中有什么用呢？这要从目标检测任务开始谈起，在一副图像中要实现目标检测任务，一种最简单的思路是如果建立滑动窗，对每次滑动窗提取出来的图像做分类，如果分类结果恰好是目标的话，就实现了检测啦，目标的属性由分类器给，目标的位置由滑动窗给。但是考虑到一次滑动遍历产生的子图像数量就不小了，同时还有不同步长和窗口尺寸的情况，此时产生的待分类图像是非常多的，这种方式显然没什么实用价值，于是就有了ss算法，**一种根据图像自身信息产生推荐区域的算法，它大概会产生1000-2000个潜在目标区域**，照比滑动遍历的方式，这个数量已经减少了很多了。\n\n使用了Selective Search1方法从一张图像生成约2000-3000个候选区域。基本思路如下： \n\n- 使用一种过分割手段，将图像分割成小区域 \n- 查看现有小区域，合并可能性最高的两个区域。重复直到整张图像合并成一个区域位置 \n- 输出所有曾经存在过的区域，所谓候选区域\n\n候选区域生成和后续步骤相对独立，实际可以使用任意算法进行。\n\n#### 合并规则\n\n优先合并以下四种区域：   \n\n- 颜色（颜色直方图）相近的   \n- 纹理（梯度直方图）相近的   \n- 合并后总面积小的   \n- 合并后，总面积在其BBOX中所占比例大的\n\n第三条，保证合并操作的尺度较为均匀，避免一个大区域陆续“吃掉”其他小区域。\n\n> 例：设有区域 a-b-c-d-e-f-g-h。较好的合并方式是：ab-cd-ef-gh -> abcd-efgh -> abcdefgh。   \n不好的合并方法是：ab-c-d-e-f-g-h -> abcd-e-f-g-h -> abcdef-gh -> abcdefgh。\n\n第四条，保证合并后形状规则。\n\n> 例：左图适于合并，右图不适于合并。   \n![](/img/2018-04-11-RCNN-2.jpg)\n\n上述四条规则只涉及区域的颜色直方图、纹理直方图、面积和位置。合并后的区域特征可以直接由子区域特征计算而来，速度较快。\n\n#### 多样化与后处理\n为尽可能不遗漏候选区域，上述操作在多个颜色空间中同时进行（RGB,HSV,Lab等）。在一个颜色空间中，使用上述四条规则的不同组合进行合并。所有颜色空间与所有规则的全部结果，在去除重复后，都作为候选区域输出。\n  \n\n\n\n### 特征提取算法\n\n这里的特征提取算法其实就是卷积神经网络，R-CNN中使用的是AlexNet，但是作者（Ross）并没有把AlexNet当做分类器来使用，而是只用了网络的特征层做ss算法输出的图像的特征提取工作，然后第7层特征给了SVM分类器，第五次特征给了Bounding Box回归模型。\n\n### 线性分类器\n\nR-CNN使用了线性SVM分类器，需要说明的是，目标检测任务是有分类的功能的，比如一个任务是检测猫和狗，那么除了要框出猫和狗的位置之外，也需要判断是猫还是狗，这也是SVM在R-CNN中的作用。**所以待检测物体有几类，那么就应该有几个二分类的SVM分类器**，在上面的例子中，就需要两个二分类分类器了，分别是“猫-非猫”模型和“狗-非狗”模型，在R-CNN中，分类器有20个，它的输入特征是AlexNet提取到的fc7层特征。\n\n由于负样本很多，使用 `hard negative mining` 方法。     \n正样本: 本类的真值标定框。     \n负样本: 考察每一个候选框，如果和本类所有标定框的重叠都小于0.3，认定其为负样本\n\n### 边界框修正回归模型\n\nBounding box也是个古老的话题了，计算机视觉常见任务中，在分类与检测之间还有一个定位任务，在一副图像中只有一个目标，然后把这个目标框出来，用到的就是Bounding box回归模型。在R-CNN中，Bounding box的作用是**修正ss推荐的区域的边界，输入的特征是AlexNet的第五层特征**，与SVM分类器一样，它也是**每一个类别都有一个模型**，一共20个。\n\n\n## R-CNN 的训练\n\nR-CNN训练了CNN，SVM与Bounding box三个模型，因为ss不用训练。ss在生成了1000-2000个推荐区域之后，就和训练任务没关系了，训练样本是由ss区域生成出来的子图构建起来的。 而且三个部分的训练时独立的，并没有整合在一起。\n\n### 1. 训练CNN\nCNN是在ImageNet上pre-train的AlexNet模型，在R-CNN中进行**fine-tune**，fine-tune的过程是将AlexNet的**Softmax改为任务需要的类别数**，然后还是当做一个分类模型来训练，训练样本的构建使用ss生成的子图，当这些图与实际样本的框（Ground-truth）的IoU大于等于0.5时，认为是某一个类的**正样本**，这样的类一共有20个；IoU小于0.5时，认为是**负样本**。然后就可以AlexNet做pre-train了，pre-train之后AlexNet的Softmax层就被扔掉了，只剩下训练后的参数，这套参数就用来做特征提取。\n\n### 2. 训练SVM\n\n之前提到了，SVM的输入特征是AlexNet fc7的输出，然后SVM做二分类，一个有20个SVM模型。那么对于其中某一个分类器来说，它的正样本是所有Ground-truth区域经过AlexNet后输出的特征，负样本是与Ground-truth区域重合IoU小于0.3的区域经过AlexNet后输出的特征，特征和标签确定了，就可以训练SVM了。\n\n### 3. 训练Bounding box回归模型 \n\nBounding box回归模型也是20个，还是拿其中一个来说，它的输入是AlexNet conv5的特征，注意这里的20指的是类的个数，但是对一个Bounding box来说，它有4套参数，因为一个Bounding box回归模型分别对4个数做回归，这4个数是表征边界框的四个值，模型的损失函数如下： \n![](/img/2018-04-11-RCNN-3.jpg)\n\n\n其中i是样本个数，*就是4个数，他们分别是x，y，w，h，其中（x，y）是中心位置，（w，h）是宽和高；P是ss给出来的区域，它由Px，Py，Pw，Ph四个数决定，这个区域经过AlexNet后再第五层输出特征，然后在特征每一个维度前都训练一个参数w，一组特征就有一组w，随4组做回归就有4组w；最后一个数就是t，它同样有4个数tx，ty，tw，th，是这样计算出来的： \n\n$$t_{x} = (G_{x} - P_x) / P_w$$   \n\n$$t_y = (G_y - P_y) / P_h$$\n\n$$t_w = \\log(\\frac{G_w}{P_w})$$\n\n$$t_h = \\log(\\frac{G_h}{P_h})$$\n\n而G就是经过修正后的边界框，它还是4个数Gx，Gy，Gw，Gh。通过上面的公式可以看到，t是边界框的偏差。 最后就是到底什么样的ss区域能够作为输入，在这里是IoU大于0.6的。 用一句话总结Bounding box回归模型就是：对于某一个类的回归模型而言，用IoU>0.6的ss区域经过卷积后作为输入特征，用同一组特征分别训练4组权值与之对应，对边界框四个属性值分别做回归。\n\n经过上面三个独立的部分，R-CNN的训练就完成了，可以看到，确实是非常麻烦，这不仅仅体现在速度慢上，过程也及其繁琐，因为每一步都需要重新构建样本。\n\n### R-CNN的测试\n\n1. ss算法提取1000-2000个区域； \n2. 对所有的区域做尺寸统一，为了CNN网络能接受； \n3. 用AlexNet网络提出两套特征，一个是fc7层的，一个是con5层的； \n4. 对于一个fc7区域的特征，分别过20个分类器，看看哪个分类器给的分数最高，以确定区域的类别，并把所有的区域一次操作； \n5. 对上述所有打好label的区域使用非极大值抑制操作，以获取没有冗余（重叠）的区域子集，经过非极大值抑制之后，就认为剩下的所有的区域都是最后要框出来的； \n6. 重新拿回第5步剩下的区域con5层的特征，送入Bounding box模型，根据模型的输出做出一次修正； \n7. 根据SVM的结果打标签，根据修正的结果画框； \n8. 结束\n\n\n```\n转载自：    \nhttp://mp.weixin.qq.com/s?__biz=MzUyMjE2MTE0Mw==&mid=2247484206&idx=1&sn=f9084165a4673affd8e23ac97f707eb8&chksm=f9d15db6cea6d4a04a777d45ae3e3f1a3ef6f657352c98db9664084fe4b4f802273dde7ac943&scene=21#wechat_redirect   \n参考\nhttps://blog.csdn.net/shenxiaolu1984/article/details/51066975\n```\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>","tags":["深度学习","目标检测"],"categories":["计算机视觉"]},{"title":"TesnsorFlow 模型的保存于读取","url":"/2018/04/06/tensorflow模型保存与读取/","content":"\n\n\n## 模型的保存于读取\n\n### 保存\n\n```python\nimport tensorflow as tf\n\n# 声明两个变量\nv1 = tf.Variable(tf.random_normal([1, 2]), name=\"v1\")\nv2 = tf.Variable(tf.random_normal([2, 3]), name=\"v2\")\ninit_op = tf.global_variables_initializer() # 初始化全部变量\n\nsaver = tf.train.Saver() # 声明tf.train.Saver类用于保存模型\n\nwith tf.Session() as sess:\n    sess.run(init_op)\n    print(\"v1:\", sess.run(v1)) # 打印v1、v2的值一会读取之后对比\n    print(\"v2:\", sess.run(v2))\n    \n    saver_path = saver.save(sess, \"save/model.ckpt\")  # 将模型保存到save/model.ckpt文件\n    print(\"Model saved in file:\", saver_path)\n```\n```\nv1: [[-0.33648431 -0.46078524]]\nv2: [[-0.73017526 -0.42726403  1.74622989]\n [-1.50939059 -0.87901741  0.94227242]]\nModel saved in file: save/model.ckpt\n```\n\n这段代码中，通过saver.save函数将TensorFlow模型保存到了save/model.ckpt文件中，这里代码中指定路径为\"save/model.ckpt\"，也就是保存到了当前程序所在文件夹里面的save文件夹中。\n\n<!-- more -->\n\nTensorFlow模型会保存在后缀为.ckpt的文件中。保存后在save这个文件夹中实际会出现3个文件，因为TensorFlow会将计算图的结构和图上参数取值分开保存。\n\n- model.ckpt.meta文件保存了TensorFlow计算图的结构，可以理解为神经网络的网络结构\n- model.ckpt文件保存了TensorFlow程序中每一个变量的取值\n- checkpoint文件保存了一个目录下所有的模型文件列表\n\n\n### 读取\n\n**方法1**\n\n```python\nimport tensorflow as tf\n\n# 使用和保存模型代码中一样的方式来声明变量\nv1 = tf.Variable(tf.random_normal([1, 2]), name=\"v1\")\nv2 = tf.Variable(tf.random_normal([2, 3]), name=\"v2\")\nsaver = tf.train.Saver() # 声明tf.train.Saver类用于保存模型\nwith tf.Session() as sess:\n    saver.restore(sess, \"save/model.ckpt\") # 即将固化到硬盘中的Session从保存路径再读取出来\n    print(\"v1:\", sess.run(v1)) # 打印v1、v2的值和之前的进行对比\n    print(\"v2:\", sess.run(v2))\n    print(\"Model Restored\")\n```\n```\nv1: [[ 0.22189325  0.81017244]]\nv2: [[-0.99802715 -0.54230249  1.92878962]\n [ 1.04581845  0.25543991 -0.6358065 ]]\nModel Restored\n```\n\n这段加载模型的代码基本上和保存模型的代码是一样的。也是先定义了TensorFlow计算图上所有的运算，并声明了一个tf.train.Saver类。两段唯一的不同是，在加载模型的代码中没有运行变量的初始化过程，而是将变量的值通过已经保存的模型加载进来。 \n\n\n**方法2**\n\n如果不希望重复定义图上的运算，也可以直接加载已经持久化的图：\n\n```python\nimport tensorflow as tf\n# 在下面的代码中，默认加载了TensorFlow计算图上定义的全部变量\n# 直接加载持久化的图\nsaver = tf.train.import_meta_graph(\"save/model.ckpt.meta\")\nwith tf.Session() as sess:\n    saver.restore(sess, \"save/model.ckpt\")\n    # 通过张量的名称来获取张量\n    print(sess.run(tf.get_default_graph().get_tensor_by_name(\"v1:0\")))\n```\n```\n[[ 0.22189325  0.81017244]]\n```\n\n**恢复操作和其它元数据**\n\n我想分享的最后一个信息是，**Saver**将保存与图有关联的任何元数据。这就意味着，当我们恢复一个模型的时候，我们还同时恢复了所有与图相关的变量、操作和集合。\n\n当我们恢复一个元模型（restore a meta checkpoint）时，实际上我们执行的操作是将恢复的图载入到当前的默认图中。所有当你完成模型恢复之后，你可以在默认图中访问载入的任何内容，比如一个张量，一个操作或者集合。\n\n\n```python   \nimport tensorflow as tf \n\n# Let's laod a previous meta graph in the current graph in use: usually the default graph \n# This actions returns a Saver \nsaver = tf.train.import_meta_graph('results/model.ckpt-1000.meta') \n\n# We can now access the default graph where all our metadata has been loaded \ngraph = tf.get_default_graph() \n\n# Finally we can retrieve tensors, operations, etc. \nglobal_step_tensor = graph.get_tensor_by_name('loss/global_step:0')\ntrain_op = graph.get_operation_by_name('loss/train_op') \nhyperparameters = tf.get_collection('hyperparameters') \n```\n\n\n  \n  \n**在新图中导入预训练模型**\n\n至此，你应该已经明白了如何去保存和恢复一个模型。然而，我们还可以使用一些技巧去帮助你更快的保存和恢复一个模型。比如：\n\n  * 一个图的输出能成为另一个图的输入吗？\n\n答案是确定的。但是目前我的做法是先将第一个图进行保存，然后在另一个图中进行恢复。但是这种方案感觉很笨重，我不知道是否有更好的方法。\n\n但是这种方法确实能工作，除非你想要去重新训练第一个图。在这种情况下，你需要将输入的梯度重新输入到第一张图中的特定的训练步骤中。我想你已经被这种复杂的方案给逼疯了把。:-)\n\n  * 我可以在一个图中混合不同的图吗？\n\n答案当然是肯定的，但是你必须非常小心命名空间。这种方法有一点好处是，简化了一切。比如，你可以预加载一个VGG-19模型。然后访问图中的任何节点，并执行你自己的后续操作，从而训练一整个完整的模型。\n\n如果你只想微调你自己的节点，那么你可以在你想要的地方中断梯度。\n    \n```python\nimport tensorflow as tf\n\n# Load the VGG-16 model in the default graph\nvgg_saver = tf.train.import_meta_graph(dir + '/vgg/results/vgg-16.meta')\n# Access the graph\nvgg_graph = tf.get_default_graph()\n\n# Retrieve VGG inputs\nself.x_plh = vgg_graph.get_tensor_by_name('input:0')\n\n# Choose which node you want to connect your own graph\noutput_conv =vgg_graph.get_tensor_by_name('conv1_2:0')\n# output_conv =vgg_graph.get_tensor_by_name('conv2_2:0')\n# output_conv =vgg_graph.get_tensor_by_name('conv3_3:0')\n# output_conv =vgg_graph.get_tensor_by_name('conv4_3:0')\n# output_conv =vgg_graph.get_tensor_by_name('conv5_3:0')\n\n# Stop the gradient for fine-tuning\noutput_conv_sg = tf.stop_gradient(output_conv) # It's an identity function\n\n# Build further operations\noutput_conv_shape = output_conv_sg.get_shape().as_list()\nW1 = tf.get_variable('W1', shape=[1, 1, output_conv_shape[3], 32], initializer=tf.random_normal_initializer(stddev=1e-1))\nb1 = tf.get_variable('b1', shape=[32], initializer=tf.constant_initializer(0.1))\nz1 = tf.nn.conv2d(output_conv_sg, W1, strides=[1, 1, 1, 1], padding='SAME') + b1\na = tf.nn.relu(z1)\n\n```\n  \n\n\n## 数据增强  \n\n\n\n\n```python\n# 随机裁剪图片\n# Randomly crop a [height, width] section of the image.\ndistorted_image = tf.random_crop(reshaped_image, [height, width, 3])\n\n# 随机翻转图片，每张图片有50%的概率被水平左右翻转，另有50%的概率保持不变\n# Randomly flip the image horizontally.\ndistorted_image = tf.image.random_flip_left_right(distorted_image)\n\n# 随机改变亮度和对比度\n# Because these operations are not commutative, consider randomizing\n# the order their operation.\ndistorted_image = tf.image.random_brightness(distorted_image,\n                                             max_delta=63)\ndistorted_image = tf.image.random_contrast(distorted_image,\n                                           lower=0.2, upper=1.8)\n```\n\n原始的训练图片是 `reshaped_image`，最后会得到一个数据增强后的训练样本 `distorted_image`。 训练时，直接使用 `distorted_image` 进行训练即可\n\n\n\n\n\n```\n转载与参考：\nhttp://www.cnblogs.com/seaspring/  \nhttps://www.jianshu.com/p/8487db911d9a\n```","tags":["python","深度学习"],"categories":["TensorFlow"]},{"title":"计算机编码：base64","url":"/2018/04/05/base64/","content":"\n\n\n\n\n## 1. 计算机开始之初，二进制与Hex\n计算机这个东西，最初是美国人发明的。作为一个可以计算、存储、通信的复杂玩意，最最基本的功能，应该是能读懂人类让它干的事情。所以呢，我们得构造一个计算机能用的语言，这个语言计算机能看懂，人也能看懂，这样才能交流嘛。\n\n计算机所用的语言是什么呢？这个语言非常简单，只有0和1两种表示。0代表否，1代表是。通过0和1的各种组合，以及0和1之间的各种运算（位运算），计算机就能进行理解、分析这个世界，并帮助人类完成工作了。\n\n但是0和1太简单了，简单到任何一个简单的数字都可能用一长串0和1来表示。举了例子，如果让计算机记住1000这个数，计算机就要记住11,1110,1000这么长一串数字。计算机倒是好记，但是人类记不住啊… 有没有一种方法，能够让计算机表示的数据短一点，好记一点呢？\n\nHex就是最简单的方法了。人类习惯于使用十进制，毕竟人类有是个手指，十个一进位，挺好的！计算机本质上是二进制，就0和1两种数字的表示方法，所有其他可以直接转换的表示方法中，进制只能有2这个数，不能有其他的数字。比如4,8,16就没问题。其他的数字，不管怎么折腾，转换后也得是2^n形式。\n\n<!-- more -->\n\n想让人类和计算机都能接受表示形式，8进制和16进制都是可以接受的。8进制的话，只使用0-7折8个数字就好了。16进制，光用数字是不够了，还得用用其他的字符。然而，计算机毕竟是美国人发明的，他们觉得，干脆，就16进制：用0-9表示前10个数，后面的用A、B、C、D、E、F表示，不区分大小写。这就是最简单的Hex编码了。\n\nHex的编码原理是：把一长串二进制数每4个分一组，如果位数不够就在高位补0。4位数字一共只有16种情况，分别用0-9，A-F表示这16种情况。编码表类似这样：\n\n![](/img/2018-04-05-base64-1.jpg)\n\n## 2. 要让可读性更强：ASCII码\n\n\nHex编码虽然好，但有个问题：从计算机上打开个文件，满眼的十六进制数，很头大啊… 十六进制还是不太好表示文本。能不能创建一种方法，能表示键盘打出来的全部英文字符、符号呢？键盘打不出来的字符，比如什么回车啦，占位啦，用特殊的符号表示。这样一来，打开一个文件，满眼英文，岂不是很爽快…\n\n美国作为计算机的始祖国家，自然要推出一个这样的标准代码表。这就是美国信息交换标准代码，简称ASCII码表。这个码表包括了数字、英文大小写、符号、以及各种各样的转义字符，可以包含英文所用的全部功能。很快地，ASCII码称为了国际标准，现在大家知道的编码形式，都是与ASCII码兼容的。（图片来自：[美国信息交换标准代码](https://link.zhihu.com/?target=http%3A//baike.baidu.com/view/492542.htm))\n\n  \n![](/img/2018-04-05-base64-2.jpg)\n\n## 3. 别的语言怎么办？UTF-8等其他编码方法\n\n\n这个码表一出来，英语国家开心了…其他国家的脑袋疼了… 带注音的符号怎么办？日语韩语怎么办？最为博大精深的中文怎么办… 于是，各个国家也推出了本国语言的编码表。但是，为了能在计算机系统中通用，这些编码表基本都与ASCII码兼容。\n\n最为知名的就是UTF-8了。这个编码又称为万国码，顾名思义，就是支持包括中文简体、中文繁体、日语、韩语等各种语言的编码。\n\n## 4. 用一种编码形式打开另一种编码形式 会怎样？\n\n\n\n既然每个国家都有自己的编码表了，问题也就来了。现在都国际化了，我要用一个支持本国语言的编码系统，打开另一个编码系统编码的文本，会出现什么情况呢？这就是乱码了… 更为严重的是，随着互联网的出现，各个国家的电脑都需要通信，而通信的一种方式就是使用URL地址。每个国家都希望把这个地址写成自己国家的语言。但这会导致其他国家根本没法访问地址，因为打不出这个字符嘛。所以，人类迫切需要一种中间编码形式，既能够兼容ASCII码，又能够把任意一种编码形式转换成只使用可读字符就能表示的编码。其中一种编码形式，就是Base64编码。\n\nBase64编码，顾名思义，用64个可读字符进行编码。与Hex的16个字符相比多了很多，但是比ASCII码又少了一倍，去除了不可读字符。标准Base64编码中，这些字符是：  \n\n\n  -  数字：0,1,2,3,4,5,6,7,8,9，共10个\n  -  小写字母：a,b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z，共26个\n  -  大写字母：A,B,C,D,E,F,G,H,I,J,K,L,M,N,O,P,Q,R,S,T,U,V,W,X,Y,Z，共26个\n  -  加好+以及斜杠/\n\n有的时候，根据不同的需要，Base64还有很多变种。比如，如果浏览器地址中用“+”和“/”的话，浏览器会将其转换为%XX的形式，又多了一步。因此可以将“+”和“/”换成“-”和“_”。\n\n \n这种编码形式长度也短，效率也高。这样一来，数据通信的时候，不管来的是什么语言，都转化成Base64后再发送和接收。要是别国地址什么的打不出来，就直接打Base64编码形式就好了。如果细心的话，会发现百度云盘共享的时候，用的就是Base64。举个例子，前几天有个朋友给我发送一个视频，给的链接是这样的（后面四位略去）：  \n\n    \n    http://pan.baidu.com/s/1gdH**** \n\n前面都是标准的网页形式，最后那个子文件，就是用Base64编码的，而且可以在任何通信工具中传递，方便快捷啊。\n\n\n## 5. 误区：Base64不是加密算法\n\n很多博客什么的都把Base64当做加密算法，这是不对的。Base64不具有可读性，但不代表这个编码是加密的。加密需要保证，没有密钥的人无法解密信息，无法从密文中获得任何明文信息。Base64编码显然没有密钥什么事\n\n\n```\n作者：刘巍然-学酥\n链接：https://www.zhihu.com/question/38036594/answer/74917716\n来源：知乎\n```","tags":["编码"],"categories":["计算机基础"]},{"title":"matplotlib 绘图与显示图像","url":"/2018/04/05/matplotlib/","content":"\n\n\n# 绘图\n## 导入需要的模块 \n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n```\n## 画基本图形\n\n### 折线图 plot\n\n```python\ny=np.random.randn(100) \nplt.plot(y,'b-') \nplt.xlabel('x') \nplt.ylabel('y') \nplt.title(u'title') \nplt.show()\n```\n\n![](/img/2018-04-05-matplotlib-1.jpg)\n\n<!-- more -->\n\n可选参数如下所示：\n\n![](/img/2018-04-05-matplotlib-2.jpg)\n\n也可以通过更改参数来改变画图效果\n\n```python \nx=np.cumsum(np.random.rand(100)) \nplt.plot(y,label='line label',color='r',linestyle='-',marker='o') \nplt.show()\n```\n![](/img/2018-04-05-matplotlib-3.jpg)\n\n可选的参数有\n\n![](/img/2018-04-05-matplotlib-4.jpg)\n\n### 散点图 scatter \n\n例如：数据服从正态分布，相关系数是0.5\n\n\n```python  \nz=np.random.randn(100,2) \nz[:,1]=0.5*z[:,0]+np.sqrt(0.5)*z[:,1] \nx=z[:,0]\ny=z[:,1]\nplt.scatter(x,y) \nplt.show()\n```\n\n![](/img/2018-04-05-matplotlib-5.jpg)\n参数也是可以修改的例如：\n\n\n```python    \nz=np.random.randn(100,2) \nz[:,1]=0.5*z[:,0]+np.sqrt(0.5)*z[:,1] \nx=z[:,0] \ny=z[:,1]\nplt.scatter(x,y,marker='s',c='r') \nplt.show()\n```\n\n\n\n![](/img/2018-04-05-matplotlib-6.jpg)\n\n### 条形图 bar\n\n>画条形图 bar,需要两个一位数组，第一个是横坐标，每个条形图的开始位置；纵坐标是条形图的高度\n\n\n```python    \ny=np.random.rand(5) \nx=np.arange(5)\nplt.bar(x,y) \nplt.show()\n```\n\n![](/img/2018-04-05-matplotlib-7.jpg)\n\n修改他的显示属性,可以使用一个颜色数组来指定每个条形图的颜色。\n\ny=np.random.rand(5);  \nx=np.arange(5);  \ncolors=['#FF0000','#FFFF00','#00FF00','#00FFFF','#0000FF']  \nplt.bar(x,y,width=0.5,color=colors,edgecolor='#000000',linewidth=5)  \nplt.show()\n\n![](/img/2018-04-05-matplotlib-8.jpg)\n\n### 图表 pie\n  \n>使用一个一维数组来表示，不要求累加和是1，可以使人以大小的正数\n\n```python\nx=np.arange(1,8) labels=['label1','label2','label3','label4','label5','label6','label7'] plt.pie(x,labels=labels) \nplt.show()\n```\n\n![](/img/2018-04-05-matplotlib-9.jpg)\n\n \n\n### 直方图 hist\n\n>需要一个数组，bins参数表示将数据分成几组，默认是10组\n    \n```python\nx=np.random.randn(2000)\nplt.hist(x,bins=30)\nplt.show()\n```\n\n![](/img/2018-04-05-matplotlib-10.jpg)\n\n如果想要生成累计直方图需要使参数cumulative为true\n\n```python\nx=np.random.randn(1000);  \nplt.hist(x,bins=20,cumulative=True);  \nplt.show()\n```\n\n![](/img/2018-04-05-matplotlib-11.jpg)\n\n### 多图表\n\n>在同一个图上画出多张图表，需要首先使用figure()函数生成一个画板，画子图时需要使用sp=add_subplot(m,n,p)来表示子图。m表示行，n表示列，p表示第几个图。\n\n返回的是子图的句柄用于设置一些参数。最后要想显示出来需要使用draw()函数，将这些子图画在画板上，然后用show()函数显示出来。\n\n```python\nfig = plt.figure() \nax = fig.add_subplot(2, 2, 1) \ny = np.random.randn(100) \nplt.plot(y); ax.set_title('1') \ny = np.random.rand(5) \nx = np.arange(5) \nax = fig.add_subplot(2, 2, 2) plt.bar(x, y) \nax.set_title('2')\ny = np.random.rand(5) \ny = y / np.sum(y) y[y < .05] = .05 \nax = fig.add_subplot(2, 2, 3) \nplt.pie(y) ax.set_title('3') \nplt.draw() \nplt.show()\n```\n\n\n\n![](/img/2018-04-05-matplotlib-12.jpg)\n\n### 3D 曲面图\n\n画线，使用plot，需要Axed3D(fig)来画出3D轴线，\n\n```python\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfig=plt.figure()\nax = Axes3D(fig)\n\nx = np.arange(-10,10,0.1)\ny = np.arange(-10,10,0.1)\n\n#网格化数据\nX, Y = np.meshgrid(x, y)\nZ = np.sqrt(X**2 + Y**2)\n\nax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='rainbow')\nplt.show()\n```\n\n![](/img/2018-04-05-matplotlib-13.jpg)\n\n### 3D 曲线图\n\n```python\nimport copy\nfrom mpl_toolkits.mplot3d import Axes3D\nx=np.linspace(0,6*np.pi,600);\nz=copy.copy(x)\nx=np.cos(z)\ny=np.sin(z);\nfig=plt.figure()\nax = Axes3D(fig)\nax.plot(x,y,zs=z)\nplt.xlabel('x')\nplt.ylabel('y')\nax.view_init(15,45)\nplt.draw()\nplt.show()\n```\n\n\n![](/img/2018-04-05-matplotlib-14.jpg)\n\n## 图像配置\n\n\n### 字体\n\n```python\nfrom matplotlib.font_manager import FontProperties  \nfont_song = FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=15)\n```\n使用文字时指定参数 `fontproperties=font_song` 即可\n\n或者\n\n```python\nax2.set_xlabel('window size', fontsize=9, fontproperties = 'Times New Roman')\n# plt.xlabel('window size', fontdict={'family' : 'Times New Roman', 'size':8})\n\n```\n\n或者\n\n将全局字体改为Times New Roman：\n\n```python\nimport matplotlib.pyplot as plt\nplt.rc('font',family='Times New Roman')\n```\n如果出现类似如下错误：\n\n```python\napps/rhel6/Python-2.7.2/lib/python2.7/site-packages/matplotlib/font_manager.py:1224: UserWarning: findfont: Font family ['Playfair Display'] not found. Falling back to Bitstream Vera Sans(prop.get_family(), self.defaultFamily[fontext]))\n```\n\n则需要删除 `fontList.cache` 文件。这个文件有点不好找。\n\n用如下命令获得目录：\n\n```python\nimport matplotlib as plt\nplt.get_cachedir()\n```\n\n然后进去删除fontList.cache就可以了！\n\n>参考 [matplotlib 字体改为 Times New Roman](https://blog.csdn.net/ginynu/article/details/70808962)\n\n### 窗口\n\n#### 窗口设置\n\n开启一个窗口，num 设置子图数量，figsize 设置窗口大小，dpi 设置分辨率\n\n\n```python\nfig = plt.figure(num=1, figsize=(15, 8),dpi=80) \n```\n\n#### 多张子图\n\n```python\nfig=plt.figure()\n\nax1=fig.add_subplot(221) \nax1.plot(x,x)\n\nax2=fig.add_subplot(222)\nax2.plot(x,-x)\n\nax3=fig.add_subplot(223)\nax3.plot(x,x**2)\n\nax4=fig.add_subplot(224)\nax4.plot(x,np.log(x))\n\nplt.show()\n```\n\n![](/img/2018-04-05-matplotlib-17.png)\n\n### 坐标轴\n\n#### 关闭刻度\n\n- 对于 `plt`\n\n```python\nplt.xticks([])\nplt.yticks([])\n```\n\n- 对于 `ax（matplotlib.axes._subplots.AxesSubplot）`\n\n```python\nax.set_xticks([])\nax.set_yticks([])\n```\n\n\n\n```python\nax.spines['top'].set_visible(False)  # 去掉上边框\nax.spines['right'].set_visible(False)  # 去掉右边框\n```\n\n- 全部坐标轴（上下左右）不显示\n\n```python\nplt.axis('off')\n\n```\n\n\n\n\n#### 坐标轴名称\n\n```python\nplt.xlabel('Window Size',fontsize=14)\nplt.ylabel('SNR',fontsize=14)\n```\n\n或者\n\n```python\nax1 = fig.add_subplot(111)\nax1.plot(x, snr, label=\"SNR\")\nax1.set_ylabel('SNR')\nax1.set_xlabel('Window Size')\n```\n#### 坐标轴范围\n\n```python\n#设置坐标轴范围\nplt.xlim((-5, 5))\nplt.ylim((-2, 2))\n```\n\n#### 图表标题\n\n```python\nplt.title('Squares',fontsize=24)\n```\n\n#### 刻度字号\n\n```python\nplt.tick_params(axis='both',which='major',labelsize=14)\n\n```\n\n#### 图例\n\n同一图表中两条线\n\n```python\nplt.legend(handles=[l1,l2],labels=['up','down'],loc='best')\n```\n\n#### 双 y 轴\n\n```python\nfig = plt.figure()\nplt.xlabel('Window Size',fontsize=14)\n\nax1 = fig.add_subplot(111)\nax1.plot(x, snr, label=\"SNR\")\nax1.set_ylabel('SNR')\nplt.legend()   # 添加图例\n\nax2 = ax1.twinx()  # this is the important function\nax2.plot(x, lsd, 'r',linestyle='--', label=\"LSD\")\nax2.set_ylabel('LSD')\nplt.legend()\n\nplt.show()\n```\n![](/img/2018-04-05-matplotlib-15.png)\n\n其中，`ax = twinx()` 意思是创建了一个独立的Y轴，共享了X轴。双坐标轴\n\n\n这时两个图例是分开的，如果想要图例合并，则应为：\n\n```python\nfig = plt.figure()\nplt.xlabel('Window Size',fontsize=14)\n\nax1 = fig.add_subplot(111)\nl1 = ax1.plot(x, snr, label=\"SNR\")\nax1.set_ylabel('SNR')\n\nax2 = ax1.twinx()  # this is the important function\nl2 = ax2.plot(x, lsd, 'r',linestyle='--', label=\"LSD\")\nax2.set_ylabel('LSD')\n\nlns = l1+l2\nlabs = [l.get_label() for l in lns]\nax1.legend(lns, labs, loc=0)\n\nplt.show()\n```\n\n![](/img/2018-04-05-matplotlib-16.png)\n\n## 保存图像\n\n使用savefig(’_filename_._ext_’) ，其中ext支持png, pdf, ps, eps or svg格式。\n\n```python\nplt.savefig('F:/where-you-want-to-save.png', dpi=300, bbox_inches=\"tight\")\n```\n- 保存文件，dpi指定保存文件的分辨率\n- `bbox_inches=\"tight\"` 可以保存图上所有的信息，不会出现横纵坐标轴的描述存掉了的情况\n\n## 论文图实例\n\n\n### 实例 1\n\n```python\n# coding=utf-8\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.sans-serif'] = ['Arial']  # 如果要显示中文字体,则在此处设为：SimHei\nplt.rcParams['axes.unicode_minus'] = False  # 显示负号\n\nx = np.array([1, 2, 3, 4, 5, 6])\nVGG_supervised = np.array([2.9749694, 3.9357018, 4.7440844, 6.482254, 8.720203, 13.687582])\nVGG_unsupervised = np.array([2.1044724, 2.9757383, 3.7754183, 5.686206, 8.367847, 14.144531])\nourNetwork = np.array([2.0205495, 2.6509762, 3.1876223, 4.380781, 6.004548, 9.9298])\n\n# label在图示(legend)中显示。若为数学公式,则最好在字符串前后添加\"$\"符号\n# color：b:blue、g:green、r:red、c:cyan、m:magenta、y:yellow、k:black、w:white、、、\n# 线型：-  --   -.  :    ,\n# marker：.  ,   o   v    <    *    +    1\nplt.figure(figsize=(10, 5),dpi=600)\nplt.grid(linestyle=\"--\")  # 设置背景网格线为虚线\nax = plt.gca()\nax.spines['top'].set_visible(False)  # 去掉上边框\nax.spines['right'].set_visible(False)  # 去掉右边框\n\n\nplt.plot(x, VGG_supervised, marker='o', color=\"blue\", label=\"VGG-style Supervised Network\", linewidth=1.5)\nplt.plot(x, VGG_unsupervised, marker='o', color=\"green\", label=\"VGG-style Unsupervised Network\", linewidth=1.5)\nplt.plot(x, ourNetwork, marker='o', color=\"red\", label=\"ShuffleNet-style Network\", linewidth=1.5)\n\ngroup_labels = ['Top 0-5%', 'Top 5-10%', 'Top 10-20%', 'Top 20-50%', 'Top 50-70%', ' Top 70-100%']  # x轴刻度的标识\nplt.xticks(x, group_labels, fontsize=12, fontweight='bold')  # 默认字体大小为10\nplt.yticks(fontsize=12, fontweight='bold')\n# plt.title(\"example\", fontsize=12, fontweight='bold')  # 默认字体大小为12\nplt.xlabel(\"Performance Percentile\", fontsize=13, fontweight='bold')\nplt.ylabel(\"4pt-Homography RMSE\", fontsize=13, fontweight='bold')\nplt.xlim(0.9, 6.1)  # 设置x轴的范围\nplt.ylim(1.5, 16)\n\n# plt.legend()          #显示各曲线的图例\nplt.legend(loc=0, numpoints=1)\nleg = plt.gca().get_legend()\nltext = leg.get_texts()\nplt.setp(ltext, fontsize=12, fontweight='bold')  # 设置图例字体的大小和粗细\n\nplt.savefig('./filename.svg', format='svg')  # 建议保存为svg格式,再用inkscape转为矢量图emf后插入word中\nplt.show()\n```\n\n![](/img/2018-04-05-matplotlib-18.png)\n\n>参考 [Matplotlib画各种论文图](https://blog.csdn.net/bskfnvjtlyzmv867/article/details/80352891)\n\n### 实例 2\n\n```python\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimport numpy as np\n\nfigure(num=None, figsize=(2.8, 1.7), dpi=300)\n# figsize的2.8和1.7指的是英寸，dpi指定图片分辨率。那么图片就是（2.8*300）*（1.7*300）像素大小\n\nplt.plot(test_mean_1000S_n, 'royalblue', label='without threshold')\nplt.plot(test_mean_1000S, 'darkorange', label='with threshold')\n# 画图，并指定颜色\n\nplt.xticks(fontproperties = 'Times New Roman', fontsize=8)\nplt.yticks(np.arange(0, 1.1, 0.2), fontproperties = 'Times New Roman', fontsize=8)\n# 指定横纵坐标的字体以及字体大小，记住是fontsize不是size。yticks上我还用numpy指定了坐标轴的变化范围。\n\nplt.legend(loc='lower right', prop={'family':'Times New Roman', 'size':8})\n# 图上的legend，记住字体是要用prop以字典形式设置的，而且字的大小是size不是fontsize，这个容易和xticks的命令弄混\n\nplt.title('1000 samples', fontdict={'family' : 'Times New Roman', 'size':8})\n# 指定图上标题的字体及大小\n\nplt.xlabel('iterations', fontdict={'family' : 'Times New Roman', 'size':8})\nplt.ylabel('accuracy', fontdict={'family' : 'Times New Roman', 'size':8})\n# 指定横纵坐标描述的字体及大小\n\nplt.savefig('F:/where-you-want-to-save.png', dpi=300, bbox_inches=\"tight\")\n# 保存文件，dpi指定保存文件的分辨率\n# bbox_inches=\"tight\" 可以保存图上所有的信息，不会出现横纵坐标轴的描述存掉了的情况\n\nplt.show()\n# 记住，如果你要show()的话，一定要先savefig，再show。如果你先show了，存出来的就是一张白纸。\n```\n\n![](/img/2018-04-05-matplotlib-19.png)\n\n>参考 [期刊论文写作之【python matplotlib 画图设置】](https://blog.csdn.net/qq_22522663/article/details/87916573)\n\n### 实例 3\n\n```python\nimport matplotlib.pyplot as plt\nfigure, ax = plt.subplots()\nitem = ['A','B','C']\nnum1 = [2.5, 2.6, 2.7]\nnum2 = [2.75, 2.85, 2.95]\nx=[1,2,3]\n\nplt.plot(x,num1,label='a',linestyle='--',color='r',marker='D')\nplt.plot(x,num2,label='b',linestyle='--',color='b',marker='o')\nplt.yticks([2.4,2.6,2.8,3.0])  #设置x,y坐标值\nplt.xticks(x)\n\nplt.tick_params(labelsize=16)\nlabels = ax.get_xticklabels() + ax.get_yticklabels()\n[label.set_fontname('Times New Roman') for label in labels]\nfont1 = {'family' : 'Times New Roman',\n'weight' : 'normal',\n'size'   : 16,\n}\nplt.legend(prop=font1,loc=4)\n\nplt.grid(axis=\"y\")\nplt.xlabel('Item',font1)\nplt.ylabel('Value',font1)\nplt.title('Line Chart',font1)\nplt.show()\n```\n\n![](/img/2018-04-05-matplotlib-20.png)\n\n### CNN 结构图\n\n>[如何用 matplotlib 画论文中的CNN结构图](https://blog.csdn.net/weixin_42180950/article/details/86770801)\n\n\n# 图片处理\n\n## 导入模块\n\n```python\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n```\n\n## 基本信息\n \n```python\nimg = Image.open('1.jpg')\n\nprint(type(img))\n<class 'PIL.JpegImagePlugin.JpegImageFile'>\n\nprint(img)\n<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=28x28 at 0x7F153040C6D8>\n\nprint(img.shape)\nAttributeError: 'JpegImageFile' object has no attribute 'shape'\n```\n\n```python\nimg_1 = np.array(Image.open('1.jpg')).astype('float')\n\nprint(img_1.shape)\n(28, 28, 3)\n\nimg_2 = np.array(Image.open('1.jpg').convert('L')).astype('float')\nprint(img_2.shape)\n(28, 28)\n```\n\n\n\n## 显示单个图片\n```python\nplt.figure(\"Image\") # 图像窗口名称\nplt.imshow(img)\nplt.axis('off') # 关掉坐标轴为 off\nplt.title('image') # 图像题目\nplt.show()\n```\n## 显示灰度图像\n\n```python\nimg = img.convert('L')  #变为灰度图像\n\nplt.figure(\"Image\")\n# 这里必须加 cmap='gray' ,否则尽管原图像是灰度图（下图1），但是显示的是伪彩色图像（下图2）（如果不加的话）\nplt.imshow(img,cmap='gray')\nplt.axis('off')\nplt.title('image')\nplt.show()\n```\n\n","tags":["python 模块"],"categories":["python"]},{"title":"mysql 基础操作（一）","url":"/2018/04/04/mysql_1/","content":"\n\n\n\n## 基础\n### 启动\n```\nservice mysqld start  #开启 centos7以下版本\nchkconfig mysqld on   #设置开机自启\n```\n### 连接数据库\n\n```\n本地连接：\nmysql -u用户名 -p\n输入密码\n\n远程连接：\nmysql  -hIP地址  -P端口 -u用户 -p\n输入密码\n```\n\n\n### sql及其规范\n1. 在数据库系统中，**SQL语句不区分大小写(建议用大写)** 。但**字符串常量区分大小写**。<u>建议命令大写，数据库名称、表名称、字段名称全部小写，用反引号括起来</u>；\n\n2. SQL语句可单行或多行书写，以“;”结尾。关键词不能跨多行或简写。\n\n3. 用空格和缩进来提高语句的可读性。子句通常位于独立行，便于编辑，提高可读性。\n\n4. 单行注释：`--`  多行注释：`/*......*/`\n\n<!-- more -->\n\n## 语句\n\n### 数据库\n\n#### 创建库\n\n```\nCREATE  {DATABASE | SCHEMA} [IF NOT EXISTS] db_name\n```\n```\nmysql> CREATE DATABASE IF NOT EXISTS `mydb`;\n```\n\n#### 查看有哪些数据库：\n\n```\nSHOW DATABASES;\n```\n```\nmysql> SHOW DATABASES;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |\n| performance_schema |\n| test               |\n+--------------------+\n```\n\n#### 删除数据库\n\n```\nDROP {DATABASE | SCHEMA} [IF EXISTS] dbname;\n```\n```\nmysql> DROP DATABASE `mydb`;\nmysql> DROP DATABASE IF EXISTS `mydb`;\n```\n\n#### 打开数据库\n\n```\nUSE 数据库名称\n```\n\n#### 判断在哪个数据库里\n\n```\nSELECT DATABASE();\n```\n```\nmysql> SELECT DATABASE();\n+------------+\n| DATABASE() |\n+------------+\n| test       |\n+------------+\n1 row in set (0.00 sec)\n```\n\n### 数据类型\n\n![](/img/2018-04-04-mysql_1-1.jpg)\n\nCHAR的长度是固定的，而VARCHAR2的长度是可以变化的\n\n### 数据表\n\n#### 查看数据表列表\n```\nSHOW TABLES [FROM db_name]\n\nSHOW TABLES\n查看当前数据库中的数据表。\n\nSHOW TABLES FROM 'mysql'  \n查看mysql这个数据库中的数据表。\n```\n#### 查看数据表结构\n\n```\nDESCRIBE tb_name;\n\nSHOW COLUMNS FROM 'tb_name'; \n```\n```\nmysql> DESCRIBE `tb1`;\n+-------+-------------+------+-----+---------+-------+\n| Field | Type        | Null | Key | Default | Extra |\n+-------+-------------+------+-----+---------+-------+\n| id    | int(11)     | YES  |     | NULL    |       |\n| name  | varchar(20) | YES  |     | NULL    |       |\n+-------+-------------+------+-----+---------+-------+\n2 rows in set (0.00 sec)\n```\n\n#### 删除数据表\n```\nDROP TABLE 'tablename'; \n```\n\n#### 创建数据表\n\n```\nCREATE TABLE [IF NOT EXISTS] table_name(\n   column_name data_type,\n)\n```\n\n```\nmysql> CREATE TABLE `tb1`(\n    -> `id` INT,\n    -> `name` VARCHAR(20)\n    -> );\nQuery OK, 0 rows affected (0.02 sec)\n```\n\n\n#### 修改数据表结构\n\n##### 添加单列\n```\nALTER TABLE tb1_name ADD [COLUNM] col_name\ncolumn_definition [FIRST|AFTER col-name]\n```\n```\n例：\nmysql> ALTER TABLE `tb1`\n    -> ADD `age` INT\n    -> ;\nQuery OK, 0 rows affected (0.03 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\nmysql> ALTER TABLE `tb1`\n    -> ADD `number` INT FIRST\n    -> ;\nQuery OK, 0 rows affected (0.03 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n```\n\n##### 添加多列\n```\nALTER TABLE tbl_name ADD [COLUMN]\n(col_name column_definition,...)\n```\n```\n例：\nmysql> ALTER TABLE `tb1`\n    -> ADD (`aa` INT,\n    ->      `bb` INT,\n    ->      `cc` INT\n    -> );\nQuery OK, 0 rows affected (0.02 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n```\n##### 删除数据表中的列\n\n```\nALTER TABLE tbl_name DROP [COLUMN] col_name ;\n```\n\n```\n例：\nmysql> ALTER TABLE `tb1`\n    -> DROP `aa`\n    -> ;\nQuery OK, 0 rows affected (0.03 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n\nmysql> ALTER TABLE `tb1`\n    -> DROP `bb`,\n    -> DROP `cc`\n    -> ;\nQuery OK, 0 rows affected (0.03 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n```\n\n##### 向数据表中输入数据\n\n```\nINSERT [INTO] tb1_name [(col_name,..)] VALUES(val,...)\n```\n\n```\n例：\nmysql> INSERT INTO `tb1`(`id`,`name`)\n    -> VALUES(1,'rose'),\n    ->       (2,'taka')\n    -> ;\nQuery OK, 2 rows affected (0.00 sec)\nRecords: 2  Duplicates: 0  Warnings: 0\n```\n\n\n#### 查询数据\n\n```\nSELECT column_name,column_name\nFROM table_name\n[WHERE Clause]\n[LIMIT N][ OFFSET M]\n```\n```\nselect * from runoob_tbl;\n```","tags":["数据库","mysql"],"categories":["数据库"]},{"title":"pymysql 基础操作（用 python 操作数据库）","url":"/2018/04/04/pysql/","content":"\n\n\n## 创建数据表\n\n```python\n\nimport pymysql\n\n# 创建连接\nconn = pymysql.connect(host='120.78.75.65', port=3306,\n                     user='root', passwd='root', db='test')\n\n# 创建游标\ncursor = conn.cursor()\n\nsql = \"\"\"CREATE TABLE EMPLOYEE (\n         FIRST_NAME  CHAR(20) NOT NULL,\n         LAST_NAME  CHAR(20),\n         AGE INT,  \n         SEX CHAR(1),\n         INCOME FLOAT )\"\"\"\n\n# 执行SQL，并返回受影响行数，执行多次\neffect_row = cursor.execute(sql)\n\n# 提交，不然无法保存新建或者修改的数据\nconn.commit()\n# 关闭游标\ncursor.close()\n# 关闭数据库连接\nconn.close()\n```\n\n<!-- more -->\n\n\n## 查询数据\n\n```python\nimport pymysql\n \nconn = pymysql.connect(host='127.0.0.1', port=3306, user='root', passwd='root', db='test')\n\ncursor = conn.cursor()\ncursor.execute(\"select * from users\")\n \n# 获取剩余结果的第一行数据\nrow_1 = cursor.fetchone()\n\n# 获取剩余结果前n行数据\nrow_2 = cursor.fetchmany(3)\n \n# 获取剩余结果所有数据\nrow_3 = cursor.fetchall()\n \nconn.commit()\n\ncursor.close()\nconn.close()\n```\n\n\n\n","tags":["数据库","mysql"],"categories":["数据库"]},{"title":"Batch Normalization 学习笔记（转载）","url":"/2018/03/28/BatchNormalization/","content":"\n\n\n## 一、背景意义\n本篇博文主要讲解2015年深度学习领域，非常值得学习的一篇文献：《Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift》，这个算法目前已经被大量的应用，最新的文献算法很多都会引用这个算法，进行网络训练，可见其强大之处非同一般啊。\n\n近年来深度学习捷报连连、声名鹊起，随机梯度下架成了训练深度网络的主流方法。尽管随机梯度下降法对于训练深度网络简单高效，但是它有个毛病，就是需要我们人为的去选择参数，比如学习率、参数初始化、权重衰减系数、Drop out比例等。这些参数的选择对训练结果至关重要，以至于我们很多时间都浪费在这些的调参上。那么学完这篇文献之后，你可以不需要那么刻意的慢慢调整参数。BN算法（Batch Normalization）其强大之处如下：\n\n1. 你可以选择比较大的初始学习率，让你的训练速度飙涨。以前还需要慢慢调整学习率，甚至在网络训练到一半的时候，还需要想着学习率进一步调小的比例选择多少比较合适，现在我们可以采用初始很大的学习率，然后学习率的衰减速度也很大，因为这个算法收敛很快。当然这个算法即使你选择了较小的学习率，也比以前的收敛速度快，因为它具有快速训练收敛的特性；\n\n2. 你再也不用去理会过拟合中drop out、L2正则项参数的选择问题，采用BN算法后，你可以移除这两项了参数，或者可以选择更小的L2正则约束参数了，因为BN具有提高网络泛化能力的特性；\n\n3. 再也不需要使用使用局部响应归一化层了（局部响应归一化是Alexnet网络用到的方法，搞视觉的估计比较熟悉），因为BN本身就是一个归一化网络层；\n\n4. 可以把训练数据彻底打乱（防止每批训练的时候，某一个样本都经常被挑选到，文献说这个可以提高1%的精度，这句话我也是百思不得其解啊）。\n\n<!-- more -->\n\n开始讲解算法前，先来思考一个问题：我们知道在神经网络训练开始前，都要对输入数据做一个归一化处理，那么具体为什么需要归一化呢？归一化后有什么好处呢？原因在于神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。\n\n对于深度网络的训练是一个复杂的过程，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度。  \n\n\n我们知道网络一旦train起来，那么参数就要发生更新，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。我们把网络中间层在训练过程中，数据分布的改变称之为：“Internal  Covariate Shift”。Paper所提出的算法，就是要解决在训练过程中，中间层数据分布发生改变的情况，于是就有了Batch  Normalization，这个牛逼算法的诞生。\n\n## 二、初识BN(Batch  Normalization)\n\n### 1、BN概述\n\n就像激活函数层、卷积层、全连接层、池化层一样，BN(Batch Normalization)也属于网络的一层。在前面我们提到网络除了输出层外，其它层因为低层网络在训练的时候更新了参数，而引起后面层输入数据分布的变化。这个时候我们可能就会想，如果在每一层输入的时候，再加个预处理操作那该有多好啊，比如网络第三层输入数据X3(X3表示网络第三层的输入数据)把它归一化至：均值0、方差为1，然后再输入第三层计算，这样我们就可以解决前面所提到的“Internal Covariate Shift”的问题了。\n\n而事实上，paper的算法本质原理就是这样：在网络的每一层输入的时候，又插入了一个归一化层，也就是先做一个归一化处理，然后再进入网络的下一层。不过文献归一化层，可不像我们想象的那么简单，它是一个可学习、有参数的网络层。既然说到数据预处理，下面就先来复习一下最强的预处理方法：白化。\n\n### 2、预处理操作选择\n\n说到神经网络输入数据预处理，最好的算法莫过于白化预处理。然而白化计算量太大了，很不划算，还有就是白化不是处处可微的，所以在深度学习中，其实很少用到白化。经过白化预处理后，数据满足条件：a、特征之间的相关性降低，这个就相当于pca；b、数据均值、标准差归一化，也就是使得每一维特征均值为0，标准差为1。如果数据特征维数比较大，要进行PCA，也就是实现白化的第1个要求，是需要计算特征向量，计算量非常大，于是为了简化计算，作者忽略了第1个要求，仅仅使用了下面的公式进行预处理，也就是近似白化预处理：\n\n![](/img/2018-03-28-BatchNormalization-1.jpg)  \n\n\n公式简单粗糙，但是依旧很牛逼。因此后面我们也将用这个公式，对某一个层网络的输入数据做一个归一化处理。需要注意的是，我们训练过程中采用batch 随机梯度下降，上面的E(xk)指的是每一批训练数据神经元xk的平均值；然后分母就是每一批数据神经元xk激活度的一个标准差了。\n\n## 三、BN算法实现\n\n### 1、BN算法概述\n\n经过前面简单介绍，这个时候可能我们会想当然的以为：好像很简单的样子，不就是在网络中间层数据做一个归一化处理嘛，这么简单的想法，为什么之前没人用呢？然而其实实现起来并不是那么简单的。其实如果是仅仅使用上面的归一化公式，对网络某一层A的输出数据做归一化，然后送入网络下一层B，这样是会影响到本层网络A所学习到的特征的。打个比方，比如我网络中间某一层学习到特征数据本身就分布在S型激活函数的两侧，你强制把它给我归一化处理、标准差也限制在了1，把数据变换成分布于s函数的中间部分，这样就相当于我这一层网络所学习到的特征分布被你搞坏了，这可怎么办？于是文献使出了一招惊天地泣鬼神的招式：变换重构，引入了可学习参数γ、β，这就是算法关键之处：\n\n![](/img/2018-03-28-BatchNormalization-2.jpg) \n\n每一个神经元xk都会有一对这样的参数γ、β。这样其实当：\n\n![](/img/2018-03-28-BatchNormalization-3.jpg)、![](/img/2018-03-28-BatchNormalization-4.jpg)  \n\n\n是可以恢复出原始的某一层所学到的特征的。因此我们引入了这个可学习重构参数γ、β，让我们的网络可以学习恢复出原始网络所要学习的特征分布。最后Batch Normalization网络层的前向传导过程公式就是：\n\n ![](/img/2018-03-28-BatchNormalization-5.jpg)\n\n上面的公式中m指的是mini-batch size。\n\n### 2、源码实现\n\n\n```python\nm = K.mean(X, axis=-1, keepdims=True)#计算均值  \nstd = K.std(X, axis=-1, keepdims=True)#计算标准差  \nX_normed = (X - m) / (std + self.epsilon)#归一化  \nout = self.gamma * X_normed + self.beta#重构变换  \n```\n\n上面的x是一个二维矩阵，对于源码的实现就几行代码而已，轻轻松松。  \n\n\n### 3、实战使用\n\n(1)可能学完了上面的算法，你只是知道它的一个训练过程，一个网络一旦训练完了，就没有了min-batch这个概念了。测试阶段我们一般只输入一个测试样本，看看结果而已。因此测试样本，前向传导的时候，上面的均值u、标准差σ 要哪里来？其实网络一旦训练完毕，参数都是固定的，这个时候即使是每批训练样本进入网络，那么BN层计算的均值u、和标准差都是固定不变的。我们可以采用这些数值来作为测试样本所需要的均值、标准差，于是最后测试阶段的u和σ 计算公式如下：\n\n![](/img/2018-03-28-BatchNormalization-6.jpg)  \n\n\n上面简单理解就是：对于均值来说直接计算所有batch u值的平均值；然后对于标准偏差采用每个batch σB的无偏估计。最后测试阶段，BN的使用公式就是：\n\n![](/img/2018-03-28-BatchNormalization-7.jpg)  \n\n\n(2)根据文献说，BN可以应用于一个神经网络的任何神经元上。文献主要是把BN变换，置于网络激活函数层的前面。在没有采用BN的时候，激活函数层是这样的：\n\n`z=g(Wu+b)`\n\n\n也就是我们希望一个激活函数，比如s型函数s(x)的自变量x是经过BN处理后的结果。因此前向传导的计算公式就应该是：\n\n`z=g(BN(Wu+b))`\n\n其实因为偏置参数b经过BN层后其实是没有用的，最后也会被均值归一化，当然BN层后面还有个β参数作为偏置项，所以b这个参数就可以不用了。因此最后把BN层+激活函数层就变成了：\n\n`z=g(BN(Wu))`\n\n## 四、Batch Normalization在CNN中的使用\n\n通过上面的学习，我们知道BN层是对于每个神经元做归一化处理，甚至只需要对某一个神经元进行归一化，而不是对一整层网络的神经元进行归一化。既然BN是对单个神经元的运算，那么在CNN中卷积层上要怎么搞？假如某一层卷积层有6个特征图，每个特征图的大小是100*100，这样就相当于这一层网络有6*100*100个神经元，如果采用BN，就会有6*100*100个参数γ、β，这样岂不是太恐怖了。因此卷积层上的BN使用，其实也是使用了类似权值共享的策略，把一整张特征图当做一个神经元进行处理。\n\n卷积神经网络经过卷积后得到的是一系列的特征图，如果min-batch sizes为m，那么网络某一层输入数据可以表示为四维矩阵(m,f,p,q)，m为min-batch sizes，f为特征图个数，p、q分别为特征图的宽高。在cnn中我们可以把每个特征图看成是一个特征处理（一个神经元），因此在使用Batch Normalization，mini-batch size 的大小就是：m*p*q，于是对于每个特征图都只有一对可学习参数：γ、β。说白了吧，这就是相当于求取所有样本所对应的一个特征图的所有神经元的平均值、方差，然后对这个特征图神经元做归一化。下面是来自于keras卷积层的BN实现一小段主要源码：\n\n\n\n```python\ninput_shape = self.input_shape  \n reduction_axes = list(range(len(input_shape)))  \n del reduction_axes[self.axis]  \n broadcast_shape = [1] * len(input_shape)  \n broadcast_shape[self.axis] = input_shape[self.axis]  \n if train:  \n     m = K.mean(X, axis=reduction_axes)  \n     brodcast_m = K.reshape(m, broadcast_shape)  \n     std = K.mean(K.square(X - brodcast_m) + self.epsilon, axis=reduction_axes)  \n     std = K.sqrt(std)  \n     brodcast_std = K.reshape(std, broadcast_shape)  \n     mean_update = self.momentum * self.running_mean + (1-self.momentum) * m  \n     std_update = self.momentum * self.running_std + (1-self.momentum) * std  \n     self.updates = [(self.running_mean, mean_update),  \n                     (self.running_std, std_update)]  \n     X_normed = (X - brodcast_m) / (brodcast_std + self.epsilon)  \n else:  \n     brodcast_m = K.reshape(self.running_mean, broadcast_shape)  \n     brodcast_std = K.reshape(self.running_std, broadcast_shape)  \n     X_normed = ((X - brodcast_m) /  \n                 (brodcast_std + self.epsilon))  \n out = K.reshape(self.gamma, broadcast_shape) * X_normed + K.reshape(self.beta, broadcast_shape)  \n```\n\n个人总结：2015年个人最喜欢深度学习的一篇paper就是Batch Normalization这篇文献，采用这个方法网络的训练速度快到惊人啊，感觉训练速度是以前的十倍以上，再也不用担心自己这破电脑每次运行一下，训练一下都要跑个两三天的时间。另外这篇文献跟空间变换网络《Spatial Transformer Networks》的思想神似啊，都是一个变换网络层。\n\n**参考文献：**\n\n1. 《Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift》\n2. 《Spatial Transformer Networks》\n3. [https://github.com/fchollet/keras](https://github.com/fchollet/keras)\n\n\n\n**原文地址**：[http://blog.csdn.net/hjimce/article/details/50866313](http://blog.csdn.net/hjimce/article/details/50866313)\n\n**作者**：hjimce\n\n\n  \n\n","tags":["深度学习"],"categories":["深度学习"]},{"title":"TensorFlow 实现 mnist 手写字识别","url":"/2018/03/26/mnist手写字识别/","content":"TensorFlow 实现 mnist 手写字识别\n\n```python\n# 加载TF 并加载数据\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n#设置输入参数\nbatch_size = 128\ntest_size = 256\n\n\n# 初始化权值与定义网络结构，建构一个3个卷积层和3个池化层，一个全连接层和一个输出层的卷积神经网络\n# 首先定义初始化权重函数\ndef init_weights(shape):\n    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n\n\n# 第一组卷积层以及池化层，最后　droupout是为了防止过拟合，在模型训练的时候丢掉一些神经元\n# padding表示对边界的处理，SAME表示卷积的输入和输出保持同样尺寸\ndef model(X, w, w2, w3, w4, w_o, p_keep_conv, p_keep_hidden):\n    l1a = tf.nn.relu(tf.nn.conv2d(X, w,\n                                  strides=[1, 1, 1, 1], padding='SAME'))\n    l1 = tf.nn.max_pool(l1a, ksize=[1, 2, 2, 1],  # l1 shape=(?, 14, 14, 32)\n                        strides=[1, 2, 2, 1], padding='SAME')\n    l1 = tf.nn.dropout(l1, p_keep_conv)\n\n    # 第二组卷积层及池化层，最后dropout一些神经元\n    l2a = tf.nn.relu(tf.nn.conv2d(l1, w2,  # l2a shape=(?, 14, 14, 64)\n                                  strides=[1, 1, 1, 1], padding='SAME'))\n    l2 = tf.nn.max_pool(l2a, ksize=[1, 2, 2, 1],  # l2 shape=(?, 7, 7, 64)\n                        strides=[1, 2, 2, 1], padding='SAME')\n    l2 = tf.nn.dropout(l2, p_keep_conv)\n\n    # 第三组卷积神经网络及池化层，同样，最后dropout一些神经元\n    l3a = tf.nn.relu(tf.nn.conv2d(l2, w3,  # l3a shape=(?, 7, 7, 128)\n                                  strides=[1, 1, 1, 1], padding='SAME'))\n    l3 = tf.nn.max_pool(l3a, ksize=[1, 2, 2, 1],  # l3 shape=(?, 4, 4, 128)\n                        strides=[1, 2, 2, 1], padding='SAME')\n    l3 = tf.reshape(l3, [-1, w4.get_shape().as_list()[0]])  # reshape to (?, 2048)\n    l3 = tf.nn.dropout(l3, p_keep_conv)\n    # 全连接层\n    l4 = tf.nn.relu(tf.matmul(l3, w4))\n    l4 = tf.nn.dropout(l4, p_keep_hidden)\n    # 输出层\n    pyx = tf.matmul(l4, w_o)\n    return pyx\n\n\n# 导入数据\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n# 定义四个变量，分别为输入训练图像矩阵及其标签，输入测试图像矩阵及其标签\ntrX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels\n# -1表示布考虑输入图片的数量，28*28为图片的像素数，1是通道(channel)的数量，\n# 因MNIST图片为黑白，彩色图片通道是3\ntrX = trX.reshape(-1, 28, 28, 1)  # 28x28x1\nteX = teX.reshape(-1, 28, 28, 1)  # 28x28x1\n\nX = tf.placeholder(\"float\", [None, 28, 28, 1])\nY = tf.placeholder(\"float\", [None, 10])  # 10为识别图片的类别从0到9，共10个取值\n\n# 定义模型函数\n# 神经网络模型的构建函数，传入以下参数\n# X：输入数据\n# w: 每一层权重\nw = init_weights([3, 3, 1, 32])  # 大小为3*3，输入的维度为1 ，输出维度为32\nw2 = init_weights([3, 3, 32, 64])  # 大小为3*3,输入维度为32，输出维度为64\nw3 = init_weights([3, 3, 64, 128])  # 大小为3*3,输入维度为64，输出维度为128\nw4 = init_weights([128 * 4 * 4, 625])  # 全连接层，输入维度为128*4*4,也就是上一层的输出，输出维度为625\nw_o = init_weights([625, 10])  # 输出层，输入的维度为625， 输出110维，代表10类（labels）\n\n# p_keep_conv,p_keep_hidden:dropout 保留神经元比例\n# 定义dropout的占位符keep_conv，表示一层中有多少比例的神经元被保留，生成网络模型，得到预测数据\n# 在训练的时候把设定比例的节点改为0，避免过拟合\np_keep_conv = tf.placeholder(\"float\")\np_keep_hidden = tf.placeholder(\"float\")\npy_x = model(X, w, w2, w3, w4, w_o, p_keep_conv, p_keep_hidden)\n\n# 定义损失函数，采用tf.nn.softmax_cross_entropy_with_logists，作为比较预测值和真实值的差距\n# 定义训练操作(train_op) 采用RMSProp算法作为优化器,\ncost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=py_x, labels=Y))\ntrain_op = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\npredict_op = tf.argmax(py_x, 1)\n\n\n#9.4.3训练模型和评估模型\n\n#在会话中定义图，开始训练和评估\n# Launch the graph in a session\nwith tf.Session() as sess:\n    # you need to initialize all variabels\n    tf.global_variables_initializer().run()\n\n    for i in range(100):\n        training_batch=zip(range(0,len(trX),batch_size),\n                       range(batch_size,len(trX)+1,batch_size))\n        for start, end in training_batch:\n            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end],\n                                         p_keep_conv: 0.8, p_keep_hidden: 0.5})\n                                       \n    test_indices = np.arange(len(teX)) # Get A Test Batch\n    np.random.shuffle(test_indices)\n    test_indices = test_indices[0:test_size]\n\n    print(i, np.mean(np.argmax(teY[test_indices], axis=1) ==\n                    sess.run(predict_op, feed_dict={X: teX[test_indices],\n                                                   p_keep_conv: 1.0,\n                                                   p_keep_hidden: 1.0})))\n                              #预测的时候设置为1 即对全部样本进行迭代训练\n```\n\n```\n# 贡献者：{沙舟}\n# 数据集下载地址：https://github.com/nlintz/TensorFlow-Tutorials/blob/master/05_convolutional_net.py\n# 代码来自于《TensorFlow技术解析与实战》\n```\n\n","tags":["深度学习","TensorFlow"],"categories":["TensorFlow"]},{"title":"numpy 总结（持续更新）","url":"/2018/03/26/numpy总结/","content":"\n\n## np.mgrid\n\n```python\n>>> x, y = np.mgrid[0:3,0:3]\n>>> x\narray([[0, 0, 0],\n       [1, 1, 1],\n       [2, 2, 2]])\n>>> y\narray([[0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2]])\n```\n\n## np.meshgrid\n\n```python\n>>> a,b=np.meshgrid(range(3),range(3))\n>>> a\narray([[0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2]])\n>>> b\narray([[0, 0, 0],\n       [1, 1, 1],\n       [2, 2, 2]])\n```\n\n## np.ceil()\n返回x的值上限 - 不小于x的最小整数\n\n```python\n>>> a = np.array([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0])\n>>> np.ceil(a)\narray([-1., -1., -0.,  1.,  2.,  2.,  2.])\n```\n\n## np.ravel() np.flatten()\n两者所要实现的功能是一致的（将多维数组降位一维）。\n\n两者的区别在于返回拷贝（copy）还是返回视图（view）\n\nnumpy.flatten()返回一份拷贝，对拷贝所做的修改不会影响（reflects）原始矩阵\n\n而numpy.ravel()返回的是视图（view，也颇有几分C/C++引用reference的意味），会影响（reflects）原始矩阵。\n\n<!-- more -->\n\n```python\nIn [14]: x=np.array([[1,2],[3,4]])\n\n# flattenh函数和ravel函数在降维时默认是行序优先\nIn [15]: x.flatten()\nOut[15]: array([1, 2, 3, 4])\n\nIn [17]: x.ravel()\nOut[17]: array([1, 2, 3, 4])\n```\n\n```python\n>>> x = np.array([[1, 2], [3, 4]])\n>>> x.flatten()[1] = 100\n>>> x\narray([[1, 2],\n       [3, 4]])            \n>>> x.ravel()[1] = 100\n>>> x\narray([[  1, 100],\n       [  3,   4]])\n```\n\n通过上面的程序可以发现flatten函数返回的是拷贝。\n\n\n## np.stack()\n\n```python\nimport numpy as np\na=[[1,2,3],\n   [4,5,6]]\nprint(\"列表a如下：\")\nprint(a)\n\nprint(\"增加一维，新维度的下标为0\")\nc=np.stack(a,axis=0)\nprint(c)\n\nprint(\"增加一维，新维度的下标为1\")\nc=np.stack(a,axis=1)\nprint(c)\n\n输出：\n列表a如下：\n[[1, 2, 3], [4, 5, 6]]\n增加一维，新维度下标为0\n[[1 2 3]\n [4 5 6]]\n增加一维，新维度下标为1\n[[1 4]\n [2 5]\n [3 6]]\n```\n\n## np.get_dummies()\n\n一种常用于统计建模或机器学习的转换方式是：将分类变量（categorical variable）转换为“哑变量矩阵”（dummy matrix）或“指标矩阵”（indicator matrix）。如果DataFrame的某一列中含有k个不同的值，则可以派生出一个k列矩阵或DataFrame（其值全为1和0）。pandas有一个get_dummies函数可以实现该功能（其实自己动手做一个也不难）。\n\n\n```python\nIn [72]: df = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],\n   ....:                    'data1': range(6)})\n\nIn [73]: pd.get_dummies(df['key'])\nOut[73]: \n   a  b  c\n0  0  1  0\n1  0  1  0\n2  1  0  0\n3  0  0  1\n4  1  0  0\n5  0  1  0\n\n[6 rows x 3 columns]\n```\n\n## np.expand_dims()\n\n`numpy.expand_dims(a, axis)`    \n\n增加一个维度，例如：`np.expand_dims(image_data, 0)` 可用于增加 batch 的维度\n\n```python\n>>> x = np.array([1,2])\n>>> x.shape\n(2,)\n\n>>> y = np.expand_dims(x, axis=0)\n>>> y\narray([[1, 2]])\n>>> y.shape\n(1, 2)\n\n>>> y = np.expand_dims(x, axis=1)  # Equivalent to x[:,newaxis]\n>>> y\narray([[1],\n       [2]])\n>>> y.shape\n(2, 1)\n\n>>> a.shape\n(3, 3)\n>>> np.expand_dims(a,-1).shape\n(3, 3, 1)\n```\n\n\n## np.newaxis\n\n## ravel() ","tags":["python 模块"],"categories":["python"]},{"title":"TensorFlow 入门（1）","url":"/2018/03/20/tensorflow入门1/","content":"\n\n## 计算 `f=x^2-2xy+3y^2=2x-2y` 最小时x与y的值\n\n```python\nimport tensorflow as tf\n\nx = tf.Variable(tf.constant(1.0))\ny = tf.Variable(tf.constant(2.0))\nloss = tf.square(x) - tf.multiply(tf.multiply(x, y), 2.0) + tf.multiply(3.0, tf.square(y)) + tf.multiply(2.0, x) - tf.multiply(2.0, y)\n\n\noptimizer = tf.train.GradientDescentOptimizer(0.05)\ntrain = optimizer.minimize(loss)\n\n# 初始化变量\ninit = tf.initialize_all_variables()\n\n# 启动图 (graph)\nsess = tf.Session()\nsess.run(init)\n\n\nfor step in range(0, 201):\n    sess.run(train)\n    if step % 20 == 0:\n        print( step, sess.run(x), sess.run(y))\n```\n<!-- more -->\n\n```\n0 1.0 1.6\n20 -0.320484 0.281646\n40 -0.796801 0.0841678\n60 -0.939242 0.0251667\n80 -0.981833 0.00752503\n100 -0.994568 0.00225003\n120 -0.998376 0.000672782\n140 -0.999514 0.000201154\n160 -0.999855 6.01531e-05\n180 -0.999956 1.80066e-05\n200 -0.999987 5.38831e-06\n220 -0.999996 1.60342e-06\n240 -0.999999 4.94772e-07\n260 -1.0 1.60986e-07\n280 -1.0 1.43105e-07\n300 -1.0 1.43105e-07\n```\n\n\n当 `f=x^2-2xy+3y^2=2x-2y` 最小时 `x = -1`  `y = 0`","tags":["深度学习","TensorFlow"],"categories":["TensorFlow"]},{"title":"卷积神经网络基础知识点","url":"/2018/02/21/CNN/","content":"\n\n## Padding\n\n如果我们有一个 \\\\(n × n\\\\) 的图像，用 \\\\(f × f\\\\) 的过滤 器做卷积，那么输出的维度就是 \\\\((n − f + 1) × (n − f + 1)\\\\) 。\n<!-- more -->\n\n>如果你用一个  \\\\(3×3\\\\)  的过滤器卷积一个  \\\\(6×6\\\\)  的图像，你最后会得 到一个  \\\\(4×4\\\\)  的输出，也就是一个  \\\\(4×4\\\\) 矩阵。那是因为你的  \\\\(3×3\\\\)  过滤器在  \\\\(6×6\\\\)  矩阵中，只可能有  \\\\(4×4\\\\)  种可能的位置\n\n习惯上，你可以用 0 去填充，如果  \\\\(p\\\\)  是填充的数量，输出也就变成了 \\\\((n + 2p − f + 1) × (n + 2p − f + 1)\\\\)   \n\n这样一来，丢失信息或者更准确来说角落或图像边缘的信息发挥的作用较小的这一缺点就被削弱了。   \n\n>沿着图像边缘再填充一层像素，那么 6×6 的图像就被填充成了一个 8×8 的图像。如果你用 3×3 的图像对这个 8×8 的图像卷积，你得到的输出就不是 4×4 的，而是 6×6 的图像，得到了一个尺寸和原始图像 6×6 的图像\n\n<br/>\n\n至于选择填充多少像素，通常有两个选择，分别叫做 `Valid` 卷积和 `Same` 卷积:\n\n1. `Valid` 卷积意味着不填充，这样的话，如果你有一个 \\\\(n × n\\\\) 的图像，用一个 \\\\(f × f\\\\) 的过滤器 卷积，它将会给你一个 \\\\((n − f + 1) × (n − f + 1)\\\\) 维的输出。\n2. 另一个经常被用到的填充方法叫做 `Same` 卷积，那意味你填充后，你的输出大小和输入 大小是一样的。让 \\\\(𝑛 + 2𝑝 − 𝑓 + 1 = 𝑛\\\\) 的话，使得输 出和输入大小相等，如果你用这个等式求解 \\\\(p\\\\) ，那么 \\\\(p =\\frac {𝑓 − 1}{2}\\\\) ，所以当 \\\\(f\\\\) 是一个奇数的时候，只要选择相应的填充尺寸，你就能确保得到和输入相同尺寸的输出。\n\n>**为什么用奇数维过滤器?**\n>\n>只有 \\\\(f\\\\) 是奇数的 情况下，`Same` 卷积才会有自然的填充，我们可以以同样的数量填充四周，而不是左边填充多一点，右边填充少一点，这样不对称的填充。\n>\n>第二个原因是当你有一个奇数维过滤器，比如 3×3 或者 5×5 的，它就有一个中心点。有 时在计算机视觉里，如果有一个中心像素点会更方便，便于指出过滤器的位置。\n\n<br/>\n\n## Strided convolutions\n\n\n如果你用一个 \\\\(𝑓 × 𝑓\\\\) 的过滤器卷积一个 \\\\(𝑓 × 𝑓\\\\) 的图像， 你的 padding 为 \\\\(p\\\\) ，步幅为 \\\\(s\\\\) ，现在不是一次移动一个步子，而是一次移动 \\\\(s\\\\) 个步子，输出于是变为 \\\\(({\\frac {n+2p-f}{s}+1})×({\\frac {n+2p-f}{s}+1})\\\\) \n\n如果商不是一个整数怎么办?在这种情况下，我们向下 取整。⌊ ⌋ 这是向下取整的符号，这也叫做对 𝑧 进行地板除(floor)，这意味着 𝑧 向下取整到最近的整数。  \n这个原则实现的方式是，只在蓝框完全包括在图像或填充完的图像内部时，才对它进行运算。如果有任意一个蓝框移动到了外面，那你就不要进行相乘操作，这是一个惯例。\n\n总的来说： \\\\(⌊{\\frac {n+2p-f}{s}+1}⌋×⌊{\\frac {n+2p-f}{s}+1}⌋\\\\) \n\n<br/>\n\n## 多维卷积\n\n![](/img/2018-02-21-CNN-1.jpg)\n\n<br/>\n\n## 池化层\n\n如果输入是三维的，那么输出也是三维的， \\\\(n_c\\\\) 个通道中每个通道都单独执行最大池化计算。\n\n\n池化的超级参数包括过滤器大小𝑓和步幅𝑠，常用的参数值为 \\\\(𝑓 = 2，𝑠 = 2\\\\) ， 应用频率非常高，其效果相当于高度和宽度缩减一半。也有使用 \\\\(𝑓 = 3，𝑠 = 2\\\\) 的情况。\n\n最大池化时，往往很少用到超参数 padding，当 然也有例外的情况。\n\n大部分情况下，最大池化很少用 padding。目前 \\\\(𝑝\\\\) 最常用\n的值是 0，即  \\\\(p=0\\\\) 。最大池化的输入就是 \\\\(𝑛\\_𝐻 × 𝑛\\_𝑊 × 𝑛\\_𝑐\\\\) ，假设没有 padding，则输出 \\\\(⌊\\frac {𝑛\\_𝑤−𝑓}{s} + 1⌋ × ⌊\\frac {𝑛\\_𝑤−𝑓}{s} + 1⌋ × 𝑛\\_𝑐 \\\\) 。\n\n输入通道与输出通道个数相同，因为我们对每个通道都做了池化。需 𝑠\n要注意的一点是，池化过程中没有需要学习的参数。执行反向传播时，反向传播没有参数适用于最大池化。\n\n<br/>\n\n## 1 × 1 卷积\n\n \\\\(1×1\\\\)  的卷积能做什么呢?不就是乘以数字么?听上去挺好笑的，结果并非如此。\n \n对于单通道的图片来说，  \\\\(1×1\\\\)  卷积的确是把数字乘以一个数，效果不佳。\n\n对于多通道的图片，使用\\\\(1×1\\\\)  卷积效果会更好。具体来说，\\\\(1×1\\\\)  卷积所实现的功能是遍历这  \\\\(n\\_c\\\\)(通道数)  个单元格，计算每一格中 \\\\(n\\_c\\\\) 个数字的元素积之和，然后应用 ReLU 非线性函数。\n\n![](/img/2018-02-21-CNN-2.jpg)\n\n\n**举个  \\\\(1×1\\\\)  卷积的例子**    \n\n>假设一个  \\\\(28×28×192\\\\)  的输入层，你可以使用池化层压缩它的高度和宽度。但如果通道数量很大，该如何把它压缩为  \\\\(28×28×32\\\\)  维度的层呢?\n>\n>你可以用  \\\\(32\\\\)  个大小为  \\\\(1×1\\\\)  的过滤器，严格来讲每个过滤器大小都是  \\\\(1×1×192\\\\)  维，**因为过滤器中通道数量 必须与输入层中通道的数量保持一致**。但是你使用了  \\\\(32\\\\)  个过滤器，输出层为  \\\\(28×28×32\\\\)    \n>\n>这就是**压缩通道数**(  \\\\(𝑛\\_𝑐\\\\) )的方法，对于池化层我只是压缩了这些层的高度和宽度。\n>\n>当然如果 你想保持通道数 192 不变，这也是可行的，1×1 卷积只是**添加了非线性函数，从而减少或保持输入层中的通道数量不变**\n\n\n<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>","tags":["深度学习"],"categories":["深度学习"]},{"title":"argparse 模块总结","url":"/2018/01/31/argparse/","content":"\n\n\n\n\n\n## 使用步骤：\n1. `import argparse`\n2. `parser = argparse.ArgumentParser()`\n3. `parser.add_argument()`\n4. `parser.parse_args()`\n\n- 首先导入该模块\n- 然后创建一个解析对象\n- 然后向该对象中添加你要关注的命令行参数和选项，每一个add_argument方法对应一个你要关注的参数或选项\n- 最后调用parse_args()方法进行解析；解析成功之后即可使用\n\n<!-- more -->\n## add_argument() 常用的参数\n\n\n```\n方法add_argument(name or flags...[, action][, nargs][, const][, default][, type][, choices][, required][, help][, metavar][, dest])\n```    \n   \n   \n`name or flags`：命令行参数名或者选项，如上面的address或者-p,--port.其中命令行参数如果没给定，且没有设置defualt，则出错。但是如果是选项的话，则设置为None\n\n`nargs`：命令行参数的个数，一般使用通配符表示，其中，`?`表示只用一个，`*`表示0到多个，`+`表示至少一个\n\n`dest`：如果提供dest，例如dest=\"a\"，那么可以通过args.a访问该参数\n\n`default`：设置参数的默认值\n\n`version`：打印程序版本信息\n\n`type`：把从命令行输入的结果转成设置的类型\n\n`help`：参数命令的介绍\n\n\n\n## 示例\n\n\n```python\nimport argparse\n\n\ndef build_arg_parser():\n    parser = argparse.ArgumentParser(description='Compress the input image \\\n            using clustering')\n    parser.add_argument(\"--input-file\", dest=\"input_file\", required=True,\n            help=\"Input image\")\n    parser.add_argument(\"--num-bits\", dest=\"num_bits\", required=False,\n            type=int, help=\"Number of bits used to represent each pixel\")\n    return parser\n\n\nif __name__=='__main__':\n    args = build_arg_parser().parse_args()\n    input_file = args.input_file\n    num_bits = args.num_bits\n    \n    print(input_file)\n    print(num_bits)\n```\n\n在命令行中输入:\n\n```\n ~  python /Users/aaron/code/argparse1.py --input-file /Users/aaron/Desktop/1.png --num-bits 8\n```\n得到：\n\n```\n/Users/aaron/Desktop/p/下载.png\n8\n```","tags":["python 模块"],"categories":["python"]},{"title":"sklearn 使用总结","url":"/2018/01/24/sklearn/","content":"\n## 1 常用算法模块\n\n```\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n```\n\n\n## 2 通用模式\n\n### 2.1 分为训练集和测试集\n\n```python\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n```\n\n<!-- more -->\n\n### 2.2 建立模型\n\n```\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\n```\n\n### 2.3 训练\n\n```\nknn.fit(X_train, y_train)\n```\n\n### 2.4 预测\n\n```\nknn.predict(X_test)\n```\n\n\n### 2.5 准确率\n\n#### 2.5.1 方法1\n```\nprint(knn.score(X_test, y_test))\n```\n#### 2.5.2 方法2\n\n```\nfrom sklearn.metrics import accuracy_score\n\nscore = accuracy_score(predict, test_labels)\n```\n\n\n\n## 3 数据预处理\n\n```python\nfrom sklearn import preprocessing\nimport numpy as np\n\n\n#建立Array\na = np.array([[10, 2.7, 3.6],\n              [-100, 5, -2],\n              [120, 20, 40]], dtype=np.float64)\n\n\n#将normalized后的a打印出\nprint(preprocessing.scale(a))\n# [[ 0.         -0.85170713 -0.55138018]\n#  [-1.22474487 -0.55187146 -0.852133  ]\n#  [ 1.22474487  1.40357859  1.40351318]]\n```\n\n### 3.1 标准正态分布\n\n>将数据转化为标准正态分布（均值为0，方差为1）\n\n```\npreprocessing.scale(X,axis=0, with_mean=True, with_std=True, copy=True)\n```\n\n### 3.2 数据缩放\n\n>将数据在缩放在固定区间，默认缩放到区间 [0, 1]\n\n\n```\npreprocessing.minmax_scale(X,feature_range=(0, 1), axis=0, copy=True)\n```\n\n### 3.3 归一化\n\n>将数据归一化到区间 [0, 1]，norm 可取值 'l1'、'l2'、'max'。可用于稀疏数据 scipy.sparse\n\n\n```\npreprocessing.normalize(X,norm='l2', axis=1, copy=True)\n```\n","tags":["机器学习"],"categories":["-机器学习"]},{"title":"用python对我和女票的聊天记录生成心形词云","url":"/2018/01/19/wordcloud/","content":"\n\n## 前言\n最近看到一些利用python制作词云的教程，突然想到用自己和女友的聊天记录做一个词云，看看平时我俩最常说的都是啥，然后用爱心的形状展示出来，以下是成品:\n![](/img/2018-01-19-wordcloud-1.jpg)\n由于导出的记录只有最近两个星期的，再加上这两个星期我女票她都在备考，因此聊天内容并不是特别多，数据可能不是特别有代表性，但至少也能看看了。\n    \n<!-- more -->\n\n## 数据处理\n\n首先我们从QQ中导出txt格式的聊天记录，并在python中打开\n\n```python\nf = open('/Users/aaron/文档/My one and only.txt')\nfl = f.readlines()\n```\n\n我们来查看一下数据：\n\n\n```\n['\\ufeff消息记录（此消息记录为文本格式，不支持重新导入）\\n',\n '\\n',\n '================================================================\\n',\n '消息分组:My one and only\\n',\n '================================================================\\n',\n '消息对象:xxx\\n',\n '================================================================\\n',\n '\\n',\n '2018-01-12 下午4:00:40 xxx\\n',\n '好丑操\\n',\n '\\n',\n '2018-01-12 下午4:00:49 xxx\\n',\n '好臭\\n',\n '\\n',\n '2018-01-12 下午4:00:50 xxx\\n',\n '好臭\\n',\n '\\n',\n '2018-01-12 下午4:01:27 xxx\\n',\n '我吃牛肉干\\n',\n '\\n',\n ```\n \n 可以看出前7行是头信息，下边的数据按照：\n - 时间，\n - 单句聊天记录，\n - '\\n'\n \n 每三行为一组，于是我们首先删去头信息\n\n`del fl[:8]`\n\n接下来我们只需要从下标为1开始，步长为3的聊天记录的数据：\n\n`fl = fl[1::3]`\n\n其中 `[1::3]` 的意思为下标为1开始，步长为3的切片，比如：\n```python\n>>> a = [0,1,2,3,4,5,6,7,8,9]\n>>> a[1::3]\n[1, 4, 7]\n>>> a[::3]\n[0, 3, 6, 9]\n>>> a[:5:2]\n[0, 2, 4]\n```\n\n之后的数据变为一个全部由聊天记录组成的列表：\n```\n['好丑操\\n',\n '好臭\\n',\n '好臭\\n',\n '我吃牛肉干\\n',\n '去去味\\n',\n '。。。。\\n',\n ...\n ]\n```\n我们将其组成一个字符串，使用 `' '.join(list)` 可以将一个列表组合成一个以空格为间隔的字符串：\n\n`strf = ' '.join(fl) `\n\n观察数据，发现记录中有非常多的杂质，例如 `/扯一扯`，`/糊脸`, `[放大招]`, `[表情]` 等，我们需要将这些杂质都去掉，于是导入re正则表达式模块：\n\n```python\nimport re\n```\n\n两种杂质，一种是以 `/` 开头，一种是 `[xx]` 形式，我们用两种正则表达式找出并转换为集合去掉重复元素 \n```\nlist1 = re.findall(r'/.{2,3}', strf)\nlist2 = re.findall(r'\\[.+?\\]', strf)\nset1 = set(list1)\nset2 = set(list2)\n```\n我们可以看到：\n\n![image](https://ws3.sinaimg.cn/large/006tNc79gy1fnm9fsfrwvj302i05v749.jpg)\n\n以及\n\n![](/img/2018-01-19-wordcloud-2.jpg)\n\n然后去掉这些杂质，因为有些出现频率太高会影响最后结果\n\n```python\nfor item in set1:\n    strf = strf.replace(item, '')\nfor item in set2:\n    strf = strf.replace(item, '')\n```\n\n还有要自己手动去掉两条：\n```\nstrf = strf.replace('请使用最新版本手机QQ查看', '')\nstrf = strf.replace('请使用最新版手机QQ体验新功能', '')\n```\n\n数据干净之后就可以制作词云了。\n\n## jieba库\n\n我们利用 `jieba` 库对记录进行分词操作，能将一个句子分为单个词语。我们对jieba做一个简单的了解，以下为官方文档中的一部分：\n\n`jieba.cut` 方法接受三个输入参数: 需要分词的字符串；cut_all 参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型\n\n代码示例:\n\n```python\n# encoding=utf-8\nimport jieba\n\nseg_list = jieba.cut(\"我来到北京清华大学\", cut_all=True)\nprint(\"Full Mode: \" + \"/ \".join(seg_list))  # 全模式\n\nseg_list = jieba.cut(\"我来到北京清华大学\", cut_all=False)\nprint(\"Default Mode: \" + \"/ \".join(seg_list))  # 精确模式\n\nseg_list = jieba.cut(\"他来到了网易杭研大厦\")  # 默认是精确模式\nprint(\", \".join(seg_list))\n\nseg_list = jieba.cut_for_search(\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")  # 搜索引擎模式\nprint(\", \".join(seg_list))\n```\n\n输出：\n\n```\n【全模式】: 我/ 来到/ 北京/ 清华/ 清华大学/ 华大/ 大学\n\n【精确模式】: 我/ 来到/ 北京/ 清华大学\n\n【新词识别】：他, 来到, 了, 网易, 杭研, 大厦    (此处，“杭研”并没有在词典中，但是也被Viterbi算法识别出来了)\n\n【搜索引擎模式】： 小明, 硕士, 毕业, 于, 中国, 科学, 学院, 科学院, 中国科学院, 计算, 计算所, 后, 在, 日本, 京都, 大学, 日本京都大学, 深造\n```\n\n若相对jieba进行更深的了解，可以 [点击此处](https://github.com/fxsjy/jieba)\n\n\n## wordcloud库\n\n我们使用wordcloud包生成词云图，首先了解一下其用法：\n\n```python\nclass wordcloud.WordCloud(font_path=None, width=400, height=200, margin=2, ranks_only=None, prefer_horizontal=0.9,mask=None, scale=1, color_func=None, max_words=200, min_font_size=4, stopwords=None, random_state=None,background_color='black', max_font_size=None, font_step=1, mode='RGB', relative_scaling=0.5, regexp=None, collocations=True,colormap=None, normalize_plurals=True)\n```\n\n\n```\nfont_path : string //字体路径，需要展现什么字体就把该字体路径+后缀名写上，如：font_path = '黑体.ttf'\n\nwidth : int (default=400) //输出的画布宽度，默认为400像素\n\nheight : int (default=200) //输出的画布高度，默认为200像素\n\nprefer_horizontal : float (default=0.90) //词语水平方向排版出现的频率，默认 0.9 （所以词语垂直方向排版出现频率为 0.1 ）\n\nmask : nd-array or None (default=None) //如果参数为空，则使用二维遮罩绘制词云。如果 mask 非空，设置的宽高值将被忽略，遮罩形状被 mask 取代。除全白（#FFFFFF）的部分将不会绘制，其余部分会用于绘制词云。如：bg_pic = imread('读取一张图片.png')，背景图片的画布一定要设置为白色（#FFFFFF），然后显示的形状为不是白色的其他颜色。可以用ps工具将自己要显示的形状复制到一个纯白色的画布上再保存，就ok了。\n\nscale : float (default=1) //按照比例进行放大画布，如设置为1.5，则长和宽都是原来画布的1.5倍。\n\nmin_font_size : int (default=4) //显示的最小的字体大小\n\nfont_step : int (default=1) //字体步长，如果步长大于1，会加快运算但是可能导致结果出现较大的误差。\n\nmax_words : number (default=200) //要显示的词的最大个数\n\nstopwords : set of strings or None //设置需要屏蔽的词，如果为空，则使用内置的STOPWORDS\n\nbackground_color : color value (default=”black”) //背景颜色，如background_color='white',背景颜色为白色。\n\nmax_font_size : int or None (default=None) //显示的最大的字体大小\n\nmode : string (default=”RGB”) //当参数为“RGBA”并且background_color不为空时，背景为透明。\n\nrelative_scaling : float (default=.5) //词频和字体大小的关联性\n\ncolor_func : callable, default=None //生成新颜色的函数，如果为空，则使用 self.color_func\n\nregexp : string or None (optional) //使用正则表达式分隔输入的文本\n\ncollocations : bool, default=True //是否包括两个词的搭配\n\ncolormap : string or matplotlib colormap, default=”viridis” //给每个单词随机分配颜色，若指定color_func，则忽略该方法。\n\n\n\nfit_words(frequencies)  //根据词频生成词云\ngenerate(text)  //根据文本生成词云\ngenerate_from_frequencies(frequencies[, ...])   //根据词频生成词云\ngenerate_from_text(text)    //根据文本生成词云\nprocess_text(text)  //将长文本分词并去除屏蔽词（此处指英语，中文分词还是需要自己用别的库先行实现，使用上面的 fit_words(frequencies) ）\nrecolor([random_state, color_func, colormap])   //对现有输出重新着色。重新上色会比重新生成整个词云快很多。\nto_array()  //转化为 numpy array\nto_file(filename)   //输出到文件\n```\n\n\n了解了这两个包之后，我们开始正式制作词云。\n\n## 制作词云图\n\n首先导入所需要的库：\n\n```python\nimport matplotlib.pyplot as plt\nimport jieba\nimport wordcloud\n```\n\n然后利用词云进行分词操作，并将生成的列表合并成字符串：\n\n```python\nword_list = jieba.cut(strf, cut_all=True)\nword = ' '.join(word_list)\n```\n\n之后利用wordcloud包，注意一定要加上中文字体的路径，因为wordcloud默认是英文字体，并不支持中文，我们只需自己指定字体即可，我这里使用的是宋体，并且指定背景颜色是白色。\n\n```python\nwc = wordcloud.WordCloud(font_path='/Library/Fonts/Songti.ttc', background_color='white').generate(word)\n```\n\n最后使用matplotlib进行绘制：\n\n```python\nplt.imshow(wc)\nplt.axis('off')\nplt.show()\n```\n\n词云图就生成好了：\n![](/img/2018-01-19-wordcloud-3.jpg)\n\n## 心形词云\n\n为了生成心形的词云，我们首先找一张心形的图片：\n\n![](/img/2018-01-19-wordcloud-4.jpg)\n\n\n然后：\n\n```python\nfrom scipy.misc import imread\n```\n\n加上mask参数后再次制作词云：\n\n```python\npic = imread('/Users/aaron/Pictures/aixin.png')\nwc = wordcloud.WordCloud(mask=pic, font_path='/Library/Fonts/Songti.ttc', width=1000, height=500, background_color='white').generate(word)\n\nplt.imshow(wc)\nplt.axis('off')\nplt.show()\n```\n\n心形词云图诞生！\n![](/img/2018-01-19-wordcloud-5.jpg)\n\n赶紧学一招然后发给自己的女朋友吧！\n\n或许不是最优方法，欢迎指导。\n\n## 代码\n最后附上全部代码：\n\n```python\nimport re\nimport matplotlib.pyplot as plt\nimport jieba\nimport wordcloud\nfrom scipy.misc import imread\n\n\n# 数据处理\nf = open('/Users/aaron/文档/My one and only.txt')   # 改成自己的聊天记录文件\nfl = f.readlines()\ndel fl[:8]\nfl = fl[1::3]\nstrf = ' '.join(fl)\nlist1 = re.findall(r'/.{2,3}', strf)\nlist2 = re.findall(r'\\[.+?\\]', strf)\nset1 = set(list1)\nset2 = set(list2)\nstrf = strf.replace('请使用最新版本手机QQ查看', '')\nstrf = strf.replace('请使用最新版手机QQ体验新功能', '')\nfor item in set1:\n    strf = strf.replace(item, '')\nfor item in set2:\n    strf = strf.replace(item, '')\n\n# 制作词云\nword_list = jieba.cut(strf, cut_all=True)\nword = ' '.join(word_list)\npic = imread('/Users/aaron/Downloads/aixin.png')    \nwc = wordcloud.WordCloud(mask=pic, font_path='/Library/Fonts/Songti.ttc', width=1000, height=500, background_color='white').generate(word)\n\nplt.imshow(wc)\nplt.axis('off')\nplt.show()\n\n```\n","tags":["python"],"categories":["python"]},{"title":"python 易错及易遗忘点总结（持续更新）","url":"/2018/01/16/python-易错及易遗忘点总结/","content":"\n\n\n## 1 收集参数\n```python\ndef test(*params, extra)\n```\n在调用时要用关键词参数来制定extra，不然python会把你的实参都列入收集参数的范畴。\n\n所以，**建议再有其他参数时，设置默认参数**，如：\n\n```python\ndef test(*params, extra = 8)\n```\n\n收集参数会被打包成一个元祖。\n\n有打包就有解包，如一个列表 `a = [1, 2, 3, 4, 5, 6]` 要作为参数放入收集参数，则要先解包，同样用`*`解包，不然会出错：\n```python\ntest(*a)\n```\n\n\n<!-- more -->\n## 2 函数变量作用域\n\n如果在函数内部试图修改全局变量，python会创建一个局部变量代替（名字一模一样），所以真正的全局变量是一点不变的。\n\n## 3 dict()\n`dict()` 的参数可以是一个序列，但不可以是多个，所以：\n```python\ndict((('a', 1), ('b', 2), ('c', 3)))  #注意括号个数\n```\n但可以：\n```python\ndict(one=1, two=2, three=3)\n```\n\n## 4 zip()\n\nzip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。\n\n如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用 * 号操作符，可以将元组解压为列表。\n\n```python\n>>>a = [1,2,3]\n>>> b = [4,5,6]\n>>> c = [4,5,6,7,8]\n>>> zipped = zip(a,b)     # 打包为元组的列表\n[(1, 4), (2, 5), (3, 6)]\n>>> zip(a,c)              # 元素个数与最短的列表一致\n[(1, 4), (2, 5), (3, 6)]\n>>> zip(*zipped)          # 与 zip 相反，可理解为解压，返回二维矩阵式\n[(1, 2, 3), (4, 5, 6)]\n```\n\n\n## 5 两个列表一起打乱\n\n```\nimport random\nc = list(zip(a, b))\nrandom.shuffle(c)\na[:], b[:] = zip(*c)\n```\n\n\n>`shuffle()` 方法将序列的所有元素随机排序\n\n```\nimport random\n\nlist = [20, 16, 10, 5];\nrandom.shuffle(list)\nprint(\"随机排序列表 : \",  list)\n\n随机排序列表 :  [16, 5, 10, 20]\n```\n\n>`zip()` 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。\n\n如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用 * 号操作符，可以将元组解压为列表。\n\n```\n>>>a = [1,2,3]\n>>> b = [4,5,6]\n>>> c = [4,5,6,7,8]\n>>> zipped = zip(a,b)     # 打包为元组的列表\n[(1, 4), (2, 5), (3, 6)]\n>>> zip(a,c)              # 元素个数与最短的列表一致\n[(1, 4), (2, 5), (3, 6)]\n>>> zip(*zipped)          # 与 zip 相反，可理解为解压，返回二维矩阵式\n[(1, 2, 3), (4, 5, 6)]\n```","tags":["python"],"categories":["python"]},{"title":"用python爬虫爬取moodle课件","url":"/2017/10/10/利用python下载moodle课件/","content":"\n\n\n# 1) 介绍\n`moodle(Modular Object-Oriented Dynamic Learning Environment)` 是一个开源的课程管理系统, 笔者作为一名香港中文大学(深圳)的学生, 教授们会把课件以及作业等都发布在 `moodle` 上, 因此学生需要时常登录 `moodle` 下载课件以及完成作业. 本篇文章将简短地介绍使用 `python` 爬取 `moodle` 上的全部课件并下载到本地, 分类保存, 省去学生登录, 选择, 下载的时间, 也能更方便的在本地管理.\n\n<!-- more -->\n# 2) code \n## 1. 引入模块\n本篇文章将使用 `requests` 模块来进行爬虫的实现, 正则表达式模块`re` 以及对本地文件进行操作 `os`\n```python\nimport requests\nimport re\nimport os\n```\n\n\n## 2. 登录moodle\n`requests.Session` 能够跨请求地保持某些参数，自动帮助我们处理cookies，很方便的就能够处理登录问题.    \n    \nmoodle没有设置反爬机制, 因此无需设置代理ip以及浏览器头部信息, 这给我们省去了不少麻烦.\n```python\nsession = requests.Session()\n\nurl_post_first_page = 'https://elearning.cuhk.edu.cn/login/index.php'  #香港中文大学(深圳)moodle登录url\n\n\n#由于我校moodle并没有对爬虫进行任何反爬的设置, 因此header内容可不要\n#header = {\n#    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n#    'Referer': 'http://i.cuhk.edu.cn/cloud/pcs/moodleLogin'\n#}\n\nusername = str(input('请输入用户名: '))\npassword = str(input('请输入密码: '))\npostdata = {\n    'username': username,\n    'password': password\n}\n\nhtml_post = session.post(url_post_first_page, postdata)   #提交表单\n```\n\n## 3. 正则表达式提取\n### a)  科目的提取\n 观察网页内容   \n \n ![1](https://ws1.sinaimg.cn/large/006tKfTcgy1fkcfvfe6nuj30l20a0abv.jpg)   \n \n ![2](https://ws4.sinaimg.cn/large/006tKfTcgy1fkcfwsoovij30i0062q3y.jpg)    \n   \n  我们使用如下表达式:`\"https://elearning\\.cuhk\\.edu\\.cn/course/view\\.php\\?id=(.+?)\"`    \n  \n  将爬取到的url储存到一个列表中, 再次单独一个一个访问来读取每一门科目下的内容.     \n```python\ndata = list(set(data))    #删除掉重复地址\n```   \n\n   \n### b) 文件夹的提取\n\n\n先单独访问每个科目对应的网站  (在外层套循环)\n```python\nurl_page = 'https://elearning.cuhk.edu.cn/course/view.php?id=' + str(num)  #num为科目对应id\npage = session.get(url_page)\n```   \n![3](https://ws1.sinaimg.cn/large/006tKfTcgy1fkcg8njqr3j30ia0katcs.jpg)   \n  \n\n 采用以下表达式   \n```python\n\"(https://elearning\\.cuhk\\.edu\\.cn/mod/folder/view\\.php\\?id=.+?)\"\n```   \n\n### c) 文件的提取\n\n```python\n\"(https://elearning\\.cuhk\\.edu\\.cn/mod/resource/view\\.php\\?id=.+?)\"\n```  \n或许大家已经发现规律了, 文件是 `resource`  文件夹是`folder`  科目是 `course`   \n  \n## 4. 下载\n\n我们定义一个函数来下载文件\n```python\ndef downloader(session, url, path):     #session作为参数传入, url是下载文件的地址, path是要保存的地址\n    file1 = session.get(url)\n    print('访问成功!开始下载')\n    file = open(path, 'wb')\n    file.write(file1.content)\n    name = path.split('/')[-1]        #对path进行切片来提取最后的文件名, 并提示已下载成功\n    print('下载成功: '+ name)\n    file.close()\n\n``` \n## 5. 其他 \n - 创建多个字典, 来保存url对应文件或者科目等\n - 由于存在多层文件夹的嵌套, 因此要注意保存路径的编写\n - 由于存在多种格式文件, 因此要在网页源码中自己判断好文件格式并下载\n\n\n\n# 3) 最后\n由于笔者水平有限, 上述代码必然有许多不优秀的地方, 为了观看方便, 省略很多杂七杂八的代码, 本篇文章只能起到一个大致的思路作用, 希望能带给一些刚刚接触爬虫的小伙伴们一点思路 ( 当然笔者也是刚刚接触爬虫的小菜鸟 )      \n\n因为自己的代码写的很乱, 还存在着一些小bug, 因此笔者就不贴上代码献丑了.   \n当然还可以通过其他多种思路去编写爬虫, 笔者也在和大家一起进步着.\n\n\n","tags":["python"],"categories":["python"]}]