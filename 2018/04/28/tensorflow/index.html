<!DOCTYPE html>
<html lang>
  <head><meta name="generator" content="Hexo 3.9.0">
    
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=0.5, maximum-scale=2.0, user-scalable=yes">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="description" content="TensorFlow 函数总结">




  <meta name="keywords" content="TensorFlow,">







  <link rel="alternate" href="/atom.xml" title="浩瀚宇宙·AaronChen">




  <link rel="shortcut icon" type="image/x-icon" href="/img/1.jpg?v=1.1">



<link rel="canonical" href="https://hhyz.me/2018/04/28/tensorflow/">


<meta name="description" content="tf 函数tf.splittf.split()：axis的意思就是输入张量的哪一个维度，如果是0就表示对第0维度进行切割。num_or_size_splits就是切割的数量，如果是2就表示输入张量被切成2份，每一份是一个列表。 1234567tf.split(    value,    num_or_size_splits,    axis=0,    num=None,    name=&apos;spl">
<meta name="keywords" content="TensorFlow">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorFlow 函数总结">
<meta property="og:url" content="https://hhyz.me/2018/04/28/tensorflow/index.html">
<meta property="og:site_name" content="浩瀚宇宙·AaronChen">
<meta property="og:description" content="tf 函数tf.splittf.split()：axis的意思就是输入张量的哪一个维度，如果是0就表示对第0维度进行切割。num_or_size_splits就是切割的数量，如果是2就表示输入张量被切成2份，每一份是一个列表。 1234567tf.split(    value,    num_or_size_splits,    axis=0,    num=None,    name=&apos;spl">
<meta property="og:locale" content="zh">
<meta property="og:updated_time" content="2019-07-02T17:05:20.632Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TensorFlow 函数总结">
<meta name="twitter:description" content="tf 函数tf.splittf.split()：axis的意思就是输入张量的哪一个维度，如果是0就表示对第0维度进行切割。num_or_size_splits就是切割的数量，如果是2就表示输入张量被切成2份，每一份是一个列表。 1234567tf.split(    value,    num_or_size_splits,    axis=0,    num=None,    name=&apos;spl">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1">
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">





<script type="text/javascript">
  var themeConfig = {
    fancybox: {
      enable: false
    },
  };
</script>




  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?3cbfa43ed54bb06d8da192dc667bed80";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  <script type="text/javascript">
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-139198240-1', 'auto');
        ga('send', 'pageview');
  </script>





  


    <title> TensorFlow 函数总结 - 浩瀚宇宙·AaronChen </title>


    <script>
(function(u, c) {
  var d = document, t = 'script', o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
  o.src = u;
  if (c) { o.addEventListener('load', function(e) { c(e); }); }
  s.parentNode.insertBefore(o, s);
})('//cdn.bootcss.com/pangu/3.3.0/pangu.min.js', function() {
  pangu.spacingPage();
});
</script>



<script type="text/javascript">
window.onload=
function(){
    var oDiv = document.getElementById("toc-test"),
    H = 0,
    Y = oDiv
    while (Y) {H += Y.offsetTop; Y = Y.offsetParent}
    window.onscroll = function()
    {
        var s = document.body.scrollTop || document.documentElement.scrollTop
        if(s>H) {
            oDiv.style = "position:fixed;top:0;"
        } else {
            oDiv.style = ""
        }
    }
}
</script>






  </head>

  <body>
    <div id="page">
      <header id="masthead"><div class="site-header-inner">
    <h1 class="site-title">
        <a href="/." class="logo">浩瀚宇宙·AaronChen</a>
    </h1>

    <nav id="nav-top">
        
            <ul id="menu-top" class="nav-top-items">
                
                    <li class="menu-item">
                        <a href="https://www.hhyz.me">
                            
                            
                                Gallery
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/about">
                            
                            
                                About
                            
                        </a>
                    </li>
                
            </ul>
        
  </nav>
</div>

      </header>
      <div id="content">
        
    <div id="primary">
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          TensorFlow 函数总结
        
      </h1>

      <time class="post-time">
          Apr 28 2018
      </time>
    </header>



    
            <div class="post-content">
            <h2 id="tf-函数"><a href="#tf-函数" class="headerlink" title="tf 函数"></a>tf 函数</h2><h3 id="tf-split"><a href="#tf-split" class="headerlink" title="tf.split"></a>tf.split</h3><p>tf.split()：axis的意思就是输入张量的哪一个维度，如果是0就表示对第0维度进行切割。num_or_size_splits就是切割的数量，如果是2就表示输入张量被切成2份，每一份是一个列表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.split(</span><br><span class="line">    value,</span><br><span class="line">    num_or_size_splits,</span><br><span class="line">    axis=<span class="number">0</span>,</span><br><span class="line">    num=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="string">'split'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>如果 num_or_size_splits 传入的是一个整数，这个整数代表这个张量最后会被切成几个小张量。此时，传入 axis 的数值就代表切割哪个维度（从0开始计数）。调用 tf.split(my_tensor, 2，0) 返回两个 10 * 30 * 40 的小张量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">A = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">a0 = tf.split(A, num_or_size_splits=<span class="number">3</span>, axis=<span class="number">1</span>)<span class="comment">#不改变维数（！！）</span></span><br><span class="line">a1 = tf.unstack(A, num=<span class="number">3</span>,axis=<span class="number">1</span>)</span><br><span class="line">a2 = tf.split(A, num_or_size_splits=<span class="number">2</span>, axis=<span class="number">0</span>)</span><br><span class="line">a3 = tf.unstack(A, num=<span class="number">2</span>,axis=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(a0))</span><br><span class="line">    print(sess.run(a1))</span><br><span class="line">    print(sess.run(a2))</span><br><span class="line">    print(sess.run(a3))</span><br><span class="line">       </span><br><span class="line">[array([[<span class="number">1</span>],[<span class="number">4</span>]]), array([[<span class="number">2</span>],[<span class="number">5</span>]]), array([[<span class="number">3</span>],[<span class="number">6</span>]])]</span><br><span class="line">[array([<span class="number">1</span>, <span class="number">4</span>]), array([<span class="number">2</span>, <span class="number">5</span>]), array([<span class="number">3</span>, <span class="number">6</span>])] </span><br><span class="line">[array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]), array([[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])] </span><br><span class="line">[array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]), array([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])]</span><br></pre></td></tr></table></figure>

<p>如果 num_or_size_splits 传入的是一个向量，那么向量有几个分量就分成几份，切割的维度还是由 axis 决定。比如调用 <code>tf.split(my\_tensor, [10, 5, 25], 2)</code>，则返回三个张量分别大小为 20 × 30 × 10、20 × 30 × 5、20 × 30 × 25。很显然，传入的这个向量各个分量加和必须等于 axis 所指示原张量维度的大小 (10 + 5 + 25 = 40)。</p>
<a id="more"></a>
<h3 id="tf-concat"><a href="#tf-concat" class="headerlink" title="tf.concat"></a>tf.concat</h3><p>连接两个（或多个）通道（矩阵）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.concat(</span><br><span class="line">    values,</span><br><span class="line">    axis,</span><br><span class="line">    name=<span class="string">'concat'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># axis：0表示行，1表示列</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; t1 = [[1, 2, 3], [4, 5, 6]]</span><br><span class="line">&gt;&gt;&gt; t2 = [[7, 8, 9], [10, 11, 12]]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; print(sess.run(tf.concat([t1, t2], 0)))</span><br><span class="line">[[ 1  2  3]</span><br><span class="line"> [ 4  5  6]</span><br><span class="line"> [ 7  8  9]</span><br><span class="line"> [10 11 12]]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; print(sess.run(tf.concat([t1, t2], 1)))</span><br><span class="line">[[ 1  2  3  7  8  9]</span><br><span class="line"> [ 4  5  6 10 11 12]]</span><br><span class="line"> </span><br><span class="line"># tensor t3 with shape [2, 3]</span><br><span class="line"># tensor t4 with shape [2, 3]</span><br><span class="line">tf.shape(tf.concat([t3, t4], 0))  # [4, 3]</span><br><span class="line">tf.shape(tf.concat([t3, t4], 1))  # [2, 6] </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; t1 = [[[1, 1, 1],[2, 2, 2]],[[3, 3, 3],[4, 4, 4]]]</span><br><span class="line">&gt;&gt;&gt; t2 = [[[5, 5, 5],[6, 6, 6]],[[7, 7, 7],[8, 8, 8]]]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; print(sess.run(tf.concat([t1, t2], 1)))</span><br><span class="line">[[[1 1 1]</span><br><span class="line">  [2 2 2]</span><br><span class="line">  [5 5 5]</span><br><span class="line">  [6 6 6]]</span><br><span class="line"></span><br><span class="line"> [[3 3 3]</span><br><span class="line">  [4 4 4]</span><br><span class="line">  [7 7 7]</span><br><span class="line">  [8 8 8]]]</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt; print(sess.run(tf.concat([t1, t2], 0)))</span><br><span class="line">[[[1 1 1]</span><br><span class="line">  [2 2 2]]</span><br><span class="line"></span><br><span class="line"> [[3 3 3]</span><br><span class="line">  [4 4 4]]</span><br><span class="line"></span><br><span class="line"> [[5 5 5]</span><br><span class="line">  [6 6 6]]</span><br><span class="line"></span><br><span class="line"> [[7 7 7]</span><br><span class="line">  [8 8 8]]]</span><br></pre></td></tr></table></figure>

<h3 id="tf-cond"><a href="#tf-cond" class="headerlink" title="tf.cond"></a>tf.cond</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cond （ </span><br><span class="line">    pred ， </span><br><span class="line">    true_fn = <span class="literal">None</span> ， </span><br><span class="line">    false_fn = <span class="literal">None</span> ， </span><br><span class="line">    strict = <span class="literal">False</span> ， </span><br><span class="line">    name = <span class="literal">None</span> ， </span><br><span class="line">    fn1 = <span class="literal">None</span> ， </span><br><span class="line">    fn2 = <span class="literal">None</span></span><br><span class="line"> ）</span><br></pre></td></tr></table></figure>

<p>如果断言 pred 为 true 则返回 true_fn() ，否则返回 false_fn()</p>
<p><strong>例子</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">a = tf.constant(<span class="number">2</span>)</span><br><span class="line">b = tf.constant(<span class="number">3</span>)</span><br><span class="line">x = tf.constant(<span class="number">4</span>)</span><br><span class="line">y = tf.constant(<span class="number">5</span>)</span><br><span class="line">z = tf.multiply(a, b)</span><br><span class="line">result = tf.cond(x &lt; y, <span class="keyword">lambda</span>: tf.add(x, z), <span class="keyword">lambda</span>: tf.square(y))</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">    print(result.eval())</span><br><span class="line">    print(z.eval())</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;<span class="number">10</span></span><br><span class="line">&gt;&gt;&gt;<span class="number">6</span></span><br></pre></td></tr></table></figure>

<p><strong>作用</strong></p>
<p>在 dropout 中判断是否在训练：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">self.is_training = tf.placeholder(tf.bool)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout_with_keep</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.dropout(conv_a, dropout_keep_prob)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout_no_keep</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.dropout(conv_a, <span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> dropout_keep_prob != <span class="number">-1</span>:</span><br><span class="line">    conv_o_dr = tf.cond(is_training, dropout_with_keep, dropout_no_keep)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    conv_o_dr = conv_a</span><br></pre></td></tr></table></figure>

<p><strong>或者：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'control'</span>):</span><br><span class="line">    <span class="comment"># it controls dropout and batch_norm layers</span></span><br><span class="line">    is_training = tf.placeholder_with_default(<span class="literal">True</span>, [], <span class="string">'is_training'</span>)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_dropout</span><span class="params">(X, is_training, rate=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    keep_prob = tf.constant(</span><br><span class="line">        <span class="number">1.0</span> - rate, tf.float32,</span><br><span class="line">        [], <span class="string">'keep_prob'</span></span><br><span class="line">    )</span><br><span class="line">    result = tf.cond(</span><br><span class="line">        is_training,</span><br><span class="line">        <span class="keyword">lambda</span>: tf.nn.dropout(X, keep_prob),</span><br><span class="line">        <span class="keyword">lambda</span>: tf.identity(X),</span><br><span class="line">        name=<span class="string">'dropout'</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>

<h3 id="tf-tile"><a href="#tf-tile" class="headerlink" title="tf.tile"></a>tf.tile</h3><p><code>tf.tile()</code> 应用于需要张量扩展的场景，具体说来就是：<br>如果现有一个形状如 <code>[width, height]</code> 的张量，需要得到一个基于原张量的，形状如 <code>[batch_size,width,height]</code> 的张量，其中每一个 batch 的内容都和原张量一模一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tile(</span><br><span class="line">    input,</span><br><span class="line">    multiples,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p><strong>示例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">temp = tf.tile([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">2</span>])</span><br><span class="line">temp2 = tf.tile([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>],[<span class="number">5</span>,<span class="number">6</span>]],[<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(temp))</span><br><span class="line">    	</span><br><span class="line">    	<span class="comment"># [1 2 3 1 2 3]</span></span><br><span class="line">    </span><br><span class="line">temp = tf.tile([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]],[<span class="number">1</span>,<span class="number">1</span>])</span><br><span class="line">temp2 = tf.tile([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]],[<span class="number">2</span>,<span class="number">1</span>])</span><br><span class="line">temp3 = tf.tile([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]],[<span class="number">2</span>,<span class="number">2</span>])</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(temp))</span><br><span class="line">    </span><br><span class="line">    	<span class="comment"># [[1 2 3] </span></span><br><span class="line">		  [<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]]</span><br><span class="line">		  </span><br><span class="line">    print(sess.run(temp2))</span><br><span class="line">    </span><br><span class="line">    	<span class="comment"># [[1 2 3] </span></span><br><span class="line">		  [<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>] </span><br><span class="line">		  [<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>] </span><br><span class="line">		  [<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]]</span><br><span class="line">		  </span><br><span class="line">    print(sess.run(temp3))</span><br><span class="line">    </span><br><span class="line">    	<span class="comment"># [[1 2 3 1 2 3] </span></span><br><span class="line">		  [<span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span>] </span><br><span class="line">		  [<span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span>] </span><br><span class="line">		  [<span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span>]]</span><br></pre></td></tr></table></figure>

<h3 id="tf-contrib-layers-flatten"><a href="#tf-contrib-layers-flatten" class="headerlink" title="tf.contrib.layers.flatten"></a>tf.contrib.layers.flatten</h3><p><code>tf.contrib.layers.flatten(P)</code> 这个函数就是把P保留第一个维度，把第一个维度包含的每一子张量展开成一个行向量，返回张量是一个二维的，<code>shape = (batch_size, ...)</code>, 一般用于卷积神经网络全链接层前的预处理。</p>
<p>例如 CNN 的 conv 层输出的 tensor 的 shape 为  \(\text{[batch_size, height, width, channel]}\), 刚展开会就是  \(\text{[batch_size, height × width × channel]}\)。 </p>
<h3 id="tf-contrib-layers-fully-connection"><a href="#tf-contrib-layers-fully-connection" class="headerlink" title="tf.contrib.layers.fully_connection"></a>tf.contrib.layers.fully_connection</h3><p><code>tf.contrib.layers.fully_connection(F，num_output,activation_fn)</code> 这个函数就是全链接成层, <code>F</code> 是输入，<code>num_output</code> 是下一层单元的个数，<code>activation_fn</code> 是激活函数，默认是 <code>relu</code></p>
<h2 id="tf-Variable与tf-get-variable"><a href="#tf-Variable与tf-get-variable" class="headerlink" title="tf.Variable与tf.get_variable()"></a>tf.Variable与tf.get_variable()</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.Variable(<span class="attribute">initial_value</span>=None, <span class="attribute">trainable</span>=<span class="literal">True</span>, \</span><br><span class="line">	<span class="attribute">collections</span>=None, <span class="attribute">validate_shape</span>=<span class="literal">True</span>, \</span><br><span class="line">	<span class="attribute">caching_device</span>=None, <span class="attribute">name</span>=None, \</span><br><span class="line">	<span class="attribute">variable_def</span>=None, <span class="attribute">dtype</span>=None, \</span><br><span class="line">	<span class="attribute">expected_shape</span>=None, <span class="attribute">import_scope</span>=None)</span><br></pre></td></tr></table></figure>


<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.get_variable(name, <span class="attribute">shape</span>=None, <span class="attribute">dtype</span>=None, \</span><br><span class="line">	<span class="attribute">initializer</span>=None, <span class="attribute">regularizer</span>=None, <span class="attribute">trainable</span>=<span class="literal">True</span>, \</span><br><span class="line">	<span class="attribute">collections</span>=None, <span class="attribute">caching_device</span>=None, \</span><br><span class="line">	<span class="attribute">partitioner</span>=None, <span class="attribute">validate_shape</span>=<span class="literal">True</span>, <span class="attribute">custom_getter</span>=None)</span><br></pre></td></tr></table></figure>

<p><strong>区别</strong></p>
<ol>
<li><p>使用<code>tf.Variable</code>时，如果检测到命名冲突，系统会自己处理。使用<code>tf.get_variable()</code>时，系统不会处理冲突，而会报错。</p>
<p> 基于这两个函数的特性，当我们需要共享变量的时候，需要使用tf.get_variable()。在其他情况下，这两个的用法是一样的</p>
</li>
<li><p>对于tf.Variable函数，变量名称是一个可选的参数，通过name=”v”的形式给出。而tf.get_variable函数，变量名称是一个必填的参数，它会根据变量名称去创建或者获取变量。    </p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">w_1 = tf.Variable(<span class="number">3</span>,name=<span class="string">"w_1"</span>)</span><br><span class="line">w_2 = tf.Variable(<span class="number">1</span>,name=<span class="string">"w_1"</span>)</span><br><span class="line"><span class="keyword">print</span> w_1.name</span><br><span class="line"><span class="keyword">print</span> w_2.name</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line"><span class="comment">#w_1:0</span></span><br><span class="line"><span class="comment">#w_1_1:0</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">w_1 = tf.get_variable(name=<span class="string">"w_1"</span>,initializer=<span class="number">1</span>)</span><br><span class="line">w_2 = tf.get_variable(name=<span class="string">"w_1"</span>,initializer=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#错误信息</span></span><br><span class="line"><span class="comment">#ValueError: Variable w_1 already exists, disallowed. Did</span></span><br><span class="line"><span class="comment">#you mean to set reuse=True in VarScope?</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义的基本等价  </span></span><br><span class="line">v = tf.get_variable(<span class="string">"v"</span>,shape=[<span class="number">1</span>],initializer.constant_initializer(<span class="number">1.0</span>))  </span><br><span class="line">v = tf.Variable(tf.constant(<span class="number">1.0</span>,shape=[<span class="number">1</span>]),name=<span class="string">"v"</span>)</span><br></pre></td></tr></table></figure>



<h3 id="tf-name-scope-tf-variable-scope"><a href="#tf-name-scope-tf-variable-scope" class="headerlink" title="tf.name_scope() / tf.variable_scope()"></a>tf.name_scope() / tf.variable_scope()</h3><blockquote>
<p>主要目的是为了更加方便地管理参数命名  </p>
</blockquote>
<p><strong>tf.name_scope 主要结合 tf.Variable() 来使用，方便参数命名管理。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'conv1'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    weights1 = tf.Variable([<span class="number">1.0</span>, <span class="number">2.0</span>], name=<span class="string">'weights'</span>)</span><br><span class="line">    bias1 = tf.Variable([<span class="number">0.3</span>], name=<span class="string">'bias'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面是在另外一个命名空间来定义变量的</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">'conv2'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">    weights2 = tf.Variable([<span class="number">4.0</span>, <span class="number">2.0</span>], name=<span class="string">'weights'</span>)</span><br><span class="line">    bias2 = tf.Variable([<span class="number">0.33</span>], name=<span class="string">'bias'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 所以，实际上weights1 和 weights2 这两个引用名指向了不同的空间，不会冲突</span></span><br><span class="line">print(weights1.name)</span><br><span class="line">print(weights2.name)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;conv1/weights:<span class="number">0</span></span><br><span class="line">&gt;&gt;&gt;conv2/weights:<span class="number">0</span></span><br></pre></td></tr></table></figure>

<p><strong>tf.variable_scope() 主要结合 tf.get_variable() 来使用，实现变量共享。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 注意， bias1 的定义方式</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'v_scope'</span>) <span class="keyword">as</span> scope1:</span><br><span class="line">    Weights1 = tf.get_variable(<span class="string">'Weights'</span>, shape=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="comment">#     bias1 = tf.Variable([0.52], name='bias')</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面来共享上面已经定义好的变量</span></span><br><span class="line"><span class="comment"># note: 在下面的 scope 中的get_variable()变量必须已经定义过了，才能设置 reuse=True，否则会报错</span></span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'v_scope'</span>, reuse=<span class="literal">True</span>) <span class="keyword">as</span> scope2:</span><br><span class="line">    Weights2 = tf.get_variable(<span class="string">'Weights'</span>)</span><br><span class="line">    bias2 = tf.Variable([<span class="number">0.52</span>], name=<span class="string">'bias'</span>)</span><br><span class="line"></span><br><span class="line">print(Weights1.name)</span><br><span class="line">print(Weights2.name)</span><br><span class="line">print(bias2.name)</span><br><span class="line"></span><br><span class="line">&gt;&gt;&gt;v_scope/Weights:<span class="number">0</span></span><br><span class="line">&gt;&gt;&gt;v_scope/Weights:<span class="number">0</span></span><br><span class="line">&gt;&gt;&gt;v_scope_1/bias:<span class="number">0</span></span><br></pre></td></tr></table></figure>

<h3 id="tf-control-dependencies"><a href="#tf-control-dependencies" class="headerlink" title="tf.control_dependencies()"></a>tf.control_dependencies()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.control_dependencies(self, control_inputs)</span><br></pre></td></tr></table></figure>

<p>通过以上的解释，我们可以知道，该函数接受的参数control_inputs，是Operation或者Tensor构成的list。返回的是一个上下文管理器，该上下文管理器用来控制在该上下文中的操作的依赖。也就是说，上下文管理器下定义的操作是依赖control_inputs中的操作的，control_dependencies用来控制control_inputs中操作执行后，才执行上下文管理器中定义的操作。</p>
<p><strong>例子</strong></p>
<p>如果我们想要确保获取更新后的参数，name我们可以这样组织我们的代码。</p>
<figure class="highlight nimrod"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">opt = tf.train.<span class="type">Optimizer</span>().minize(loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.control_dependencies([opt]): <span class="comment">#先执行opt</span></span><br><span class="line">  updated_weight = tf.identity(weight)  <span class="comment">#再执行该操作</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.<span class="type">Session</span>() <span class="keyword">as</span> sess:</span><br><span class="line">  tf.global_variables_initializer().run()</span><br><span class="line">  sess.run(updated_weight, feed_dict=<span class="meta">&#123;...&#125;</span>) <span class="comment"># 这样每次得到的都是更新后的weight</span></span><br></pre></td></tr></table></figure>

<h3 id="tf-placeholder-with-default"><a href="#tf-placeholder-with-default" class="headerlink" title="tf.placeholder_with_default()"></a>tf.placeholder_with_default()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">placeholder_with_default(</span><br><span class="line">    input,</span><br><span class="line">    shape,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>该函数将返回一个张量。与 input 具有相同的类型。一个占位符张量，默认为 input 的占位符张量 (如果未送入)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'inputs'</span>):</span><br><span class="line">    X = tf.placeholder_with_default(</span><br><span class="line">        data[<span class="string">'x_batch'</span>], [<span class="literal">None</span>, IMAGE_SIZE, IMAGE_SIZE, <span class="number">3</span>], <span class="string">'X'</span>)</span><br><span class="line">    Y = tf.placeholder_with_default(</span><br><span class="line">        data[<span class="string">'y_batch'</span>], [<span class="literal">None</span>, NUM_CLASSES], <span class="string">'Y'</span>)</span><br></pre></td></tr></table></figure>

<h3 id="tf-device"><a href="#tf-device" class="headerlink" title="tf.device()"></a>tf.device()</h3><p>如果需要切换成CPU运算，可以调用 <code>tf.device(device_name)</code> 函数，其中 <code>device_name</code> 格式如 <code>/cpu:0</code> 其中的0表示设备号，TF 不区分 CPU 的设备号，设置为0即可。GPU 区分设备号 <code>\gpu:0</code> 和 <code>\gpu:1</code> 表示两张不同的显卡。 </p>
<p>在一些情况下，我们即使是在GPU下跑模型，也会将部分Tensor储存在内存里，因为这个Tensor可能太大了，显存不够放，相比于显存，内存一般大多了，于是这个时候就常常人为指定为CPU设备。这种形式我们在一些代码中能见到。如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">    build_CNN() <span class="comment"># 此时，这个CNN的Tensor是储存在内存里的，而非显存里。</span></span><br></pre></td></tr></table></figure>

<p><strong>例子</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>), tf.variable_scope(<span class="string">'input_pipeline'</span>):</span><br><span class="line">    data = _get_data(NUM_CLASSES, IMAGE_SIZE)</span><br></pre></td></tr></table></figure>

<h3 id="tf-Graph"><a href="#tf-Graph" class="headerlink" title="tf.Graph()"></a>tf.Graph()</h3><p>一个TensorFlow的运算，被表示为一个数据流的图。<br>一幅图中包含一些操作（Operation）对象，这些对象是计算节点。前面说过的Tensor对象，则是表示在不同的操作（operation）间的数据节点</p>
<p>你一旦开始你的任务，就已经有一个默认的图已经创建好了。而且可以通过调用tf.get_default_graph()来访问到。<br>添加一个操作到默认的图里面，只要简单的调用一个定义了新操作的函数就行。比如下面的例子展示的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">c=tf.constant(value=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> c.graph <span class="keyword">is</span> tf.get_default_graph()</span><br><span class="line">print(c.graph)</span><br><span class="line">print(tf.get_default_graph())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">&lt;tensorflow.python.framework.ops.Graph object at <span class="number">0x107324cc0</span>&gt;</span><br><span class="line">&lt;tensorflow.python.framework.ops.Graph object at <span class="number">0x107324cc0</span>&gt;</span><br></pre></td></tr></table></figure>

<p>另外一种典型的用法就是要使用到 <code>Graph.as_default()</code> 的上下文管理器（ context manager），它能够在这个上下文里面覆盖默认的图。如下例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">c = tf.constant(value=<span class="number">1</span>)</span><br><span class="line">print(c.graph)</span><br><span class="line">print(tf.get_default_graph())</span><br><span class="line">print()</span><br><span class="line"></span><br><span class="line">g = tf.Graph()</span><br><span class="line">print(<span class="string">"g:"</span>, g)</span><br><span class="line"><span class="keyword">with</span> g.as_default():</span><br><span class="line">    d = tf.constant(value=<span class="number">2</span>)</span><br><span class="line">    print(d.graph)</span><br><span class="line">    print()</span><br><span class="line"></span><br><span class="line">g2 = tf.Graph()</span><br><span class="line">print(<span class="string">"g2:"</span>, g2)</span><br><span class="line"><span class="keyword">with</span> g2.as_default():</span><br><span class="line">    e = tf.constant(value=<span class="number">15</span>)</span><br><span class="line">    print(e.graph)</span><br><span class="line">    print()</span><br><span class="line"></span><br><span class="line">f = tf.constant(value=<span class="number">1</span>)</span><br><span class="line">print(f.graph)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">&lt;tensorflow.python.framework.ops.Graph object at <span class="number">0x104845da0</span>&gt;</span><br><span class="line">&lt;tensorflow.python.framework.ops.Graph object at <span class="number">0x104845da0</span>&gt;</span><br><span class="line"></span><br><span class="line">g: &lt;tensorflow.python.framework.ops.Graph object at <span class="number">0x1815af77f0</span>&gt;</span><br><span class="line">&lt;tensorflow.python.framework.ops.Graph object at <span class="number">0x1815af77f0</span>&gt;</span><br><span class="line"></span><br><span class="line">g2: &lt;tensorflow.python.framework.ops.Graph object at <span class="number">0x1815af7748</span>&gt;</span><br><span class="line">&lt;tensorflow.python.framework.ops.Graph object at <span class="number">0x1815af7748</span>&gt;</span><br><span class="line"></span><br><span class="line">&lt;tensorflow.python.framework.ops.Graph object at <span class="number">0x104845da0</span>&gt;</span><br></pre></td></tr></table></figure>

<p>可以看到，如果在 <code>with</code> 外的话，graph 是系统默认的图，而不是带 <code>with</code> 的默认图</p>
<h3 id="基本-CNN-函数"><a href="#基本-CNN-函数" class="headerlink" title="基本 CNN 函数"></a>基本 CNN 函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_batch_norm</span><span class="params">(X, is_training)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.layers.batch_normalization(</span><br><span class="line">        X, scale=<span class="literal">False</span>, center=<span class="literal">True</span>,</span><br><span class="line">        momentum=BATCH_NORM_MOMENTUM,</span><br><span class="line">        training=is_training, fused=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_global_average_pooling</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(</span><br><span class="line">        X, axis=[<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        name=<span class="string">'global_average_pooling'</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_max_pooling</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(</span><br><span class="line">        X, [<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], <span class="string">'SAME'</span>,</span><br><span class="line">        name=<span class="string">'max_pooling'</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_avg_pooling</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.avg_pool(</span><br><span class="line">        X, [<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], <span class="string">'SAME'</span>,</span><br><span class="line">        name=<span class="string">'avg_pooling'</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_nonlinearity</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.relu(X, name=<span class="string">'ReLU'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_dropout</span><span class="params">(X, is_training, rate=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    keep_prob = tf.constant(</span><br><span class="line">        <span class="number">1.0</span> - rate, tf.float32,</span><br><span class="line">        [], <span class="string">'keep_prob'</span></span><br><span class="line">    )</span><br><span class="line">    result = tf.cond(</span><br><span class="line">        is_training,</span><br><span class="line">        <span class="keyword">lambda</span>: tf.nn.dropout(X, keep_prob),</span><br><span class="line">        <span class="keyword">lambda</span>: tf.identity(X),</span><br><span class="line">        name=<span class="string">'dropout'</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reading cifar dataset</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unpickle</span><span class="params">(file)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> cPickle</span><br><span class="line">    f = open(file, <span class="string">'rb'</span>)</span><br><span class="line">    dict = cPickle.load(f)</span><br><span class="line">    f.close()</span><br><span class="line">    <span class="keyword">return</span> dict</span><br></pre></td></tr></table></figure>

<h3 id="tf-one-hot"><a href="#tf-one-hot" class="headerlink" title="tf.one_hot()"></a>tf.one_hot()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CLASS = <span class="number">8</span></span><br><span class="line">label1 = tf.constant([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>])</span><br><span class="line">sess1 = tf.Session()</span><br><span class="line">print(<span class="string">'label1:'</span>, sess1.run(label1))</span><br><span class="line">b = tf.one_hot(label1, CLASS, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    sess.run(b)</span><br><span class="line">    print(<span class="string">'after one_hot'</span>, sess.run(b))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">label1: [<span class="number">0</span> <span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span>]</span><br><span class="line">after one_hot [[<span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">				[<span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> 				[<span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> 				[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">				[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> 				[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span>]</span><br><span class="line">				[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line">				[<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span>]]</span><br></pre></td></tr></table></figure>

<h2 id="tf-梯度"><a href="#tf-梯度" class="headerlink" title="tf 梯度"></a>tf 梯度</h2><h3 id="apply-gradients"><a href="#apply-gradients" class="headerlink" title="apply_gradients()"></a>apply_gradients()</h3><p><code>apply_gradients(grads_and_vars,global_step=None,name=None)</code></p>
<p><strong>作用：</strong><br>把梯度 “应用”（Apply）到变量上面去。其实就是按照梯度下降的方式加到上面去。这是 minimize() 函数的第二个步骤。 返回一个应用的操作。 </p>
<p><strong>参数:</strong><br><code>grads_and_vars</code>: <code>compute_gradients()</code> 函数返回的 <code>(gradient, variable)</code> 对的列表<br><code>global_step</code>: Optional Variable to increment by one after the variables have been updated.<br><code>name</code>: 可选，名字       </p>
<h3 id="tf-gradients"><a href="#tf-gradients" class="headerlink" title="tf.gradients()"></a>tf.gradients()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.gradients(ys, xs, </span><br><span class="line">             grad_ys=<span class="literal">None</span>, </span><br><span class="line">             name=<span class="string">'gradients'</span>,</span><br><span class="line">             colocate_gradients_with_ops=<span class="literal">False</span>,</span><br><span class="line">             gate_gradients=<span class="literal">False</span>,</span><br><span class="line">             aggregation_method=<span class="literal">None</span>,</span><br><span class="line">             stop_gradients=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<p>对求导函数而言，其主要功能即求导公式： \(\frac{∂y}{∂x}\) 。在tensorflow中， \(y\) 和 \(x\) 都是 tensor。</p>
<p>更进一步，<code>tf.gradients()</code> 接受求导值 <code>ys</code> 和 <code>xs</code> 不仅可以是 tensor，还可以是 list，形如 <code>[tensor1, tensor2, …, tensorn]</code>。当 <code>ys</code> 和 <code>xs</code> 都是 list 时，它们的求导关系为：</p>
<p>假设返回值是 \([grad1, grad2, grad3]，ys=[y1, y2]，xs=[x1, x2, x3]\) 。则，真实的计算过程为: </p>
<p>$$grad1=\frac{∂ y1}{∂x1}+\frac{∂y2}{∂x1}$$<br>$$grad2=\frac{∂ y1}{∂x2}+\frac{∂y2}{∂x2}$$<br>$$grad3=\frac{∂ y1}{∂x3}+\frac{∂y2}{∂x3}$$</p>
<p><strong>实例</strong></p>
<p>以线性回归为例，实践 <code>tf.gradients()</code> 的基础功能。线性回归： \(y=3×x+2\) </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">x_input = tf.placeholder(tf.float32, name=<span class="string">'x_input'</span>)</span><br><span class="line">y_input = tf.placeholder(tf.float32, name=<span class="string">'y_input'</span>)</span><br><span class="line">w = tf.Variable(<span class="number">2.0</span>, name=<span class="string">'weight'</span>)</span><br><span class="line">b = tf.Variable(<span class="number">1.0</span>, name=<span class="string">'biases'</span>)</span><br><span class="line">y = tf.add(tf.multiply(x_input, w), b)</span><br><span class="line">loss_op = tf.reduce_sum(tf.pow(y_input - y, <span class="number">2</span>)) / (<span class="number">2</span> * <span class="number">32</span>)</span><br><span class="line">train_op = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(loss_op)</span><br><span class="line"></span><br><span class="line">gradients_node = tf.gradients(loss_op, w)</span><br><span class="line">print(gradients_node)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="string">'''构造数据集'''</span></span><br><span class="line">x_pure = np.random.randint(<span class="number">-10</span>, <span class="number">100</span>, <span class="number">32</span>)</span><br><span class="line">x_train = x_pure + np.random.randn(<span class="number">32</span>) / <span class="number">10</span>  <span class="comment"># 为x加噪声</span></span><br><span class="line">y_train = <span class="number">3</span> * x_pure + <span class="number">2</span> + np.random.randn(<span class="number">32</span>) / <span class="number">10</span>  <span class="comment"># 为y加噪声</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    _, gradients, loss = sess.run([train_op, gradients_node, loss_op],</span><br><span class="line">                                  feed_dict=&#123;x_input: x_train[i], y_input: y_train[i]&#125;)</span><br><span class="line">    print(<span class="string">"epoch: &#123;&#125; \t loss: &#123;&#125; \t gradients: &#123;&#125;"</span>.format(i, loss, gradients))</span><br><span class="line"></span><br><span class="line">sess.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 输出</span></span><br><span class="line">[&lt;tf.Tensor <span class="string">'gradients_1/Mul_grad/Reshape_1:0'</span> shape=() dtype=float32&gt;]</span><br><span class="line">epoch: <span class="number">0</span> 	 loss: <span class="number">0.06110262870788574</span> 	 gradients: [<span class="number">-0.064300433</span>]</span><br><span class="line">epoch: <span class="number">1</span> 	 loss: <span class="number">108.1637191772461</span> 	 gradients: [<span class="number">-212.91476</span>]</span><br><span class="line">epoch: <span class="number">2</span> 	 loss: <span class="number">34.10615921020508</span> 	 gradients: [<span class="number">61.373066</span>]</span><br><span class="line">epoch: <span class="number">3</span> 	 loss: <span class="number">16.358537673950195</span> 	 gradients: [<span class="number">64.797104</span>]</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">epoch: <span class="number">18</span> 	 loss: <span class="number">0.0004277984262444079</span> 	 gradients: [<span class="number">0.28949624</span>]</span><br><span class="line">epoch: <span class="number">19</span> 	 loss: <span class="number">0.00298550957813859</span> 	 gradients: [<span class="number">1.0919241</span>]</span><br></pre></td></tr></table></figure>

<p>可以看到梯度逐渐减小，说明模型逐渐收敛</p>
<h3 id="tf-stop-gradients"><a href="#tf-stop-gradients" class="headerlink" title="tf.stop_gradients()"></a>tf.stop_gradients()</h3><p>stop_gradients 也是一个 list，list 中的元素是 tensorflow graph 中的 op，一旦进入这个list，将不会被计算梯度，更重要的是，在该op之后的BP计算都不会运行。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(<span class="number">0.</span>)</span><br><span class="line">b = <span class="number">2</span> * a</span><br><span class="line">c = a + b</span><br><span class="line">g = tf.gradients(c, [a, b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"></span><br><span class="line">计算得 g = [<span class="number">3.0</span>, <span class="number">1.0</span>]。因为 ∂c/∂a=∂a/∂a+∂b/∂a=<span class="number">3.0</span></span><br></pre></td></tr></table></figure>

<p>但如果冻结operator a和b的梯度计算：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = tf.constant(<span class="number">0.</span>)</span><br><span class="line">b = <span class="number">2</span> * a</span><br><span class="line">g = tf.gradients(a + b, [a, b], stop_gradients=[a, b])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line">计算得g=[<span class="number">1.0</span>, <span class="number">1.0</span>]</span><br></pre></td></tr></table></figure>

<p>上面的代码也等效于：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = tf.stop_gradient(tf.constant(<span class="number">0.</span>))</span><br><span class="line">b = tf.stop_gradient(<span class="number">2</span> * a)</span><br><span class="line">g = tf.gradients(a + b, [a, b])</span><br></pre></td></tr></table></figure>

<h3 id="处理梯度"><a href="#处理梯度" class="headerlink" title="处理梯度"></a>处理梯度</h3><ol>
<li>计算全部gradient </li>
</ol>
<p><code>gradient_all = optimizer.compute_gradients(loss)</code>   </p>
<ol start="2">
<li>得到可进行梯度计算的变量  </li>
</ol>
<p><code>grads_vars = [v for (g,v) in gradient_all if g is not None]</code>   </p>
<ol start="3">
<li>得到所需梯度   </li>
</ol>
<p><code>gradient = optimizer.compute_gradients(loss, grads_vars)</code>   </p>
<ol start="4">
<li>生成holder  </li>
</ol>
<p><code>grads_holder = [(tf.placeholder(tf.float32, shape=g.get_shape()), v) for (g,v) in gradient]</code>   </p>
<ol start="5">
<li>继续进行BP算法   </li>
</ol>
<p><code>train_op = optimizer.apply_gradients(grads_holder)</code>   </p>
<h2 id="tf-contrib-slim"><a href="#tf-contrib-slim" class="headerlink" title="tf.contrib.slim"></a>tf.contrib.slim</h2><h3 id="tf-contrib-slim-conv2d"><a href="#tf-contrib-slim-conv2d" class="headerlink" title="tf.contrib.slim.conv2d"></a>tf.contrib.slim.conv2d</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">convolution(inputs,</span><br><span class="line">          num_outputs,</span><br><span class="line">          kernel_size,</span><br><span class="line">          stride=<span class="number">1</span>,</span><br><span class="line">          padding=<span class="string">'SAME'</span>,</span><br><span class="line">          data_format=<span class="literal">None</span>,</span><br><span class="line">          rate=<span class="number">1</span>,</span><br><span class="line">          activation_fn=nn.relu,</span><br><span class="line">          normalizer_fn=<span class="literal">None</span>,</span><br><span class="line">          normalizer_params=<span class="literal">None</span>,</span><br><span class="line">          weights_initializer=initializers.xavier_initializer(),</span><br><span class="line">          weights_regularizer=<span class="literal">None</span>,</span><br><span class="line">          biases_initializer=init_ops.zeros_initializer(),</span><br><span class="line">          biases_regularizer=<span class="literal">None</span>,</span><br><span class="line">          reuse=<span class="literal">None</span>,</span><br><span class="line">          variables_collections=<span class="literal">None</span>,</span><br><span class="line">          outputs_collections=<span class="literal">None</span>,</span><br><span class="line">          trainable=<span class="literal">True</span>,</span><br><span class="line">          scope=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<ul>
<li>inputs：同样是指需要做卷积的输入图像</li>
<li>num_outputs：指定卷积核的个数（就是filter的个数）</li>
<li>kernel_size：用于指定卷积核的维度（卷积核的宽度，卷积核的高度）</li>
<li>stride：为卷积时在图像每一维的步长</li>
<li>padding：为padding的方式选择，VALID或者SAME</li>
<li>data_format：是用于指定输入的input的格式</li>
<li>rate：使用atrous convolution的膨胀率</li>
<li>activation_fn：用于激活函数的指定，默认的为ReLU函数</li>
<li>normalizer_fn：用于指定正则化函数</li>
<li>normalizer_params：用于指定正则化函数的参数</li>
<li>weights_initializer：用于指定权重的初始化程序</li>
<li>weights_regularizer：为权重可选的正则化程序</li>
<li>biases_initializer：用于指定biase的初始化程序</li>
<li>biases_regularizer: biases可选的正则化程序</li>
<li>reuse：指定是否共享层或者和变量</li>
<li>variable_collections：指定所有变量的集合列表或者字典</li>
<li>outputs_collections：指定输出被添加的集合</li>
<li>trainable：卷积层的参数是否可被训练</li>
<li>scope：共享变量所指的variable_scope</li>
</ul>
<h2 id="1"><a href="#1" class="headerlink" title="1"></a>1</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># fc1</span></span><br><span class="line">layer = tf.layers.dense(</span><br><span class="line">    inputs=self.tf_obs,</span><br><span class="line">    units=<span class="number">10</span>,   <span class="comment"># 输出个数</span></span><br><span class="line">    activation=tf.nn.tanh,  <span class="comment"># 激励函数</span></span><br><span class="line">    kernel_initializer=tf.random_normal_initializer(mean=<span class="number">0</span>, stddev=<span class="number">0.3</span>),</span><br><span class="line">    bias_initializer=tf.constant_initializer(<span class="number">0.1</span>),</span><br><span class="line">    name=<span class="string">'fc1'</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># fc2</span></span><br><span class="line">all_act = tf.layers.dense(</span><br><span class="line">    inputs=layer,</span><br><span class="line">    units=self.n_actions,   <span class="comment"># 输出个数</span></span><br><span class="line">    activation=<span class="literal">None</span>,    <span class="comment"># 之后再加 Softmax</span></span><br><span class="line">    kernel_initializer=tf.random_normal_initializer(mean=<span class="number">0</span>, stddev=<span class="number">0.3</span>),</span><br><span class="line">    bias_initializer=tf.constant_initializer(<span class="number">0.1</span>),</span><br><span class="line">    name=<span class="string">'fc2'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">self.all_act_prob = tf.nn.softmax(all_act, name=<span class="string">'act_prob'</span>)  <span class="comment"># 激励函数 softmax 出概率</span></span><br></pre></td></tr></table></figure>

<h2 id="2"><a href="#2" class="headerlink" title="2"></a>2</h2><p>更新固定网络参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">'update_oldpi'</span>):</span><br><span class="line">    self.update_oldpi_op = [oldp.assign(p) <span class="keyword">for</span> p, oldp <span class="keyword">in</span> zip(pi_params, oldpi_params)]</span><br></pre></td></tr></table></figure>

<h2 id="待记录"><a href="#待记录" class="headerlink" title="待记录"></a>待记录</h2><p><a href="https://github.com/balancap/SSD-Tensorflow/blob/master/datasets/pascalvoc_to_tfrecords.py" target="_blank" rel="noopener">https://github.com/balancap/SSD-Tensorflow/blob/master/datasets/pascalvoc_to_tfrecords.py</a></p>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

            </div>
          

    
      <footer class="post-footer">
        <div class="post-tags">
          
            <a href="/tags/TensorFlow/">TensorFlow</a>
          
        </div>

        <div class="post-tags">
          
            <a href="/categories/TensorFlow/">TensorFlow</a>
          
        </div>

        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2018/04/28/PerceptualLosses/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">Perceptual Loss 感知损失 图像超分辨重建（二）</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/2018/04/27/SuperResolution/">
        <span class="next-text nav-default">SRCNN 图像超分辨重建（一）</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

        
  <div class="comments" id="comments">
    
  </div>


      </footer>
    
  </article>

    </div>


      <div class="post-toc-warp">
        
      <!--noindex-->
          <div class="post-toc" id="toc-test">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-函数"><span class="nav-text">tf 函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-split"><span class="nav-text">tf.split</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-concat"><span class="nav-text">tf.concat</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-cond"><span class="nav-text">tf.cond</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-tile"><span class="nav-text">tf.tile</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-contrib-layers-flatten"><span class="nav-text">tf.contrib.layers.flatten</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-contrib-layers-fully-connection"><span class="nav-text">tf.contrib.layers.fully_connection</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-Variable与tf-get-variable"><span class="nav-text">tf.Variable与tf.get_variable()</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-name-scope-tf-variable-scope"><span class="nav-text">tf.name_scope() / tf.variable_scope()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-control-dependencies"><span class="nav-text">tf.control_dependencies()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-placeholder-with-default"><span class="nav-text">tf.placeholder_with_default()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-device"><span class="nav-text">tf.device()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-Graph"><span class="nav-text">tf.Graph()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#基本-CNN-函数"><span class="nav-text">基本 CNN 函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-one-hot"><span class="nav-text">tf.one_hot()</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-梯度"><span class="nav-text">tf 梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#apply-gradients"><span class="nav-text">apply_gradients()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-gradients"><span class="nav-text">tf.gradients()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-stop-gradients"><span class="nav-text">tf.stop_gradients()</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#处理梯度"><span class="nav-text">处理梯度</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf-contrib-slim"><span class="nav-text">tf.contrib.slim</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#tf-contrib-slim-conv2d"><span class="nav-text">tf.contrib.slim.conv2d</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1"><span class="nav-text">1</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2"><span class="nav-text">2</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#待记录"><span class="nav-text">待记录</span></a></li></ol></div>
            

            <div class="back-to-top" id="back-to-top">
              <i class="iconfont icon-up"></i>
            </div>

          </div>
      <!--/noindex-->
      
      </div>


      </div>
      <footer id="colophon"><span class="copyright-year">
    
        &copy;
    
        2017 -
    
    2019
    <span class="footer-author">haoyu.</span>
    <span class="power-by">
        Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a> and <a class="theme-link" href="https://github.com/henryhuang/hexo-theme-polarbearsimple">Polar Bear Simple</a>
    </span>
</span>

      </footer>


    </div>
    


    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  

    <script type="text/javascript" src="/js/src/theme.js?v=1.1"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=1.1"></script>

  </body>
</html>
