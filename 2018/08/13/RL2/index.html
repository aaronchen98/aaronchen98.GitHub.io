<!DOCTYPE html>
<html lang>
  <head><meta name="generator" content="Hexo 3.9.0">
    
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=0.5, maximum-scale=2.0, user-scalable=yes">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="description" content="强化学习（二）">




  <meta name="keywords" content="强化学习,">







  <link rel="alternate" href="/atom.xml" title="浩瀚宇宙·AaronChen">




  <link rel="shortcut icon" type="image/x-icon" href="/img/1.jpg?v=1.1">



<link rel="canonical" href="https://hhyz.me/2018/08/13/RL2/">


<meta name="description" content="![](/img/2018-08-13-RL2-1.jpg)         强化学习（RL，基于MDP）的求解policy的方式一般分为三种：  Value &amp;lt;—critic Policy &amp;lt;—actor Value + Policy &amp;lt;— Actor-critic  策略梯度强化学习是一个通过奖惩来学习正确行为的机制. 家族中有很多种不一样的成员, 有学习奖惩值, 根据自">
<meta name="keywords" content="强化学习">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习（二）">
<meta property="og:url" content="https://hhyz.me/2018/08/13/RL2/index.html">
<meta property="og:site_name" content="浩瀚宇宙·AaronChen">
<meta property="og:description" content="![](/img/2018-08-13-RL2-1.jpg)         强化学习（RL，基于MDP）的求解policy的方式一般分为三种：  Value &amp;lt;—critic Policy &amp;lt;—actor Value + Policy &amp;lt;— Actor-critic  策略梯度强化学习是一个通过奖惩来学习正确行为的机制. 家族中有很多种不一样的成员, 有学习奖惩值, 根据自">
<meta property="og:locale" content="zh">
<meta property="og:image" content="https://hhyz.me/img/2018-08-13-RL2-2.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-08-13-RL2-3.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-08-13-RL2-101.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-08-13-RL2-102.jpg">
<meta property="og:updated_time" content="2019-07-04T19:24:30.084Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="强化学习（二）">
<meta name="twitter:description" content="![](/img/2018-08-13-RL2-1.jpg)         强化学习（RL，基于MDP）的求解policy的方式一般分为三种：  Value &amp;lt;—critic Policy &amp;lt;—actor Value + Policy &amp;lt;— Actor-critic  策略梯度强化学习是一个通过奖惩来学习正确行为的机制. 家族中有很多种不一样的成员, 有学习奖惩值, 根据自">
<meta name="twitter:image" content="https://hhyz.me/img/2018-08-13-RL2-2.jpg">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1">
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">





<script type="text/javascript">
  var themeConfig = {
    fancybox: {
      enable: false
    },
  };
</script>




  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?3cbfa43ed54bb06d8da192dc667bed80";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  <script type="text/javascript">
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-139198240-1', 'auto');
        ga('send', 'pageview');
  </script>





  


    <title> 强化学习（二） - 浩瀚宇宙·AaronChen </title>


    <script>
(function(u, c) {
  var d = document, t = 'script', o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
  o.src = u;
  if (c) { o.addEventListener('load', function(e) { c(e); }); }
  s.parentNode.insertBefore(o, s);
})('//cdn.bootcss.com/pangu/3.3.0/pangu.min.js', function() {
  pangu.spacingPage();
});
</script>



<script type="text/javascript">
window.onload=
function(){
    var oDiv = document.getElementById("toc-test"),
    H = 0,
    Y = oDiv
    while (Y) {H += Y.offsetTop; Y = Y.offsetParent}
    window.onscroll = function()
    {
        var s = document.body.scrollTop || document.documentElement.scrollTop
        if(s>H) {
            oDiv.style = "position:fixed;top:0;"
        } else {
            oDiv.style = ""
        }
    }
}
</script>






  </head>

  <body>
    <div id="page">
      <header id="masthead"><div class="site-header-inner">
    <h1 class="site-title">
        <a href="/." class="logo">浩瀚宇宙·AaronChen</a>
    </h1>

    <nav id="nav-top">
        
            <ul id="menu-top" class="nav-top-items">
                
                    <li class="menu-item">
                        <a href="https://www.hhyz.me">
                            
                            
                                Gallery
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/about">
                            
                            
                                About
                            
                        </a>
                    </li>
                
            </ul>
        
  </nav>
</div>

      </header>
      <div id="content">
        
    <div id="primary">
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          强化学习（二）
        
      </h1>

      <time class="post-time">
          Aug 13 2018
      </time>
    </header>



    
            <div class="post-content">
            <br>
<div align="center">
![](/img/2018-08-13-RL2-1.jpg)
<center><small><font color="gray">    </font></small></center>
</div>


<p>强化学习（RL，基于MDP）的求解policy的方式一般分为三种：</p>
<ul>
<li>Value &lt;—critic</li>
<li>Policy &lt;—actor</li>
<li>Value + Policy &lt;— Actor-critic</li>
</ul>
<h2 id="策略梯度"><a href="#策略梯度" class="headerlink" title="策略梯度"></a>策略梯度</h2><p>强化学习是一个通过奖惩来学习正确行为的机制. 家族中有很多种不一样的成员, 有学习奖惩值, 根据自己认为的高价值选行为, 比如 Q learning, Deep Q Network, 也有不通过分析奖励值, 直接输出行为的方法, 这就是今天要说的 Policy Gradients 了. 甚至我们可以为 Policy Gradients 加上一个神经网络来输出预测的动作. 对比起以值为基础的方法, Policy Gradients 直接输出动作的最大好处就是, 它能在一个连续区间内挑选动作, 而基于值的, 比如 Q-learning, 它如果在无穷多的动作中计算价值, 从而选择行为, 这, 它可吃不消.</p>
<p>有了神经网络当然方便, 但是, 我们怎么进行神经网络的误差反向传递呢? Policy Gradients 的误差又是什么呢? 答案是! 哈哈, 没有误差! 但是他的确是在进行某一种的反向传递. 这种反向传递的目的是让这次被选中的行为更有可能在下次发生. 但是我们要怎么确定这个行为是不是应当被增加被选的概率呢? 这时候我们的老朋友, reward 奖惩正可以在这时候派上用场,</p>
<p><img src="/img/2018-08-13-RL2-2.jpg" alt></p>
<div align="center">
<center><small><font color="gray">    </font></small></center>
</div>

<p>现在我们来演示一遍, 观测的信息通过神经网络分析, 选出了左边的行为, 我们直接进行反向传递, 使之下次被选的可能性增加, 但是奖惩信息却告诉我们, 这次的行为是不好的, 那我们的动作可能性增加的幅度 随之被减低. 这样就能靠奖励来左右我们的神经网络反向传递. 我们再来举个例子, 假如这次的观测信息让神经网络选择了右边的行为, 右边的行为随之想要进行反向传递, 使右边的行为下次被多选一点, 这时, 奖惩信息也来了, 告诉我们这是好行为, 那我们就在这次反向传递的时候加大力度, 让它下次被多选的幅度更猛烈! 这就是 Policy Gradients 的核心思想了. 很简单吧.</p>
<p>Policy gradient 是 RL 中另外一个大家族, 他不像 Value-based 方法 (Q learning, Sarsa), 但他也要接受环境信息 (observation), 不同的是他要输出不是 action 的 value, 而是具体的那一个 action, 这样 policy gradient 就跳过了 value 这个阶段. 而且个人认为 Policy gradient 最大的一个优势是: 输出的这个 action 可以是一个连续的值, 之前我们说到的 value-based 方法输出的都是不连续的值, 然后再选择值最大的 action. 而 policy gradient 可以在一个连续分布上选取 action.</p>
<p>介绍的 policy gradient 的第一个算法是一种基于 整条回合数据 的更新, 也叫 REINFORCE 方法. 这种方法是 policy gradient 的最基本方法, 有了这个的基础, 我们再来做更高级的.</p>
<p><img src="/img/2018-08-13-RL2-3.jpg" alt></p>
<div align="center">

<center><small><font color="gray">    </font></small></center>
</div>


<p><code>log(Policy(s,a))*V</code> 中的 <code>log(Policy(s,a))</code> 表示在 状态 <code>s</code> 对所选动作 <code>a</code> 的吃惊度, 如果 <code>Policy(s,a)</code> 概率越小, 反向的 <code>log(Policy(s,a))</code> (即 <code>-log(P)</code>) 反而越大. 如果在 <code>Policy(s,a)</code> 很小的情况下, 拿到了一个 大的 <code>R</code>, 也就是 大的 <code>V</code>, 那 <code>-log(Policy(s, a))*V</code> 就更大, 表示更吃惊, (我选了一个不常选的动作, 却发现原来它能得到了一个好的 reward, 那我就得对我这次的参数进行一个大幅修改). 这就是 <code>log(Policy)*V</code> 的物理意义啦</p>
<h3 id="策略梯度-1"><a href="#策略梯度-1" class="headerlink" title="策略梯度"></a>策略梯度</h3><h2 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor Critic"></a>Actor Critic</h2><p>我们有了像 Q-learning 这么伟大的算法, 为什么还要瞎折腾出一个 Actor-Critic? 原来 Actor-Critic 的 Actor 的前生是 Policy Gradients, 这能让它毫不费力地在连续动作中选取合适的动作, 而 Q-learning 做这件事会瘫痪. 那为什么不直接用 Policy Gradients 呢? 原来 Actor Critic 中的 Critic 的前生是 Q-learning 或者其他的 以值为基础的学习法 , 能进行单步更新, 而传统的 Policy Gradients 则是回合更新, 这降低了学习效率.</p>
<p>结合了 Policy Gradient (Actor) 和 Function Approximation (Critic) 的方法. Actor 基于概率选行为, Critic 基于 Actor 的行为评判行为的得分, Actor 根据 Critic 的评分修改选行为的概率.</p>
<p><img src="/img/2018-08-13-RL2-101.jpg" alt></p>
<div align="center">

<center><small><font color="gray">    </font></small></center>
</div>


<p>或者说详细点, 就是 Actor 在运用 Policy Gradient 的方法进行 Gradient ascent 的时候, 由 Critic 来告诉他, 这次的 Gradient ascent 是不是一次正确的 ascent, 如果这次的得分不好, 那么就不要 ascent 那么多.</p>
<h2 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG"></a>DDPG</h2><p><img src="/img/2018-08-13-RL2-102.jpg" alt></p>
<div align="center">
<center><small><font color="gray">    </font></small></center>
</div>


            </div>
          

    
      <footer class="post-footer">
        <div class="post-tags">
          
            <a href="/tags/强化学习/">强化学习</a>
          
        </div>

        <div class="post-tags">
          
            <a href="/categories/强化学习/">强化学习</a>
          
        </div>

        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2018/09/19/gan/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">GAN</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/2018/08/08/AlphaGO-Zero/">
        <span class="next-text nav-default">AlphaGO Zero 原理</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

        
  <div class="comments" id="comments">
    
  </div>


      </footer>
    
  </article>

    </div>


      <div class="post-toc-warp">
        
      <!--noindex-->
          <div class="post-toc" id="toc-test">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#策略梯度"><span class="nav-text">策略梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#策略梯度-1"><span class="nav-text">策略梯度</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Actor-Critic"><span class="nav-text">Actor Critic</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DDPG"><span class="nav-text">DDPG</span></a></li></ol></div>
            

            <div class="back-to-top" id="back-to-top">
              <i class="iconfont icon-up"></i>
            </div>

          </div>
      <!--/noindex-->
      
      </div>


      </div>
      <footer id="colophon"><span class="copyright-year">
    
        &copy;
    
        2017 -
    
    2019
    <span class="footer-author">haoyu.</span>
    <span class="power-by">
        Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a> and <a class="theme-link" href="https://github.com/henryhuang/hexo-theme-polarbearsimple">Polar Bear Simple</a>
    </span>
</span>

      </footer>


    </div>
    


    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  

    <script type="text/javascript" src="/js/src/theme.js?v=1.1"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=1.1"></script>

  </body>
</html>
