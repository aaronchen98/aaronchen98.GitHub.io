<!DOCTYPE html>
<html lang>
  <head><meta name="generator" content="Hexo 3.9.0">
    
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=0.5, maximum-scale=2.0, user-scalable=yes">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="description" content="Batch Normalization 学习笔记（转载）">




  <meta name="keywords" content="深度学习,">





<script type="text/javascript">
window.onload=
function(){
    var oDiv = document.getElementById("toc-test"),
    H = 0,
    Y = oDiv
    while (Y) {H += Y.offsetTop; Y = Y.offsetParent}
    window.onscroll = function()
    {
        var s = document.body.scrollTop || document.documentElement.scrollTop
        if(s>H) {
            oDiv.style = "position:fixed;top:0;"
        } else {
            oDiv.style = ""
        }
    }
}
</script>








  <link rel="alternate" href="/atom.xml" title="浩瀚宇宙·AaronChen">




  <link rel="shortcut icon" type="image/x-icon" href="/img/1.jpg?v=1.1">



<link rel="canonical" href="https://hhyz.me/2018/03/28/BatchNormalization/">


<meta name="description" content="一、背景意义本篇博文主要讲解2015年深度学习领域，非常值得学习的一篇文献：《Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift》，这个算法目前已经被大量的应用，最新的文献算法很多都会引用这个算法，进行网络训练，可见其强大之处非同一般啊。 近年来深度学习捷报连连、">
<meta name="keywords" content="深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="Batch Normalization 学习笔记（转载）">
<meta property="og:url" content="https://hhyz.me/2018/03/28/BatchNormalization/index.html">
<meta property="og:site_name" content="浩瀚宇宙·AaronChen">
<meta property="og:description" content="一、背景意义本篇博文主要讲解2015年深度学习领域，非常值得学习的一篇文献：《Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift》，这个算法目前已经被大量的应用，最新的文献算法很多都会引用这个算法，进行网络训练，可见其强大之处非同一般啊。 近年来深度学习捷报连连、">
<meta property="og:locale" content="zh">
<meta property="og:image" content="https://hhyz.me/img/2018-03-28-BatchNormalization-1.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-03-28-BatchNormalization-2.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-03-28-BatchNormalization-3.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-03-28-BatchNormalization-4.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-03-28-BatchNormalization-5.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-03-28-BatchNormalization-6.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-03-28-BatchNormalization-7.jpg">
<meta property="og:updated_time" content="2019-07-02T17:05:20.582Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Batch Normalization 学习笔记（转载）">
<meta name="twitter:description" content="一、背景意义本篇博文主要讲解2015年深度学习领域，非常值得学习的一篇文献：《Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift》，这个算法目前已经被大量的应用，最新的文献算法很多都会引用这个算法，进行网络训练，可见其强大之处非同一般啊。 近年来深度学习捷报连连、">
<meta name="twitter:image" content="https://hhyz.me/img/2018-03-28-BatchNormalization-1.jpg">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1">
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">





<script type="text/javascript">
  var themeConfig = {
    fancybox: {
      enable: false
    },
  };
</script>




  





  


    <title> Batch Normalization 学习笔记（转载） - 浩瀚宇宙·AaronChen </title>


    <script>
(function(u, c) {
  var d = document, t = 'script', o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
  o.src = u;
  if (c) { o.addEventListener('load', function(e) { c(e); }); }
  s.parentNode.insertBefore(o, s);
})('//cdn.bootcss.com/pangu/3.3.0/pangu.min.js', function() {
  pangu.spacingPage();
});
</script>


  </head>

  <body>
    <div id="page">
      <header id="masthead"><div class="site-header-inner">
    <h1 class="site-title">
        <a href="/." class="logo">浩瀚宇宙·AaronChen</a>
    </h1>

    <nav id="nav-top">
        
            <ul id="menu-top" class="nav-top-items">
                
                    <li class="menu-item">
                        <a href="https://www.hhyz.me">
                            
                            
                                Gallery
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/about">
                            
                            
                                About
                            
                        </a>
                    </li>
                
            </ul>
        
  </nav>
</div>

      </header>
      <div id="content">
        
    <div id="primary">
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          Batch Normalization 学习笔记（转载）
        
      </h1>

      <time class="post-time">
          Mar 28 2018
      </time>
    </header>



    
            <div class="post-content">
            <h2 id="一、背景意义"><a href="#一、背景意义" class="headerlink" title="一、背景意义"></a>一、背景意义</h2><p>本篇博文主要讲解2015年深度学习领域，非常值得学习的一篇文献：《Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift》，这个算法目前已经被大量的应用，最新的文献算法很多都会引用这个算法，进行网络训练，可见其强大之处非同一般啊。</p>
<p>近年来深度学习捷报连连、声名鹊起，随机梯度下架成了训练深度网络的主流方法。尽管随机梯度下降法对于训练深度网络简单高效，但是它有个毛病，就是需要我们人为的去选择参数，比如学习率、参数初始化、权重衰减系数、Drop out比例等。这些参数的选择对训练结果至关重要，以至于我们很多时间都浪费在这些的调参上。那么学完这篇文献之后，你可以不需要那么刻意的慢慢调整参数。BN算法（Batch Normalization）其强大之处如下：</p>
<ol>
<li><p>你可以选择比较大的初始学习率，让你的训练速度飙涨。以前还需要慢慢调整学习率，甚至在网络训练到一半的时候，还需要想着学习率进一步调小的比例选择多少比较合适，现在我们可以采用初始很大的学习率，然后学习率的衰减速度也很大，因为这个算法收敛很快。当然这个算法即使你选择了较小的学习率，也比以前的收敛速度快，因为它具有快速训练收敛的特性；</p>
</li>
<li><p>你再也不用去理会过拟合中drop out、L2正则项参数的选择问题，采用BN算法后，你可以移除这两项了参数，或者可以选择更小的L2正则约束参数了，因为BN具有提高网络泛化能力的特性；</p>
</li>
<li><p>再也不需要使用使用局部响应归一化层了（局部响应归一化是Alexnet网络用到的方法，搞视觉的估计比较熟悉），因为BN本身就是一个归一化网络层；</p>
</li>
<li><p>可以把训练数据彻底打乱（防止每批训练的时候，某一个样本都经常被挑选到，文献说这个可以提高1%的精度，这句话我也是百思不得其解啊）。</p>
</li>
</ol>
<a id="more"></a>

<p>开始讲解算法前，先来思考一个问题：我们知道在神经网络训练开始前，都要对输入数据做一个归一化处理，那么具体为什么需要归一化呢？归一化后有什么好处呢？原因在于神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。</p>
<p>对于深度网络的训练是一个复杂的过程，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度。  </p>
<p>我们知道网络一旦train起来，那么参数就要发生更新，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。我们把网络中间层在训练过程中，数据分布的改变称之为：“Internal  Covariate Shift”。Paper所提出的算法，就是要解决在训练过程中，中间层数据分布发生改变的情况，于是就有了Batch  Normalization，这个牛逼算法的诞生。</p>
<h2 id="二、初识BN-Batch-Normalization"><a href="#二、初识BN-Batch-Normalization" class="headerlink" title="二、初识BN(Batch  Normalization)"></a>二、初识BN(Batch  Normalization)</h2><h3 id="1、BN概述"><a href="#1、BN概述" class="headerlink" title="1、BN概述"></a>1、BN概述</h3><p>就像激活函数层、卷积层、全连接层、池化层一样，BN(Batch Normalization)也属于网络的一层。在前面我们提到网络除了输出层外，其它层因为低层网络在训练的时候更新了参数，而引起后面层输入数据分布的变化。这个时候我们可能就会想，如果在每一层输入的时候，再加个预处理操作那该有多好啊，比如网络第三层输入数据X3(X3表示网络第三层的输入数据)把它归一化至：均值0、方差为1，然后再输入第三层计算，这样我们就可以解决前面所提到的“Internal Covariate Shift”的问题了。</p>
<p>而事实上，paper的算法本质原理就是这样：在网络的每一层输入的时候，又插入了一个归一化层，也就是先做一个归一化处理，然后再进入网络的下一层。不过文献归一化层，可不像我们想象的那么简单，它是一个可学习、有参数的网络层。既然说到数据预处理，下面就先来复习一下最强的预处理方法：白化。</p>
<h3 id="2、预处理操作选择"><a href="#2、预处理操作选择" class="headerlink" title="2、预处理操作选择"></a>2、预处理操作选择</h3><p>说到神经网络输入数据预处理，最好的算法莫过于白化预处理。然而白化计算量太大了，很不划算，还有就是白化不是处处可微的，所以在深度学习中，其实很少用到白化。经过白化预处理后，数据满足条件：a、特征之间的相关性降低，这个就相当于pca；b、数据均值、标准差归一化，也就是使得每一维特征均值为0，标准差为1。如果数据特征维数比较大，要进行PCA，也就是实现白化的第1个要求，是需要计算特征向量，计算量非常大，于是为了简化计算，作者忽略了第1个要求，仅仅使用了下面的公式进行预处理，也就是近似白化预处理：</p>
<p><img src="/img/2018-03-28-BatchNormalization-1.jpg" alt>  </p>
<p>公式简单粗糙，但是依旧很牛逼。因此后面我们也将用这个公式，对某一个层网络的输入数据做一个归一化处理。需要注意的是，我们训练过程中采用batch 随机梯度下降，上面的E(xk)指的是每一批训练数据神经元xk的平均值；然后分母就是每一批数据神经元xk激活度的一个标准差了。</p>
<h2 id="三、BN算法实现"><a href="#三、BN算法实现" class="headerlink" title="三、BN算法实现"></a>三、BN算法实现</h2><h3 id="1、BN算法概述"><a href="#1、BN算法概述" class="headerlink" title="1、BN算法概述"></a>1、BN算法概述</h3><p>经过前面简单介绍，这个时候可能我们会想当然的以为：好像很简单的样子，不就是在网络中间层数据做一个归一化处理嘛，这么简单的想法，为什么之前没人用呢？然而其实实现起来并不是那么简单的。其实如果是仅仅使用上面的归一化公式，对网络某一层A的输出数据做归一化，然后送入网络下一层B，这样是会影响到本层网络A所学习到的特征的。打个比方，比如我网络中间某一层学习到特征数据本身就分布在S型激活函数的两侧，你强制把它给我归一化处理、标准差也限制在了1，把数据变换成分布于s函数的中间部分，这样就相当于我这一层网络所学习到的特征分布被你搞坏了，这可怎么办？于是文献使出了一招惊天地泣鬼神的招式：变换重构，引入了可学习参数γ、β，这就是算法关键之处：</p>
<p><img src="/img/2018-03-28-BatchNormalization-2.jpg" alt> </p>
<p>每一个神经元xk都会有一对这样的参数γ、β。这样其实当：</p>
<p><img src="/img/2018-03-28-BatchNormalization-3.jpg" alt>、<img src="/img/2018-03-28-BatchNormalization-4.jpg" alt>  </p>
<p>是可以恢复出原始的某一层所学到的特征的。因此我们引入了这个可学习重构参数γ、β，让我们的网络可以学习恢复出原始网络所要学习的特征分布。最后Batch Normalization网络层的前向传导过程公式就是：</p>
<p> <img src="/img/2018-03-28-BatchNormalization-5.jpg" alt></p>
<p>上面的公式中m指的是mini-batch size。</p>
<h3 id="2、源码实现"><a href="#2、源码实现" class="headerlink" title="2、源码实现"></a>2、源码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">m = K.mean(X, axis=<span class="number">-1</span>, keepdims=<span class="literal">True</span>)<span class="comment">#计算均值  </span></span><br><span class="line">std = K.std(X, axis=<span class="number">-1</span>, keepdims=<span class="literal">True</span>)<span class="comment">#计算标准差  </span></span><br><span class="line">X_normed = (X - m) / (std + self.epsilon)<span class="comment">#归一化  </span></span><br><span class="line">out = self.gamma * X_normed + self.beta<span class="comment">#重构变换</span></span><br></pre></td></tr></table></figure>

<p>上面的x是一个二维矩阵，对于源码的实现就几行代码而已，轻轻松松。  </p>
<h3 id="3、实战使用"><a href="#3、实战使用" class="headerlink" title="3、实战使用"></a>3、实战使用</h3><p>(1)可能学完了上面的算法，你只是知道它的一个训练过程，一个网络一旦训练完了，就没有了min-batch这个概念了。测试阶段我们一般只输入一个测试样本，看看结果而已。因此测试样本，前向传导的时候，上面的均值u、标准差σ 要哪里来？其实网络一旦训练完毕，参数都是固定的，这个时候即使是每批训练样本进入网络，那么BN层计算的均值u、和标准差都是固定不变的。我们可以采用这些数值来作为测试样本所需要的均值、标准差，于是最后测试阶段的u和σ 计算公式如下：</p>
<p><img src="/img/2018-03-28-BatchNormalization-6.jpg" alt>  </p>
<p>上面简单理解就是：对于均值来说直接计算所有batch u值的平均值；然后对于标准偏差采用每个batch σB的无偏估计。最后测试阶段，BN的使用公式就是：</p>
<p><img src="/img/2018-03-28-BatchNormalization-7.jpg" alt>  </p>
<p>(2)根据文献说，BN可以应用于一个神经网络的任何神经元上。文献主要是把BN变换，置于网络激活函数层的前面。在没有采用BN的时候，激活函数层是这样的：</p>
<p><code>z=g(Wu+b)</code></p>
<p>也就是我们希望一个激活函数，比如s型函数s(x)的自变量x是经过BN处理后的结果。因此前向传导的计算公式就应该是：</p>
<p><code>z=g(BN(Wu+b))</code></p>
<p>其实因为偏置参数b经过BN层后其实是没有用的，最后也会被均值归一化，当然BN层后面还有个β参数作为偏置项，所以b这个参数就可以不用了。因此最后把BN层+激活函数层就变成了：</p>
<p><code>z=g(BN(Wu))</code></p>
<h2 id="四、Batch-Normalization在CNN中的使用"><a href="#四、Batch-Normalization在CNN中的使用" class="headerlink" title="四、Batch Normalization在CNN中的使用"></a>四、Batch Normalization在CNN中的使用</h2><p>通过上面的学习，我们知道BN层是对于每个神经元做归一化处理，甚至只需要对某一个神经元进行归一化，而不是对一整层网络的神经元进行归一化。既然BN是对单个神经元的运算，那么在CNN中卷积层上要怎么搞？假如某一层卷积层有6个特征图，每个特征图的大小是100<em>100，这样就相当于这一层网络有6</em>100<em>100个神经元，如果采用BN，就会有6</em>100*100个参数γ、β，这样岂不是太恐怖了。因此卷积层上的BN使用，其实也是使用了类似权值共享的策略，把一整张特征图当做一个神经元进行处理。</p>
<p>卷积神经网络经过卷积后得到的是一系列的特征图，如果min-batch sizes为m，那么网络某一层输入数据可以表示为四维矩阵(m,f,p,q)，m为min-batch sizes，f为特征图个数，p、q分别为特征图的宽高。在cnn中我们可以把每个特征图看成是一个特征处理（一个神经元），因此在使用Batch Normalization，mini-batch size 的大小就是：m<em>p</em>q，于是对于每个特征图都只有一对可学习参数：γ、β。说白了吧，这就是相当于求取所有样本所对应的一个特征图的所有神经元的平均值、方差，然后对这个特征图神经元做归一化。下面是来自于keras卷积层的BN实现一小段主要源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">input_shape = self.input_shape  </span><br><span class="line"> reduction_axes = list(range(len(input_shape)))  </span><br><span class="line"> <span class="keyword">del</span> reduction_axes[self.axis]  </span><br><span class="line"> broadcast_shape = [<span class="number">1</span>] * len(input_shape)  </span><br><span class="line"> broadcast_shape[self.axis] = input_shape[self.axis]  </span><br><span class="line"> <span class="keyword">if</span> train:  </span><br><span class="line">     m = K.mean(X, axis=reduction_axes)  </span><br><span class="line">     brodcast_m = K.reshape(m, broadcast_shape)  </span><br><span class="line">     std = K.mean(K.square(X - brodcast_m) + self.epsilon, axis=reduction_axes)  </span><br><span class="line">     std = K.sqrt(std)  </span><br><span class="line">     brodcast_std = K.reshape(std, broadcast_shape)  </span><br><span class="line">     mean_update = self.momentum * self.running_mean + (<span class="number">1</span>-self.momentum) * m  </span><br><span class="line">     std_update = self.momentum * self.running_std + (<span class="number">1</span>-self.momentum) * std  </span><br><span class="line">     self.updates = [(self.running_mean, mean_update),  </span><br><span class="line">                     (self.running_std, std_update)]  </span><br><span class="line">     X_normed = (X - brodcast_m) / (brodcast_std + self.epsilon)  </span><br><span class="line"> <span class="keyword">else</span>:  </span><br><span class="line">     brodcast_m = K.reshape(self.running_mean, broadcast_shape)  </span><br><span class="line">     brodcast_std = K.reshape(self.running_std, broadcast_shape)  </span><br><span class="line">     X_normed = ((X - brodcast_m) /  </span><br><span class="line">                 (brodcast_std + self.epsilon))  </span><br><span class="line"> out = K.reshape(self.gamma, broadcast_shape) * X_normed + K.reshape(self.beta, broadcast_shape)</span><br></pre></td></tr></table></figure>

<p>个人总结：2015年个人最喜欢深度学习的一篇paper就是Batch Normalization这篇文献，采用这个方法网络的训练速度快到惊人啊，感觉训练速度是以前的十倍以上，再也不用担心自己这破电脑每次运行一下，训练一下都要跑个两三天的时间。另外这篇文献跟空间变换网络《Spatial Transformer Networks》的思想神似啊，都是一个变换网络层。</p>
<p><strong>参考文献：</strong></p>
<ol>
<li>《Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift》</li>
<li>《Spatial Transformer Networks》</li>
<li><a href="https://github.com/fchollet/keras" target="_blank" rel="noopener">https://github.com/fchollet/keras</a></li>
</ol>
<p><strong>原文地址</strong>：<a href="http://blog.csdn.net/hjimce/article/details/50866313" target="_blank" rel="noopener">http://blog.csdn.net/hjimce/article/details/50866313</a></p>
<p><strong>作者</strong>：hjimce</p>

            </div>
          

    
      <footer class="post-footer">
        <div class="post-tags">
          
            <a href="/tags/深度学习/">深度学习</a>
          
        </div>

        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2018/04/04/pysql/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">pymysql 基础操作（用 python 操作数据库）</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/2018/03/26/mnist手写字识别/">
        <span class="next-text nav-default">TensorFlow 实现 mnist 手写字识别</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

        
  <div class="comments" id="comments">
    
  </div>


      </footer>
    
  </article>

    </div>


      <div class="post-toc-warp">
        
      <!--noindex-->
          <div class="post-toc" id="toc-test">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#一、背景意义"><span class="nav-text">一、背景意义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#二、初识BN-Batch-Normalization"><span class="nav-text">二、初识BN(Batch  Normalization)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1、BN概述"><span class="nav-text">1、BN概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2、预处理操作选择"><span class="nav-text">2、预处理操作选择</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#三、BN算法实现"><span class="nav-text">三、BN算法实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1、BN算法概述"><span class="nav-text">1、BN算法概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2、源码实现"><span class="nav-text">2、源码实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3、实战使用"><span class="nav-text">3、实战使用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#四、Batch-Normalization在CNN中的使用"><span class="nav-text">四、Batch Normalization在CNN中的使用</span></a></li></ol></div>
            

            <div class="back-to-top" id="back-to-top">
              <i class="iconfont icon-up"></i>
            </div>

          </div>
      <!--/noindex-->
      
      </div>


      </div>
      <footer id="colophon"><span class="copyright-year">
    
        &copy;
    
        2017 -
    
    2019
    <span class="footer-author">haoyu.</span>
    <span class="power-by">
        Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a> and <a class="theme-link" href="https://github.com/henryhuang/hexo-theme-polarbearsimple">Polar Bear Simple</a>
    </span>
</span>

      </footer>


    </div>
    


    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  

    <script type="text/javascript" src="/js/src/theme.js?v=1.1"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=1.1"></script>

  </body>
</html>
