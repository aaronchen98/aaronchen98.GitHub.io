<!DOCTYPE html>
<html lang>
  <head><meta name="generator" content="Hexo 3.9.0">
    
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,minimum-scale=1,maximum-scale=1">


<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="description" content="CNN 优化网络总结">




  <meta name="keywords" content="深度学习,">





  <link rel="alternate" href="/atom.xml" title="浩瀚宇宙·AaronChen">




  <link rel="shortcut icon" type="image/x-icon" href="/img/1.jpg?v=1.1">



<link rel="canonical" href="https://hhyz.me/2018/06/24/youhua/">


<meta name="description" content="新技术深度可分离卷积 将标准卷积分解成一个深度卷积和一个点卷积（1 × 1卷积核）               直观上来看，这种分解在效果上确实是等价的。    比如，把上图的代号化为实际的数字，输入图片维度是 \(11 × 11 × 3\) ，标准卷积为 \(3 × 3 × 3 ×16\) （假设stride为 2，padding为 1），那么可以得到输出为 \(6 × 6 × 16\) 的输出">
<meta name="keywords" content="深度学习">
<meta property="og:type" content="article">
<meta property="og:title" content="CNN 优化网络总结">
<meta property="og:url" content="https://hhyz.me/2018/06/24/youhua/index.html">
<meta property="og:site_name" content="浩瀚宇宙·AaronChen">
<meta property="og:description" content="新技术深度可分离卷积 将标准卷积分解成一个深度卷积和一个点卷积（1 × 1卷积核）               直观上来看，这种分解在效果上确实是等价的。    比如，把上图的代号化为实际的数字，输入图片维度是 \(11 × 11 × 3\) ，标准卷积为 \(3 × 3 × 3 ×16\) （假设stride为 2，padding为 1），那么可以得到输出为 \(6 × 6 × 16\) 的输出">
<meta property="og:locale" content="zh">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-101.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-1.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-2.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-3.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-102.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-103.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-4.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-5.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-6.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-7.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-8.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-9.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-10.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-11.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-104.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-12.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-13.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-14.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-15.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-16.jpg">
<meta property="og:image" content="https://hhyz.me/img/2018-06-24-youhua-17.jpg">
<meta property="og:updated_time" content="2019-07-04T17:26:39.002Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CNN 优化网络总结">
<meta name="twitter:description" content="新技术深度可分离卷积 将标准卷积分解成一个深度卷积和一个点卷积（1 × 1卷积核）               直观上来看，这种分解在效果上确实是等价的。    比如，把上图的代号化为实际的数字，输入图片维度是 \(11 × 11 × 3\) ，标准卷积为 \(3 × 3 × 3 ×16\) （假设stride为 2，padding为 1），那么可以得到输出为 \(6 × 6 × 16\) 的输出">
<meta name="twitter:image" content="https://hhyz.me/img/2018-06-24-youhua-101.jpg">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1">
<link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">





<script type="text/javascript">
  var themeConfig = {
    fancybox: {
      enable: false
    },
  };
</script>




  





  


    <title> CNN 优化网络总结 - 浩瀚宇宙·AaronChen </title>
    <script>
(function(u, c) {
  var d = document, t = 'script', o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
  o.src = u;
  if (c) { o.addEventListener('load', function(e) { c(e); }); }
  s.parentNode.insertBefore(o, s);
})('//cdn.bootcss.com/pangu/3.3.0/pangu.min.js', function() {
  pangu.spacingPage();
});
</script>
  </head>

  <body>
    <div id="page">
      <header id="masthead"><div class="site-header-inner">
    <h1 class="site-title">
        <a href="/." class="logo">浩瀚宇宙·AaronChen</a>
    </h1>

    <nav id="nav-top">
        
            <ul id="menu-top" class="nav-top-items">
                
                    <li class="menu-item">
                        <a href="https://www.hhyz.me">
                            
                            
                                Gallery
                            
                        </a>
                    </li>
                
                    <li class="menu-item">
                        <a href="/about">
                            
                            
                                About
                            
                        </a>
                    </li>
                
            </ul>
        
  </nav>
</div>

      </header>
      <div id="content">
        
    <div id="primary">
        
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          CNN 优化网络总结
        
      </h1>

      <time class="post-time">
          Jun 24 2018
      </time>
    </header>



    
            <div class="post-content">
            <h2 id="新技术"><a href="#新技术" class="headerlink" title="新技术"></a>新技术</h2><h3 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a>深度可分离卷积</h3><blockquote>
<p><strong>将标准卷积分解成一个深度卷积和一个点卷积（1 × 1卷积核）</strong></p>
</blockquote>
<p>  <img src="/img/2018-06-24-youhua-101.jpg" alt></p>
<div align="center">

<center><small><font color="gray">    </font></small></center>
</div>


<p>直观上来看，这种分解在效果上确实是等价的。   </p>
<p>比如，把上图的代号化为实际的数字，输入图片维度是 \(11 × 11 × 3\) ，标准卷积为 \(3 × 3 × 3 ×16\) （假设stride为 2，padding为 1），那么可以得到输出为 \(6 × 6 × 16\) 的输出结果。   </p>
<p>现在输入图片不变，先通过一个维度是 \(3 × 3 × 1 × 3\) 的深度卷积（输入是3通道，这里有3个卷积核，对应着进行计算，理解成for循环），得到 \(6 × 6 × 3\) 的中间输出，然后再通过一个维度是 \(1 × 1 × 3 ×16\) 的 \(1 ×1\) 卷积，同样得到输出为 \(6 × 6 × 16\) 。</p>
<p><strong>运算量：</strong></p>
<p>标准卷积：</p>
<p>$$D_K · D_K · M · N · D_F · D_F$$</p>
<p>深度卷积：</p>
<p>$$D_K·D_K·M·D_F·D_F$$</p>
<p>点卷积：</p>
<p>$$M·N·D_F·D_F$$</p>
<p>深度可分离卷积：</p>
<p>$$D_K·D_K·M·D_F·D_F+M·N·D_F·D_F$$</p>
<p>深度可分离卷积与标准卷积之比：</p>
<p>$$\frac{D_K·D_K·M·D_F·D_F+M·N·D_F·D_F}{D_K · D_K · M · N · D_F · D_F} = {\frac{1}{N}}+{\frac{1}{D_K^2}}$$</p>
<p>使用大量的 \(3 × 3\) 的卷积核，极大地减少了计算量（原来的 1/ 8到 1/9 之间）</p>
<p><strong>举例：</strong></p>
<p>给定输入图像的为3通道的 \(224×224\) 的图像，VGG16网络的第3个卷积层conv2_1输入的是尺寸为112的特征图，通道数为64，卷积核尺寸为3，卷积核个数为128</p>
<p>传统卷积运算量就是：</p>
<p>$$3×3×128×64×112×112=924844032$$</p>
<p>deep-wise 结合  \(1×1\)  方式的卷积的运算量：</p>
<p>$$112×112×3×3×64 + 128×64×112×112 = 109985792$$</p>
<p>两者比例为：</p>
<p>$$\frac{109985792}{924844032} = 0.1189$$</p>
<p><strong>优点：</strong></p>
<p>可以看出运用深度可分离卷积比普通卷积减少了所需要的参数。重要的是深度可分离卷积将以往普通卷积操作同时考虑通道和区域改变成，<em><strong>卷积先只考虑区域，然后再考虑通道。实现了通道和区域的分离。</strong></em></p>
<br>

<h3 id="群卷积"><a href="#群卷积" class="headerlink" title="群卷积"></a>群卷积</h3><p>我们假设上一层的输出 feature map 有 \(N\) 个，即通道数 \(channel=N\) ，也就是说上一层有 \(N\) 个卷积核。再假设群卷积的群数目 \(M\) 。那么该群卷积层的操作就是，先将 channel 分成 \(M\) 份。每一个 group 对应 \(N/M\) 个 channel，与之独立连接。然后各个 group 卷积完成后将输出叠在一起（concatenate），作为这一层的输出 channel。</p>
<p><img src="/img/2018-06-24-youhua-1.jpg" alt></p>
<div align="center">

<center><small><font color="gray">    </font></small></center>
</div>


<p>下图则是一个群卷积的 CNN 结构。filters 被分成了两个 group。每一个 group 都只有原来一半的 feature map</p>
<p><img src="/img/2018-06-24-youhua-2.jpg" alt></p>
<div align="center">

<center><small><font color="gray">    </font></small></center>
</div>



<p>可以看到，图中将输入数据分成了 2 组（组数为 \(g\) ），需要注意的是，<strong>这种分组只是在深度上进行划分，即某几个通道编为一组，</strong>这个具体的数量由 \(C_1/g\) 决定。 </p>
<p>因为输出数据的改变，相应的，卷积核也需要做出同样的改变。即每组中卷积核的深度也就变成了 \(C_1/g\) ，而卷积核的大小是不需要改变的，此时每组的卷积核的个数就变成了 \(C_2/g\) 个，而不是原来的 \(C_2\) 了。然后用每组的卷积核同它们对应组内的输入数据卷积，得到了输出数据以后，再用 concatenate 的方式组合起来，最终的输出数据的通道仍旧是 \(C_2\) 。     </p>
<p>也就是说，分组数 \(g\) 决定以后，那么我们将并行的运算 \(g\) 个相同的卷积过程，每个过程里（每组），输入数据为 \(H_1×W_1×C_1/g\) ，卷积核大小为 \(h_1×w_1×C_1/g\) ，一共有 \(C_2/g\) 个，输出数据为 \(H_2×W_2×C_2/g\) </p>
<br>

<h2 id="1-MobileNet"><a href="#1-MobileNet" class="headerlink" title="1. MobileNet"></a>1. MobileNet</h2><blockquote>
<p><strong>论文地址：</strong><a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a><br><strong>发布时间</strong>：2017 年 4 月<br><strong>官方代码：</strong><a href="https://github.com/tensorflow/models/tree/5bb9e6f349e22270420dd637f3fa89260ab5b441/research/slim/nets/mobilenet" target="_blank" rel="noopener">tensorflow/model/sresearch/slim/nets/mobilenet/</a><br><strong>民间实现：</strong><a href="https://github.com/shicai/MobileNet-Caffe" target="_blank" rel="noopener">caffe</a> | <a href="https://github.com/Zehaos/MobileNet" target="_blank" rel="noopener">Tensorflow</a> </p>
</blockquote>
<p>MobileNets结构建立在上述深度可分解卷积中（只有第一层是标准卷积）。该网络允许我们探索网络拓扑，找到一个适合的良好网络。其具体架构在表1说明。除了最后的全连接层，所有层后面跟了batchnorm和ReLU，最终输入到softmax进行分类。二者都附带了BN和ReLU层。</p>
<p>按照作者的计算方法，MobileNets 总共 28 层 \(（1 + 2 × 13 + 1 = 28）\) 。</p>
<p><img src="/img/2018-06-24-youhua-3.jpg" alt></p>
<div align="center">
<center><small><font color="gray">  对比标准卷积和分解卷积的结构, 表格中含有dw的就表示这一层采用了 deep-wise 结合 1x1 的方式  </font></small></center>
</div>


<p><img src="/img/2018-06-24-youhua-102.jpg" alt></p>
<div align="center">
<center><small><font color="gray">  传统卷积（左）与深度可分离卷积（右）在 MobileNet 中的区别   </font></small></center>
</div>


<blockquote>
<p>deep-wise 的卷积和后面的  \(1×1\)  卷积被当成了两个独立的模块，都在输出结果的部分加入了 Batch Normalization 和非线性激活单元</p>
</blockquote>
<p>下图为 MobileNet 对于不同结构单元在计算量和参数数量方面的统计:<br> <img src="/img/2018-06-24-youhua-103.jpg" alt></p>
<div align="center">

<center><small><font color="gray">  在MobileNet中，有95%的计算量和75%的参数属于 \\(1×1\\) 卷积    </font></small></center>
</div>




<br>
### 训练细节

<p>作者基于 TensorFlow 训练 MobileNet，使用 RMSprop 算法优化网络参数。考虑到较小的网络不会有严重的过拟合问题，因此没有做大量的数据增强工作。在训练过程中也没有采用训练大网络时的一些常用手段，例如：辅助损失函数，随机图像裁剪输入等。</p>
<p>deep-wise 卷积核含有的参数较少，作者发现这部分最好使用较小的 weight decay或者不使用 weightdecay。</p>
<br>
### 宽度因子和分辨率因子

<blockquote>
<p>尽管标准的MobileNet在计算量和模型尺寸方面具备了很明显的优势，但是，在一些对运行速度或内存有极端要求的场合，还需要更小更快的模型，如何能够在不重新设计模型的情况下，以最小的改动就可以获得更小更快的模型呢？</p>
</blockquote>
<blockquote>
<p>本文中提出的<strong>宽度因子（width multiplier）</strong>和<strong>分辨率因子（resolution multiplier）</strong>就是解决这些问题的配置参数。</p>
</blockquote>
<p><strong>宽度因子</strong>  \(\alpha \) 是一个属于 \((0,1]\) 之间的数，附加于网络的通道数。简单来说就是新网络中每一个模块要使用的卷积核数量相较于标准的 MobileNet 比例。</p>
<p>输入通道数  \(M\)  变为  \(\alpha M\) ;<br>输出通道数 \(N\) 变为  \(\alpha N\) </p>
<p>对于 deep-wise 结合  \(1x1\)  方式的卷积核，计算量为：</p>
<p>$$D_K·D_K·\alpha M·D_F·D_F+\alpha M·\alpha N·D_F·D_F$$</p>
<p> \(\alpha \) 常用的配置为 \(1, 0.75, 0.5, 0.25\) 。当 \(\alpha \) 等于1时就是标准的 MobileNet。通过参数 \(\alpha \) 可以非常有效的将计算量和参数数量约减到 \(\alpha \) 的平方倍。</p>
<p>通过下图可以看出使用 \(\alpha \) 系数进行网络参数的约减时，在ImageNet上的准确率，为准确率，参数数量和计算量之间的权衡提供了参考（每一个项中最前面的数字表示 \(\alpha \) 的取值）</p>
<p><img src="/img/2018-06-24-youhua-4.jpg" alt></p>
<div align="center">
<center><small><font color="gray">    </font></small></center>
</div>

<br>

<p><strong>分辨率因子</strong> \(\beta\) 的取值范围在 \((0,1]\) 之间，是作用于每一个模块输入尺寸的约减因子，简单来说就是将输入数据以及由此在每一个模块产生的特征图都变小了，</p>
<p>结合宽度因子 \(\alpha \) ，deep-wise 结合 \(1x1\) 方式的卷积核计算量为：</p>
<p>$$D_K·D_K·\alpha M·\beta D_F·\beta D_F+\alpha M·\alpha N·\beta D_F·\beta D_F$$</p>
<p>下图为使用不同的 \(\beta\) 系数作用于标准 MobileNet 时，对精度和计算量以的影响（224，192，160，128指的是输入分辨率）:</p>
<p><img src="/img/2018-06-24-youhua-5.jpg" alt></p>
<div align="center">
<center><small><font color="gray">    </font></small></center>
</div>


<blockquote>
<p>要注意再使用宽度和分辨率参数调整网络结构之后，都要从随机初始化重新训练才能得到新网络</p>
</blockquote>
<br>
### 实验结果

<p><strong>1) 基础网络</strong></p>
<p>MobileNet极大地降低了网络参数数量和计算量，但是相比起经典的基础网络，其精度并未明显的降低</p>
<p><img src="/img/2018-06-24-youhua-6.jpg" alt></p>
<div align="center">

<center><small><font color="gray">   与VGG相比，在ImageNet分类任务上的精度差距较小   </font></small></center>
</div>


<p><img src="/img/2018-06-24-youhua-7.jpg" alt></p>
<div align="center">

<center><small><font color="gray"> 与经典的小型网络SqueezeNet相比，在精度和计算量方面都有明显提升   </font></small></center>
</div>



<p><strong>2) 人脸属性提取</strong></p>
<p>MobileNet 的一个使用情景是压缩具有未知或复杂训练程序的大型系统。在人脸属性分类任务中，本文证明了 MobileNet 与 distillation 间的协同关系，这是网络的一种知识迁移技术。本文在 YFCC100M 多属性数据集上训练。</p>
<p>使用 MobileNet 架构提取一个人脸属性分类器。distillation是通过训练分类器模拟一个更大的模型的输出，而非人工标注标签工作，因此能够从大型（可能是无限大）未标记数据集训练。</p>
<p><img src="/img/2018-06-24-youhua-8.jpg" alt></p>
<div align="center">

<center><small><font color="gray">  结合 distillation 的可扩展性和MobileNet 的简约参数化，相比于一个具有7500万超参数和16亿 Mult-Adds 的大型人脸属性分类器，终端系统不仅不需要正则化，而且表现出更好的性能  </font></small></center>
</div>

<p><strong>3) 人脸识别</strong></p>
<p>FaceNet 模型是目前顶级水平的人脸识别模型，它基于 triplet loss 构建 faceEmbedding。为了能够在移动设备上运行 FaceNet 模型，使用 distillation来最小化训练数据上 FaceNet和 MobileNet 输出的方差。</p>
<p>下图列出了输入尺寸较小的MobileNet在此任务上的性能表现。</p>
<p><img src="/img/2018-06-24-youhua-9.jpg" alt></p>
<div align="center">
<center><small><font color="gray">  在没有过分损失精度的情况下，运算量大大减少  </font></small></center>
</div>


<p><strong>4) 目标检测</strong></p>
<p>MobileNet 可以作为一个有效的基网络部署在目标检测系统上。基于2016 COCO 数据集，比较了在 COCO 数据上训练的 MobileNet 进行目标检测的结果。   </p>
<p>下图列出了在 Faster-RCNN 和 SSD 框架下，MobileNet，VGG 以及 Inception V2 作为基础网络的对比结果。</p>
<p><img src="/img/2018-06-24-youhua-10.jpg" alt></p>
<div align="center">
<center><small><font color="gray">  在不同而检测框架和输入尺寸设定下，以MobileNet为基础网络的检测框架表现出了不明显逊色于两个基础网络的性能，而且在计算量和模型尺寸方面有较大优势  </font></small></center>
</div>





<br>
### 总结

<p>MobileNet在计算量，存储空间和准确率方面取得了非常不错的平衡。与VGG16相比，在很小的精度损失情况下，将运算量减小了30倍。通过实验结果，我们认为MobileNet的设计思想会在自动驾驶汽车，机器人和无人机等对实时性、存储空间、能耗有严格要求的终端智能应用中发挥显著作用。</p>
<h2 id="2-Xception"><a href="#2-Xception" class="headerlink" title="2. Xception"></a>2. Xception</h2><blockquote>
<p>论文地址：<a href="https://arxiv.org/abs/1610.02357" target="_blank" rel="noopener">Xception: Deep Learning with Depthwise Separable Convolutions</a><br>发布时间：2016 年 10 月</p>
</blockquote>
<h2 id="3-ResNeXt"><a href="#3-ResNeXt" class="headerlink" title="3. ResNeXt"></a>3. ResNeXt</h2><blockquote>
<p>论文地址：<a href="https://arxiv.org/abs/1611.05431" target="_blank" rel="noopener">Aggregated Residual Transformations for Deep Neural Networks</a><br>发布时间：2016 年 11 月（v1）   2017 年 4 月 （v2）</p>
</blockquote>
<p>cardinality，原文的解释是 the size of the set of transformations。分支的个数就是 cardinality</p>
<p><strong>增加 cardinality 比增加深度和宽度更有效</strong></p>
<p><img src="/img/2018-06-24-youhua-11.jpg" alt></p>
<div align="center">
<center><small><font color="gray">    </font></small></center>
</div>


<p>计算一下便可得：由 Figure1 得知， </p>
<p>resnet 的参数量为：<br>$$256×64+3×3×64×64+64×256=70k $$<br>ResNeXt 的参数量为： </p>
<p>$$C×(256×d+3×3×d×d+d×256)$$</p>
<p>当 C 取 \(32\) ， \(d=4\) 时，上式也等于 \(70k\) 。 </p>
<p><img src="/img/2018-06-24-youhua-104.jpg" alt></p>
<div align="center">

<center><small><font color="gray">  在每个conv中，总的通道数要比Resnet多的多，但是两者的参数量是一样的   </font></small></center>
</div>


<p><img src="/img/2018-06-24-youhua-12.jpg" alt></p>
<div align="center">

<center><small><font color="gray">  三种等价设计  </font></small></center>
</div>



<h2 id="4-ShuffleNet"><a href="#4-ShuffleNet" class="headerlink" title="4. ShuffleNet"></a>4. ShuffleNet</h2><blockquote>
<p>论文地址：<a href="https://arxiv.org/abs/1707.01083" target="_blank" rel="noopener">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</a><br><strong>发布时间：</strong>2017 年 7 月（v1）    2017 年 12 月（v2）</p>
</blockquote>
<p>其中，最先进的网络例如 Xception 和 ResNeXt 将有效的深度可分离卷积或群卷积引入构建 block 中，在表示能力和计算消耗之间取得很好的折中。但是，我们注意到这<strong>两个设计都没有充分采用 \(1×1\) 的逐点卷积</strong>，因为这需要很大的计算复杂度。例如，在 ResNeXt 中 \(3×3\) 卷积配有群卷积，逐点卷积占了 93.4% 的 multiplication-adds。</p>
<p><strong>在小型网络中，昂贵的逐点卷积造成有限的通道之间充满约束，这会显著的损失精度。</strong>为了解决这个问题，一个直接的方法是<strong>应用通道稀疏连接，例如组卷积(group convolutions)</strong>。通过确保每个卷积操作仅在对应的输入通道组上，组卷积可以显著的降低计算损失。</p>
<p>然而，如果多个组卷积堆叠在一起，会有一个副作用： 某个通道输出仅从一小部分输入通道中导出，如下图(a)所示，<strong>这样的属性降低了通道组之间的信息流通，降低了信息表示能力。</strong><br><img src="/img/2018-06-24-youhua-13.jpg" alt></p>
<div align="center">
<center><small><font color="gray">    </font></small></center>
</div>



<p>如果我们<strong>允许组卷积能够得到不同组的输入数据</strong>，即上图(b)所示效果，那么<strong>输入和输出通道会是全关联的</strong>。具体来说，对于上一层输出的通道，我们可做一个混洗 (Shuffle) 操作，如上图(c)所示，再分成几个组，feed 到下一层。</p>
<p>对于这个混洗操作，有一个有效高雅(efficiently and elegantly)的实现:</p>
<p>对于一个卷积层分为g组，有一个有效高雅(efficiently and elegantly)的实现:</p>
<ol>
<li>有 \(g×n\) 个输出通道</li>
<li>reshape 为 \((g,n)\) </li>
<li>再转置为 \((n,g)\) </li>
<li>平坦化,再分回 \(g\) 组作为下一层的输入</li>
</ol>
<p>示意图如下：</p>
<p><img src="/img/2018-06-24-youhua-14.jpg" alt></p>
<div align="center">

<center><small><font color="gray">    </font></small></center>
</div>



<h3 id="Shuffle-unit"><a href="#Shuffle-unit" class="headerlink" title="Shuffle unit"></a>Shuffle unit</h3><p>我们已经了解通道混洗的好，而在实际过程中我们构建了一个 <strong>ShuffleNet unit</strong>，便于构建实际模型。</p>
<p><img src="/img/2018-06-24-youhua-15.jpg" alt></p>
<div align="center">

<center><small><font color="gray">    </font></small></center>
</div>


<ul>
<li><p>图(a)是一个残差模块。对于主分支部分，我们可将其中标准卷积3×33×3拆分成深度分离卷积。我们将第一个 \(1×1\) 卷积替换为逐点组卷积，再作通道混洗(即(b))。其中的 DWConv 指的是 depthwise convolution。</p>
</li>
<li><p>图(b)即 ShuffleNet unit，相比于图a,只是将第一个 \(1x1\) 卷积改成了 group convolution，同时后续增加通道 shuffle。</p>
</li>
<li><p>图(c)即是做降采样的 ShuffleNet unit，这主要做了两点修改：</p>
<ul>
<li>在辅分支加入步长为2的 \(3×3\) 平均池化</li>
<li>原本做元素相加的操作转为了通道级联，这扩大了通道维度，增加的计算成本却很少</li>
</ul>
</li>
</ul>
<p>归功于逐点群卷积和通道混洗，ShuffleNet unit 可以高效的计算。相比于其他先进的单元，在相同设置下复杂度较低。</p>
<p>例如：给定输入大小 \(h×w×c\) ,通道数为 \(c\) 。对于的 bottleneck 通道为mm:</p>
<ul>
<li>ResNet unit 需要 \(h·w·(2·c·m+9m^2)FLOPS\) 计算量。</li>
<li>ResNeXt 需要 \(h·w(2·c·m+9·m^2/g)FLOPS\) </li>
<li>而 ShuffleNet unit 只需要 \( h·w(2·c·m/g+9·m)FLOPS\) . </li>
</ul>
<p>其中 \(g\) 代表组卷积数目。即表示：<strong>给定一个计算限制，ShuffleNet 可以使用更宽的特征映射。我们发现这对小型网络很重要，因为小型网络没有足够的通道传递信息</strong></p>
<blockquote>
<p><strong>需要注意的是：</strong>虽然深度卷积可以减少计算量和参数量，但在低功耗设备上，与密集的操作相比，计算/存储访问的效率更差。故在ShuffleNet上我们只在bottleneck上使用深度卷积，尽可能的减少开销。</p>
</blockquote>
<h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><p>在上面的基本单元基础上，我们提出了ShuffleNet的整体架构：</p>
<p><img src="/img/2018-06-24-youhua-16.jpg" alt></p>
<div align="center">
<center><small><font color="gray">    </font></small></center>
</div>





<h3 id="实验性能"><a href="#实验性能" class="headerlink" title="实验性能"></a>实验性能</h3><p><img src="/img/2018-06-24-youhua-17.jpg" alt></p>
<div align="center">
<center><small><font color="gray"> 不同配置下的网络性能对比图   </font></small></center>
</div>

<ul>
<li>“num x”表示 feature map 的 channels 数量与原始结构的比例</li>
<li>arch2: 将 stage3 中的两个单元移除，同时增加整体的网络宽度。对于小网络，feture map的宽度至关重要</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>ShuffleNet 的核心就是用 pointwise group convolution，channel shuffle 和 depthwise separable convolution 代替 ResNet block 的相 应层构成了 ShuffleNet uint，达到了减少计算量和提高准确率的目的。</p>
<p>channel shuffle 解决了多个 group convolution 叠加出现的边界效应，pointwise group convolution 和 depthwise separable convolution 主要减少了计算量。</p>
<h2 id="转载与参考"><a href="#转载与参考" class="headerlink" title="转载与参考"></a>转载与参考</h2><ol>
<li><a href="https://blog.csdn.net/t800ghb/article/details/78879612" target="_blank" rel="noopener">深度解读谷歌MobileNet</a></li>
<li><a href="https://blog.csdn.net/wfei101/article/details/78310226" target="_blank" rel="noopener">mobilenet网络的理解</a></li>
<li><a href="https://blog.csdn.net/u011974639/article/details/79200559" target="_blank" rel="noopener">轻量级网络–ShuffleNet论文解读</a></li>
</ol>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
            </div>
          

    
      <footer class="post-footer">
        <div class="post-tags">
          
            <a href="/tags/深度学习/">深度学习</a>
          
        </div>

        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2018/06/25/tqdm/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">python 进度条包 tqdm 的使用</span>
        <span class="prev-text nav-mobile">Prev</span>
      </a>
    
    
      <a class="next" href="/2018/06/22/cv2/">
        <span class="next-text nav-default">计算机视觉基础</span>
        <span class="prev-text nav-mobile">Next</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

        
  <div class="comments" id="comments">
    
  </div>


      </footer>
    
  </article>

    </div>



        
      <!--noindex-->
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#新技术"><span class="nav-text">新技术</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#深度可分离卷积"><span class="nav-text">深度可分离卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#群卷积"><span class="nav-text">群卷积</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-MobileNet"><span class="nav-text">1. MobileNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Xception"><span class="nav-text">2. Xception</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-ResNeXt"><span class="nav-text">3. ResNeXt</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-ShuffleNet"><span class="nav-text">4. ShuffleNet</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Shuffle-unit"><span class="nav-text">Shuffle unit</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#整体架构"><span class="nav-text">整体架构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#实验性能"><span class="nav-text">实验性能</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#转载与参考"><span class="nav-text">转载与参考</span></a></li></ol></div>
            

            <div class="back-to-top" id="back-to-top">
              <i class="iconfont icon-up"></i>
            </div>

          </div>
      <!--/noindex-->
      



      </div>
      <footer id="colophon"><span class="copyright-year">
    
        &copy;
    
        2017 -
    
    2019
    <span class="footer-author">haoyu.</span>
    <span class="power-by">
        Powered by <a class="hexo-link" href="https://hexo.io/">Hexo</a> and <a class="theme-link" href="https://github.com/henryhuang/hexo-theme-polarbearsimple">Polar Bear Simple</a>
    </span>
</span>

      </footer>


    </div>
    


    




  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  

    <script type="text/javascript" src="/js/src/theme.js?v=1.1"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=1.1"></script>

  </body>
</html>
