<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>浩瀚宇宙·AaronChen</title>
  <icon>https://www.gravatar.com/avatar/b286774732a4159f228b7e9d1c21e536</icon>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://hhyz.me/"/>
  <updated>2019-07-13T08:50:01.565Z</updated>
  <id>https://hhyz.me/</id>
  
  <author>
    <name>haoyu</name>
    <email>aaronchen98@outlook.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>regression</title>
    <link href="https://hhyz.me/2019/07/13/regression/"/>
    <id>https://hhyz.me/2019/07/13/regression/</id>
    <published>2019-07-12T16:00:00.000Z</published>
    <updated>2019-07-13T08:50:01.565Z</updated>
    
    <content type="html"><![CDATA[<h1 id="linear-regression"><a href="#linear-regression" class="headerlink" title="linear regression"></a>linear regression</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;linear-regression&quot;&gt;&lt;a href=&quot;#linear-regression&quot; class=&quot;headerlink&quot; title=&quot;linear regression&quot;&gt;&lt;/a&gt;linear regression&lt;/h1&gt;
      
    
    </summary>
    
      <category term="机器学习" scheme="https://hhyz.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://hhyz.me/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记</title>
    <link href="https://hhyz.me/2019/07/12/papers/"/>
    <id>https://hhyz.me/2019/07/12/papers/</id>
    <published>2019-07-11T16:00:00.000Z</published>
    <updated>2019-07-12T12:44:57.980Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="深度学习" scheme="https://hhyz.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="论文" scheme="https://hhyz.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="论文" scheme="https://hhyz.me/tags/%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>论文写作参考汇总</title>
    <link href="https://hhyz.me/2019/07/06/img/"/>
    <id>https://hhyz.me/2019/07/06/img/</id>
    <published>2019-07-05T16:00:00.000Z</published>
    <updated>2019-07-06T08:49:08.751Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h2><ul><li><p><a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Wang_ESRGAN_Enhanced_Super-Resolution_Generative_Adversarial_Networks_ECCVW_2018_paper.pdf" target="_blank" rel="noopener">ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks</a></p></li><li><p><a href="https://arxiv.org/pdf/1506.01497.pdf" target="_blank" rel="noopener">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></p></li></ul><blockquote><p><strong>To investigate the behavior of RPNs as a proposal method, we conducted several ablation studies.</strong> <strong>First, we show the effect of</strong> sharing convolutional layers between the RPN and Fast R-CNN detection network. <strong>To do this,</strong> we stop<br>after the second step in the 4-step training process. Using separate networks reduces the result slightly to 58.7% (RPN+ZF, unshared, Table 2).** We observe that** this is because in the third step when the detectortuned features are used to fine-tune the RPN, the proposal quality is improved.<br><strong>Next, we</strong> disentangle the RPN’s influence on training the Fast R-CNN detection network. <strong>For this purpose,</strong> we train a Fast R-CNN model by using the 2000 SS proposals and ZF net. We fix this detector and evaluate the detection mAP by changing the proposal regions used at test-time. <strong>In these ablation experiments,</strong> the RPN does not share features with the detector<br>…</p></blockquote><h2 id="写作"><a href="#写作" class="headerlink" title="写作"></a>写作</h2><h3 id="单词"><a href="#单词" class="headerlink" title="单词"></a>单词</h3><ul><li>complicated 复杂的</li></ul><h3 id="句式"><a href="#句式" class="headerlink" title="句式"></a>句式</h3><ul><li>For some cases, a slight improvement can be observed from the 2nd and 3rd columns in Fig. 8       </li><li>We can further observe that …</li><li>We observe that this is because …</li></ul><h2 id="Figure-Table"><a href="#Figure-Table" class="headerlink" title="Figure/Table"></a>Figure/Table</h2><p><img src="/img/2019-07-06-img-1.png" alt><br><img src="/img/2019-07-06-img-2.png" alt><br><img src="/img/2019-07-06-img-3.png" alt><br><img src="/img/2019-07-06-img-4.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Ablation-Study&quot;&gt;&lt;a href=&quot;#Ablation-Study&quot; class=&quot;headerlink&quot; title=&quot;Ablation Study&quot;&gt;&lt;/a&gt;Ablation Study&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http
      
    
    </summary>
    
      <category term="论文" scheme="https://hhyz.me/categories/%E8%AE%BA%E6%96%87/"/>
    
    
      <category term="论文" scheme="https://hhyz.me/tags/%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>Normalization</title>
    <link href="https://hhyz.me/2019/06/24/norm/"/>
    <id>https://hhyz.me/2019/06/24/norm/</id>
    <published>2019-06-23T16:00:00.000Z</published>
    <updated>2019-07-05T06:29:04.787Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h2><h3 id="独立同分布与白化"><a href="#独立同分布与白化" class="headerlink" title="独立同分布与白化"></a>独立同分布与白化</h3><p>独立同分布，即 independent and identically distributed，简称为 i.i.d. 独立同分布并非所有机器学习模型的必然要求（比如 Naive Bayes 模型就建立在特征彼此独立的基础之上，而 Logistic Regression 和神经网络则在非独立的特征数据上依然可以训练出很好的模型），但独立同分布的数据可以简化常规机器学习模型的训练、提升机器学习模型的预测能力，已经是一个共识。</p><p>因此，在把数据喂给机器学习模型之前，“白化（whitening）”是一个重要的数据预处理步骤。白化一般包含两个目的：</p><ol><li>去除特征之间的相关性  \(\longrightarrow\)  <strong>独立</strong>；</li><li>使得所有特征具有相同的均值和方差 \(\longrightarrow\) <strong>同分布</strong>。</li></ol><p>白化最典型的方法就是PCA，可以参考阅读 <a href="https://link.zhihu.com/?target=http%3A//ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/" target="_blank" rel="noopener">PCAWhitening</a>。</p><h3 id="深度学习中的-Internal-Covariate-Shift"><a href="#深度学习中的-Internal-Covariate-Shift" class="headerlink" title="深度学习中的 Internal Covariate Shift"></a>深度学习中的 Internal Covariate Shift</h3><p>深度神经网络模型的训练为什么会很困难？其中一个重要的原因是，深度神经网络涉及到很多层的叠加，<strong>而每一层的参数更新会导致上层的输入数据分布发生变化</strong>，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。为了训好模型，我们需要非常谨慎地去设定学习率、初始化权重、以及尽可能细致的参数更新策略。</p><p>Google 将这一现象总结为 <strong>Internal Covariate Shift</strong>，简称 ICS.</p><blockquote><p>大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如 transfer learning / domain adaptation 等。而 covariate shift 就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同，即：对所有 \(x \in \mathcal{X}\) ,</p><p>$$P_{s}(Y | X=x)=P_{t}(Y | X=x)$$</p><p>但是</p><p>$$P_{s}(X) \neq P_{t}(X)$$</p></blockquote><p>大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。</p><h3 id="ICS-会导致什么问题？"><a href="#ICS-会导致什么问题？" class="headerlink" title="ICS 会导致什么问题？"></a>ICS 会导致什么问题？</h3><p>简而言之，每个神经元的输入数据不再是 “独立同分布”。</p><ol><li><p>上层参数需要不断适应新的输入数据分布，降低学习速度。</p></li><li><p>下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。    </p></li><li><p>每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。</p></li></ol><h2 id="Normalization-的通用框架与基本思想"><a href="#Normalization-的通用框架与基本思想" class="headerlink" title="Normalization 的通用框架与基本思想"></a>Normalization 的通用框架与基本思想</h2><blockquote><p><font color="red"> <strong>BN的基本思想其实相当直观：</strong>  </font></p><p>因为深层神经网络在做非线性变换前的激活输入值（就是那个x=WU+B，U是输入）随着网络深度加深或者在训练过程中，其分布逐渐<strong>发生偏移或者变动</strong>，之所以训练收敛慢，一般是<strong>整体分布逐渐往非线性函数的取值区间的上下限两端靠近</strong>（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这导致反向传播时<strong>低层神经网络的梯度消失</strong>，这是训练深层神经网络收敛越来越慢的<strong>本质原因</strong>。</p><p>而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，其实就是把越来越偏的分布强制拉回比较标准的分布，这样<strong>使得激活输入值落在非线性函数对输入比较敏感的区域</strong>，这样输入的小变化就会导致损失函数较大的变化，意思是这样<strong>让梯度变大，避免梯度消失问题产生</strong>，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。</p></blockquote><p>我们以神经网络中的一个普通神经元为例。神经元接收一组输入向量  \(\mathbf{x}=\left(x_{1}, x_{2}, \cdots, x_{d}\right)\)  通过某种运算后，输出一个标量值：  \(y = f(\mathbf{x})\)  </p><p>由于 ICS 问题的存在，  \(\mathbf{x}\)  的分布可能相差很大。要解决独立同分布的问题，“理论正确”的方法就是对每一层的数据都进行白化操作。然而标准的白化操作代价高昂，特别是我们还希望白化操作是可微的，保证白化操作可以通过反向传播来更新梯度。</p><p>因此，以 BN 为代表的 Normalization 方法退而求其次，进行了简化的白化操作。基本思想是：在将  \(\mathbf{x}\) 送给神经元之前，先对其做平移和伸缩变换， 将 \(\mathbf{x}\) 的分布规范化成在固定区间范围的标准分布。</p><p>通用变换框架就如下所示：</p><p>$$<br>h=f\left(\mathbf{g} \cdot \frac{\mathbf{x}-\mu}{\sigma}+\mathbf{b}\right)<br>$$</p><p>我们来看看这个公式中的各个参数。</p><ol><li><p>\(\mu\)  是<strong>平移参数</strong>（shift parameter），  \(\sigma\)  是<strong>缩放参数</strong>（scale parameter）。通过这两个参数进行 shift 和 scale 变换：  \(\hat{\mathbf{x}}=\frac{\mathbf{x}-\mu}{\sigma}\)  得到的数据符合均值为 0、方差为 1 的标准分布。</p></li><li><p>\(\mathbf{b}\)  是<strong>再平移参数</strong>（re-shift parameter），\(\mathbf{g}\)  是<strong>再缩放参数</strong>（re-scale parameter）。将 上一步得到的  \(\hat{\mathbf{x}}\)  进一步变换为： </p></li></ol><p>$$<br>\mathbf{y}=\mathbf{g} \cdot \hat{\mathbf{x}}+\mathbf{b}<br>$$</p><p>最终得到的数据符合均值为 \(\mathbf{g}\) 、方差为 \(\mathbf{g}^{2}\) 的分布。</p><p>奇不奇怪？奇不奇怪？<br>说好的处理 ICS，第一步都已经得到了标准分布，第二步怎么又给变走了？</p><p>答案是 —— <strong>为了保证模型的表达能力不因为规范化而下降</strong>。</p><p>我们可以看到，第一步的变换将输入数据限制到了一个全局统一的确定范围（均值为 0、方差为 1）。下层神经元可能很努力地在学习，但不论其如何变化，其输出的结果在交给上层神经元进行处理之前，将被粗暴地重新调整到这一固定范围。</p><p>难道我们底层神经元人民就在做无用功吗？</p><p>所以，为了尊重底层神经网络的学习结果，我们将规范化后的数据进行再平移和再缩放，使得每个神经元对应的输入范围是针对该神经元量身定制的一个确定范围（均值为   \(\mathbf{b}\)  、方差为 \(\mathbf{g}^{2}\)  ）。rescale 和 reshift 的参数都是可学习的，这就使得 Normalization 层可以学习如何去尊重底层的学习结果。</p><p><strong>除了充分利用底层学习的能力，另一方面的重要意义在于保证获得非线性的表达能力。</strong> Sigmoid 等激活函数在神经网络中有着重要作用，通过区分饱和区和非饱和区，使得神经网络的数据变换具有了非线性计算能力。<em>而第一步的规范化会将几乎所有数据映射到激活函数的非饱和区（线性区），仅利用到了线性变化能力，从而降低了神经网络的表达能力。<em>而进行再变换，</em>则可以将数据从线性区变换到非线性区，恢复模型的表达能力。</em></p><blockquote><p><strong>经过这么的变回来再变过去，会不会跟没变一样？</strong></p><p>不会。因为，再变换引入的两个新参数 g 和 b，可以表示旧参数作为输入的同一族函数，但是新参数有不同的学习动态。在旧参数中， \(\mathbf{X}\)  的均值取决于下层神经网络的复杂关联；但在新参数中，  \(\mathbf{y}=\mathbf{g} \cdot \hat{\mathbf{x}}+\mathbf{b}\) 仅由 \(\mathbf{b}\) 来确定，去除了与下层计算的密切耦合。新参数很容易通过梯度下降来学习，简化了神经网络的训练。</p><p><strong>这样的 Normalization 离标准的白化还有多远？</strong></p><p>标准白化操作的目的是 “独立同分布”。独立就不说了，暂不考虑。变换为均值为\(\mathbf{b}\) 、方差为 \(\mathbf{g}^{2}\)的分布，也并不是严格的同分布，只是映射到了一个确定的区间范围而已。（所以，这个坑还有得研究呢！）</p></blockquote><h2 id="主流-Normalization-方法梳理"><a href="#主流-Normalization-方法梳理" class="headerlink" title="主流 Normalization 方法梳理"></a>主流 Normalization 方法梳理</h2><h3 id="Batch-Normalization-纵向规范化"><a href="#Batch-Normalization-纵向规范化" class="headerlink" title="Batch Normalization 纵向规范化"></a>Batch Normalization 纵向规范化</h3><p><img src="/img/2019-06-24-norm-1.jpg" alt></p><p>$$<br>\mu_{i}=\frac{1}{M} \sum x_{i}, \quad \sigma_{i}=\sqrt{\frac{1}{M} \sum\left(x_{i}-\mu_{i}\right)^{2}+\epsilon}<br>$$</p><p>其中  \(\mathbf{M}\) 是 mini-batch 的大小。</p><p><img src="/img/2019-06-24-norm-2.png" alt></p><p><img src="/img/2019-06-24-norm-3.png" alt></p><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Normalization&quot;&gt;&lt;a href=&quot;#Normalization&quot; class=&quot;headerlink&quot; title=&quot;Normalization&quot;&gt;&lt;/a&gt;Normalization&lt;/h2&gt;&lt;h3 id=&quot;独立同分布与白化&quot;&gt;&lt;a href=&quot;#独
      
    
    </summary>
    
      <category term="深度学习" scheme="https://hhyz.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://hhyz.me/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>R 语言入门</title>
    <link href="https://hhyz.me/2019/06/21/R/"/>
    <id>https://hhyz.me/2019/06/21/R/</id>
    <published>2019-06-20T16:00:00.000Z</published>
    <updated>2019-07-09T17:10:34.077Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基础操作"><a href="#基础操作" class="headerlink" title="基础操作"></a>基础操作</h1><p>查看数据类型：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt; class(spam)[<span class="number">1</span>] <span class="string">"data.frame"</span></span><br></pre></td></tr></table></figure><p>安装包：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">install.packages(<span class="string">"ElemStatLearn"</span>)</span><br></pre></td></tr></table></figure><p>使用包：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(ElemStatLearn)</span><br></pre></td></tr></table></figure><h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><h2 id="1-Vectors"><a href="#1-Vectors" class="headerlink" title="1. Vectors"></a>1. Vectors</h2><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建向量</span></span><br><span class="line">a &lt;-c(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line">b &lt;-c(<span class="string">"one"</span>, <span class="string">"two"</span>, <span class="string">"three"</span>)</span><br><span class="line">c &lt;-c(<span class="literal">TRUE</span>, <span class="literal">FALSE</span>, <span class="literal">TRUE</span>, <span class="literal">TRUE</span>, <span class="literal">FALSE</span>)</span><br><span class="line"></span><br><span class="line">&gt; <span class="number">5</span>:<span class="number">1</span></span><br><span class="line">[<span class="number">1</span>] <span class="number">5</span> <span class="number">4</span> <span class="number">3</span> <span class="number">2</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">&gt; <span class="number">2</span>*(<span class="number">1</span>:<span class="number">5</span>)</span><br><span class="line">[<span class="number">1</span>]  <span class="number">2</span>  <span class="number">4</span>  <span class="number">6</span>  <span class="number">8</span> <span class="number">10</span></span><br><span class="line"></span><br><span class="line">&gt; rep(<span class="number">1</span>,<span class="number">9</span>)</span><br><span class="line">[<span class="number">1</span>] <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">&gt; rep(c(<span class="number">1</span>,<span class="number">0</span>,<span class="number">4</span>), each=<span class="number">3</span>)</span><br><span class="line">[<span class="number">1</span>] <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">4</span> <span class="number">4</span> <span class="number">4</span></span><br><span class="line"></span><br><span class="line">&gt; seq(<span class="number">1</span>,<span class="number">3</span>, by=<span class="number">0.2</span>)</span><br><span class="line">&gt; seq(from=<span class="number">1</span>, to=<span class="number">3</span>, by=<span class="number">0.2</span>)</span><br><span class="line"> [<span class="number">1</span>] <span class="number">1.0</span> <span class="number">1.2</span> <span class="number">1.4</span> <span class="number">1.6</span> <span class="number">1.8</span> <span class="number">2.0</span> <span class="number">2.2</span> <span class="number">2.4</span> <span class="number">2.6</span> <span class="number">2.8</span> <span class="number">3.0</span></span><br><span class="line"> </span><br><span class="line">&gt; seq(<span class="number">3</span>,<span class="number">1</span>, by=-<span class="number">0.2</span>)</span><br><span class="line"> [<span class="number">1</span>] <span class="number">3.0</span> <span class="number">2.8</span> <span class="number">2.6</span> <span class="number">2.4</span> <span class="number">2.2</span> <span class="number">2.0</span> <span class="number">1.8</span> <span class="number">1.6</span> <span class="number">1.4</span> <span class="number">1.2</span> <span class="number">1.0</span></span><br><span class="line">&gt; seq(<span class="number">3</span>,<span class="number">1</span>, by=<span class="number">0.2</span>)</span><br><span class="line">Error <span class="keyword">in</span> seq.default(<span class="number">3</span>, <span class="number">1</span>, by = <span class="number">0.2</span>) : wrong sign <span class="keyword">in</span> <span class="string">'by'</span> argument</span><br><span class="line"></span><br><span class="line">&gt; seq(<span class="number">3</span>,<span class="number">1.1</span>)</span><br><span class="line">[<span class="number">1</span>] <span class="number">3</span> <span class="number">2</span></span><br></pre></td></tr></table></figure><figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 向量索引</span></span><br><span class="line"><span class="keyword">a</span>[<span class="number">2</span>]  <span class="comment"># 第二个元素    </span></span><br><span class="line"><span class="comment"># [1] 2</span></span><br><span class="line"><span class="keyword">a</span>[<span class="number">-2</span>] <span class="comment"># 删除第二个元素，a 还是原来的 a</span></span><br><span class="line"><span class="comment"># [1] 1 3 4 5 6</span></span><br><span class="line"><span class="keyword">a</span>[c(<span class="number">2</span>:<span class="number">4</span>)] <span class="comment"># 取出第二到第四个元素</span></span><br><span class="line"><span class="comment"># [1] 2 3 4</span></span><br></pre></td></tr></table></figure><h2 id="2-矩阵"><a href="#2-矩阵" class="headerlink" title="2. 矩阵"></a>2. 矩阵</h2><p><strong>二维数组</strong></p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建矩阵</span></span><br><span class="line">mymat &lt;- matrix(c(<span class="number">1</span>:<span class="number">10</span>), nrow=<span class="number">2</span>, ncol=<span class="number">5</span>, byrow=<span class="literal">TRUE</span>)</span><br></pre></td></tr></table></figure><p>得到：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">     [,<span class="number">1</span>] [,<span class="number">2</span>] [,<span class="number">3</span>] [,<span class="number">4</span>] [,<span class="number">5</span>]</span><br><span class="line">[<span class="number">1</span>,]    <span class="number">1</span>    <span class="number">2</span>    <span class="number">3</span>    <span class="number">4</span>    <span class="number">5</span></span><br><span class="line">[<span class="number">2</span>,]    <span class="number">6</span>    <span class="number">7</span>    <span class="number">8</span>    <span class="number">9</span>   <span class="number">10</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#矩阵索引</span></span><br><span class="line">mymat[<span class="number">2</span>,]   <span class="comment"># 取第二行</span></span><br><span class="line">mymat[,<span class="number">2</span>]   <span class="comment"># 取第二列</span></span><br><span class="line">mymat[<span class="number">1</span>,<span class="number">5</span>]  <span class="comment"># 第一行第五列的元素</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nrow(mymat)  <span class="comment"># 获取行数</span></span><br><span class="line">ncol(mymat)  <span class="comment"># 获取列数</span></span><br></pre></td></tr></table></figure><h2 id="3-数组"><a href="#3-数组" class="headerlink" title="3. 数组"></a>3. 数组</h2><p>维度可以大于 2</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建数组</span></span><br><span class="line">myarr &lt;- array(c(<span class="number">1</span>:<span class="number">12</span>),dim=c(<span class="number">2</span>,<span class="number">3</span>,<span class="number">2</span>))</span><br></pre></td></tr></table></figure><p>得到：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">, , <span class="number">1</span></span><br><span class="line"></span><br><span class="line">     [,<span class="number">1</span>] [,<span class="number">2</span>] [,<span class="number">3</span>]</span><br><span class="line">[<span class="number">1</span>,]    <span class="number">1</span>    <span class="number">3</span>    <span class="number">5</span></span><br><span class="line">[<span class="number">2</span>,]    <span class="number">2</span>    <span class="number">4</span>    <span class="number">6</span></span><br><span class="line"></span><br><span class="line">, , <span class="number">2</span></span><br><span class="line"></span><br><span class="line">     [,<span class="number">1</span>] [,<span class="number">2</span>] [,<span class="number">3</span>]</span><br><span class="line">[<span class="number">1</span>,]    <span class="number">7</span>    <span class="number">9</span>   <span class="number">11</span></span><br><span class="line">[<span class="number">2</span>,]    <span class="number">8</span>   <span class="number">10</span>   <span class="number">12</span></span><br></pre></td></tr></table></figure><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dim(myarr) <span class="comment"># 取矩阵或数组的维度</span></span><br><span class="line"><span class="comment"># [1] 2 3 2</span></span><br><span class="line">myarr[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>] <span class="comment"># 取第一个矩阵的第一行第二列</span></span><br><span class="line"><span class="comment"># [1] 3</span></span><br></pre></td></tr></table></figure><h2 id="4-Data-Frame"><a href="#4-Data-Frame" class="headerlink" title="4. Data Frame"></a>4. Data Frame</h2><p>类似矩阵，每一列可以有不同的模式</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建数据框</span></span><br><span class="line">kids &lt;- c(<span class="string">"Wang"</span>, <span class="string">"Li"</span>)</span><br><span class="line">age &lt;- c(<span class="string">"18"</span>, <span class="string">"16"</span>)</span><br><span class="line">df &lt;- data.frame(kids, age)</span><br></pre></td></tr></table></figure><p>得到：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">  kids age</span><br><span class="line"><span class="number">1</span> Wang  <span class="number">18</span></span><br><span class="line"><span class="number">2</span>   Li  <span class="number">16</span></span><br></pre></td></tr></table></figure><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#数据框索引</span></span><br><span class="line">df[<span class="number">1</span>,]   <span class="comment"># 第一行</span></span><br><span class="line">df[,<span class="number">2</span>]   <span class="comment"># 第二列</span></span><br><span class="line">df[<span class="number">1</span>:<span class="number">2</span>,<span class="number">1</span>:<span class="number">2</span>]   <span class="comment"># 前两行，前两列</span></span><br><span class="line">df$age   <span class="comment"># 根据列名称</span></span><br><span class="line"></span><br><span class="line">&gt; df$age</span><br><span class="line">[<span class="number">1</span>] <span class="number">18</span> <span class="number">16</span></span><br><span class="line">Levels: <span class="number">16</span> <span class="number">18</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#数据框常用函数</span></span><br><span class="line">str(df)   <span class="comment"># 数据框的结构</span></span><br><span class="line"></span><br><span class="line">&gt; str(df)</span><br><span class="line"><span class="string">'data.frame'</span>:<span class="number">2</span> obs. of  <span class="number">2</span> variables:</span><br><span class="line"> $ kids: Factor w/ <span class="number">2</span> levels <span class="string">"Li"</span>,<span class="string">"Wang"</span>: <span class="number">2</span> <span class="number">1</span></span><br><span class="line"> $ age : Factor w/ <span class="number">2</span> levels <span class="string">"16"</span>,<span class="string">"18"</span>: <span class="number">2</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">rownames(df)   <span class="comment"># 行名称</span></span><br><span class="line">colnames(df)   <span class="comment"># 列名称</span></span><br><span class="line"></span><br><span class="line">&gt; rownames(df)</span><br><span class="line">[<span class="number">1</span>] <span class="string">"1"</span> <span class="string">"2"</span></span><br><span class="line">&gt; colnames(df)</span><br><span class="line">[<span class="number">1</span>] <span class="string">"kids"</span> <span class="string">"age"</span></span><br></pre></td></tr></table></figure><p>更换列名：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">colnames(m) &lt;- c(<span class="string">"Date"</span>, <span class="string">"Bill"</span>, <span class="string">"Dollar"</span>, <span class="string">"Gold"</span>)</span><br></pre></td></tr></table></figure><p>最小值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">min(m$Gold, na.rm=T)</span><br></pre></td></tr></table></figure><p>其中 <code>na.rm=T</code> 是为了防止 na 值影响结果，会移除 na 数据</p><h3 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h3><p>根据某一共同列合并：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># student</span></span><br><span class="line"></span><br><span class="line">  SID  Course Score</span><br><span class="line"><span class="number">1</span>  <span class="number">11</span>    Math    <span class="number">90</span></span><br><span class="line"><span class="number">2</span>  <span class="number">11</span> English    <span class="number">80</span></span><br><span class="line"><span class="number">3</span>  <span class="number">12</span>    Math    <span class="number">80</span></span><br><span class="line"><span class="number">4</span>  <span class="number">12</span> Chinese    <span class="number">95</span></span><br><span class="line"><span class="number">5</span>  <span class="number">13</span>    Math    <span class="number">96</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">result &lt;- merge(student,score,by.x=<span class="string">"ID"</span>,by.y=<span class="string">"SID"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># result</span></span><br><span class="line"></span><br><span class="line"> ID   Name Gender  Birthdate Age  Course Score</span><br><span class="line"><span class="number">1</span> <span class="number">11</span>  Devin      M <span class="number">1984</span>-<span class="number">12</span>-<span class="number">29</span>  <span class="number">31</span>    Math    <span class="number">90</span></span><br><span class="line"><span class="number">2</span> <span class="number">11</span>  Devin      M <span class="number">1984</span>-<span class="number">12</span>-<span class="number">29</span>  <span class="number">31</span> English    <span class="number">80</span></span><br><span class="line"><span class="number">3</span> <span class="number">12</span> Edward      M <span class="number">1983</span>-<span class="number">05</span>-<span class="number">06</span>  <span class="number">32</span>    Math    <span class="number">80</span></span><br><span class="line"><span class="number">4</span> <span class="number">12</span> Edward      M <span class="number">1983</span>-<span class="number">05</span>-<span class="number">06</span>  <span class="number">32</span> Chinese    <span class="number">95</span></span><br><span class="line"><span class="number">5</span> <span class="number">13</span>  Wenli      <span class="literal">F</span> <span class="number">1986</span>-<span class="number">08</span>-<span class="number">08</span>  <span class="number">29</span>    Math    <span class="number">96</span></span><br></pre></td></tr></table></figure><p>合并多个：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">m &lt;- Reduce(<span class="keyword">function</span>(x,y)merge(x, y, by=<span class="string">"DATE"</span>, all=<span class="literal">TRUE</span>), list(data1, data2, data3))</span><br></pre></td></tr></table></figure><h3 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:ncol(df)) &#123;</span><br><span class="line">   df[,i] &lt;- as.integer(df[,i])  <span class="comment">#将每列类型变为integer型</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h1><h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><p>单变量序列的排序常用到 <code>rank</code>、<code>sort</code> 和 <code>order</code> 函数。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt; a &lt;- c(<span class="number">3</span>, <span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">&gt; rank(a)</span><br><span class="line">[<span class="number">1</span>] <span class="number">2</span> <span class="number">1</span> <span class="number">3</span></span><br><span class="line">&gt; sort(a)</span><br><span class="line">[<span class="number">1</span>] <span class="number">1</span> <span class="number">3</span> <span class="number">5</span></span><br><span class="line">&gt; order(a)</span><br><span class="line">[<span class="number">1</span>] <span class="number">2</span> <span class="number">1</span> <span class="number">3</span></span><br></pre></td></tr></table></figure><ul><li><code>rank</code> 用来计算序列中每个元素的秩，这里的“秩”可以理解为该元素在序列中由小到大排列的次序</li><li><code>sort</code> 函数给出的是排序后的结果</li><li><code>order</code> 函数给出的是排序后的序列中各元素在原始序列中的位置，序列 [3, 1, 5] 按升序规则排序后的结果是 [1, 3, 5] ，其中 [1, 3, 5] 在原始序列中的位置是 [2, 1, 3]</li></ul><h2 id="筛选"><a href="#筛选" class="headerlink" title="筛选"></a>筛选</h2><h2 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h2><p>中心化：<code>数据 - 均值</code><br>标准化：<code>(数据 - 均值）/ 标准差</code></p><p>数据中心化：  <code>scale(data,center=T,scale=F)</code><br>数据标准化：  <code>scale(data,center=T,scale=T)</code> 或默认参数 <code>scale(data)</code></p><p>scale 方法中的两个参数 center 和 scale 的解释：</p><ol><li>center 和 scale 默认为真,即 T 或者 TRUE</li><li>center 为真表示数据中心化</li><li>scale 为真表示数据标准化</li></ol><h1 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h1><h2 id="读取-CSV"><a href="#读取-CSV" class="headerlink" title="读取 CSV"></a>读取 CSV</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">read.table(file, header = FALSE, sep = <span class="string">""</span>, quote = <span class="string">"\"'"</span>,</span><br><span class="line">           dec = <span class="string">"."</span>, numerals = c(<span class="string">"allow.loss"</span>, <span class="string">"warn.loss"</span>, <span class="string">"no.loss"</span>),</span><br><span class="line">           row.names, col.names, <span class="keyword">as</span>.<span class="keyword">is</span> = !stringsAsFactors,</span><br><span class="line">           na.strings = <span class="string">"NA"</span>, colClasses = NA, nrows = <span class="number">-1</span>,</span><br><span class="line">           skip = <span class="number">0</span>, check.names = TRUE, fill = !blank.lines.skip,</span><br><span class="line">           strip.white = FALSE, blank.lines.skip = TRUE,</span><br><span class="line">           comment.char = <span class="string">"#"</span>,</span><br><span class="line">           allowEscapes = FALSE, flush = FALSE,</span><br><span class="line">           stringsAsFactors = default.stringsAsFactors(),</span><br><span class="line">           fileEncoding = <span class="string">""</span>, encoding = <span class="string">"unknown"</span>, text, skipNul = FALSE)</span><br></pre></td></tr></table></figure><p><img src="/img/2019-06-21-R-1.jpg" alt></p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset &lt;- read.table(<span class="string">"C:\\Datasets\\haberman.csv"</span>, header=<span class="literal">FALSE</span>, sep=<span class="string">","</span>)</span><br></pre></td></tr></table></figure><ul><li>如果第一行数据包含列的名字，则第二个参数应该为：<code>header = TRUE</code></li><li>分割符：<ul><li>空格： <code>sep = &quot; &quot;</code></li><li>tabs： <code>sep = &quot;\t&quot;</code></li><li>默认是 “white space” (one or more spaces, tabs, etc.)</li></ul></li></ul><p>如果分割符号为逗号，那么也可以用 <code>read.csv</code>，并且不用写 <code>seq</code> 参数：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset &lt;- read.csv(<span class="string">"C:\\Datasets\\haberman.csv"</span>, header=<span class="literal">FALSE</span>)</span><br></pre></td></tr></table></figure><h1 id="机器学习算法"><a href="#机器学习算法" class="headerlink" title="机器学习算法"></a>机器学习算法</h1><h2 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h2><p>引入 <code>class</code> 包：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(class)</span><br></pre></td></tr></table></figure><p><code>knn()</code> 函数的语法和参数如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knn(train, test, cl, k = <span class="number">1</span>, l = <span class="number">0</span>, prob = FALSE, use.all = TRUE)</span><br></pre></td></tr></table></figure><ul><li>train：指定训练样本集</li><li>test ：指定测试样本集</li><li>cl ：指定训练样本集中的分类变量</li><li>k ：指定最邻近的k个已知分类样本点，默认为1</li><li>l ：指定待判样本点属于某类的最少已知分类样本数，默认为0</li><li>prob：设为TRUE时，可以得到待判样本点属于某类的概率，默认为FALSE</li><li>use.all：控制节点的处理办法，即如果有多个第K近的点与待判样本点的距离相等，默认情况下将这些点都纳入判别样本点，当该参数设为FALSE时，则随机挑选一个样本点作为第K近的判别点</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="comment"># z-score数据标准化</span></span><br><span class="line">&gt; iris_scale &lt;- scale(iris[-<span class="number">5</span>]) </span><br><span class="line">&gt; train &lt;- iris_scale[c(<span class="number">1</span>:<span class="number">25</span>,<span class="number">50</span>:<span class="number">75</span>,<span class="number">100</span>:<span class="number">125</span>),] <span class="comment">#训练集</span></span><br><span class="line">&gt; test &lt;- iris_scale[c(<span class="number">26</span>:<span class="number">49</span>,<span class="number">76</span>:<span class="number">99</span>,<span class="number">126</span>:<span class="number">150</span>),] <span class="comment">#测试集</span></span><br><span class="line">&gt; train_lab &lt;- iris[c(<span class="number">1</span>:<span class="number">25</span>,<span class="number">50</span>:<span class="number">75</span>,<span class="number">100</span>:<span class="number">125</span>),<span class="number">5</span>]</span><br><span class="line">&gt; test_lab &lt;- iris[c(<span class="number">26</span>:<span class="number">49</span>,<span class="number">76</span>:<span class="number">99</span>,<span class="number">126</span>:<span class="number">150</span>),<span class="number">5</span>]</span><br><span class="line">&gt; pre &lt;- knn(train=train,test=test,cl=train_lab,k=round(sqrt(dim(train)[<span class="number">1</span>])),prob = <span class="literal">F</span>)  </span><br><span class="line">&gt; table(pre,test_lab)</span><br><span class="line">            test_lab</span><br><span class="line">pre          setosa versicolor virginica</span><br><span class="line">  setosa         <span class="number">24</span>          <span class="number">0</span>         <span class="number">0</span></span><br><span class="line">  versicolor      <span class="number">0</span>         <span class="number">24</span>         <span class="number">3</span></span><br><span class="line">  virginica       <span class="number">0</span>          <span class="number">0</span>        <span class="number">22</span></span><br></pre></td></tr></table></figure><p>实例：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(ElemStatLearn)</span><br><span class="line"></span><br><span class="line"><span class="comment">####################################</span></span><br><span class="line"><span class="comment">## Part 1. Load and Explore Data   #</span></span><br><span class="line"><span class="comment">####################################</span></span><br><span class="line"></span><br><span class="line">data(spam)</span><br><span class="line">colnames(spam) &lt;- c(paste(<span class="string">"X"</span>, <span class="number">1</span>:<span class="number">57</span>, sep = <span class="string">""</span>), <span class="string">"spam"</span>)</span><br><span class="line">spam$spam &lt;- factor(spam$spam, levels = c(<span class="string">"spam"</span>, <span class="string">"email"</span>), labels = c(<span class="string">"spam"</span>, <span class="string">"email"</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#######################</span></span><br><span class="line"><span class="comment"># Normalize Variables #</span></span><br><span class="line"><span class="comment">#######################</span></span><br><span class="line"></span><br><span class="line">spam_n &lt;- as.data.frame(scale(spam[<span class="number">1</span>:<span class="number">57</span>]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">################################################</span></span><br><span class="line"><span class="comment">## Part 2. Creating training and test datasets #</span></span><br><span class="line"><span class="comment">################################################</span></span><br><span class="line"></span><br><span class="line">index &lt;- sample(<span class="number">1</span>:<span class="number">4601</span>, <span class="number">3000</span>)</span><br><span class="line"></span><br><span class="line">spam_train &lt;- spam_n[index, ]</span><br><span class="line">spam_test &lt;- spam_n[-index, ]</span><br><span class="line">spam_train_label &lt;- spam[index, <span class="number">58</span>]</span><br><span class="line">spam_tset_label &lt;- spam[-index, <span class="number">58</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">########################################</span></span><br><span class="line"><span class="comment">## Part 3. Train the model on the data #</span></span><br><span class="line"><span class="comment">########################################</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">library</span>(class)</span><br><span class="line"></span><br><span class="line">spam_test_pred &lt;- knn(train = spam_train, test = spam_test, cl = spam_train_label, k = <span class="number">3</span>)</span><br><span class="line">result &lt;- data.frame(spam_tset_label, spam_test_pred)</span><br><span class="line"></span><br><span class="line">mean(spam_test_pred == spam_tset_label)</span><br></pre></td></tr></table></figure><h2 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h2><p><img src="/img/2019-06-21-R-2.png" alt></p><p>实例：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">###########################################</span></span><br><span class="line"><span class="comment">#    Classification with Naive Bayes      #  </span></span><br><span class="line"><span class="comment">###########################################</span></span><br><span class="line"></span><br><span class="line">setwd(<span class="string">"D:/data/ML2019/"</span>)</span><br><span class="line"><span class="keyword">library</span>(data.table)     <span class="comment"># read text data</span></span><br><span class="line"><span class="keyword">library</span>(wordcloud)      <span class="comment"># make word cloud</span></span><br><span class="line"><span class="keyword">library</span>(RColorBrewer)   <span class="comment"># make word cloud</span></span><br><span class="line"><span class="keyword">library</span>(tm)             <span class="comment"># construct text vector</span></span><br><span class="line"><span class="keyword">library</span>(magrittr)       <span class="comment"># enable pipeline operator        </span></span><br><span class="line"><span class="keyword">library</span>(e1071)          <span class="comment"># naive bayes </span></span><br><span class="line"><span class="keyword">library</span>(caret)          <span class="comment"># split data into training and test sets</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">####################################</span></span><br><span class="line"><span class="comment">## Part 1. Load and Explore Data   #</span></span><br><span class="line"><span class="comment">####################################</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sms_raw &lt;- fread(<span class="string">"SMSSpamCollection"</span>, header = <span class="literal">FALSE</span>, encoding = <span class="string">"Latin-1"</span>,sep = <span class="string">"\t"</span>, quote=<span class="string">""</span>)</span><br><span class="line"></span><br><span class="line">colnames(sms_raw)&lt;-c(<span class="string">"type"</span>, <span class="string">"text"</span>)</span><br><span class="line"></span><br><span class="line">sms_raw$type &lt;- factor(sms_raw$type)</span><br><span class="line">table(sms_raw[, type])</span><br><span class="line"></span><br><span class="line"><span class="comment">#  ham spam </span><span class="comment"># 4827  747</span></span><br><span class="line"></span><br><span class="line">table(sms_raw[, type]) %&gt;% prop.table()</span><br><span class="line"></span><br><span class="line"><span class="comment">#       ham      spam </span><span class="comment"># 0.8659849 0.1340151</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##########################</span></span><br><span class="line"><span class="comment">#     Word Cloud 词云     #</span></span><br><span class="line"><span class="comment">##########################</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pal &lt;- brewer.pal(<span class="number">7</span>, <span class="string">"Dark2"</span>)</span><br><span class="line">sms_raw[type == <span class="string">"spam"</span>, text] %&gt;%</span><br><span class="line">    wordcloud(min.freq = <span class="number">20</span>,</span><br><span class="line">              random.order = <span class="literal">FALSE</span>, colors = pal</span><br><span class="line">    )</span><br><span class="line">sms_raw[type == <span class="string">"ham"</span>, text] %&gt;%</span><br><span class="line">    wordcloud(min.freq = <span class="number">70</span>,</span><br><span class="line">              random.order = <span class="literal">FALSE</span>, colors = pal</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment">################################################</span></span><br><span class="line"><span class="comment">## Part 2. Creating training and test datasets #</span></span><br><span class="line"><span class="comment">################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## p=75% of data are allocated to training </span></span><br><span class="line">train_index &lt;- createDataPartition(sms_raw$type, p = <span class="number">0.75</span>, list = <span class="literal">FALSE</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># createDataPartition 来自 caret 包</span></span><br><span class="line"><span class="comment"># 数据划分函数，对象是 spam$typ，</span></span><br><span class="line"><span class="comment"># p=0.75表示训练数据所占的比例为75%</span></span><br><span class="line"><span class="comment"># list是输出结果的格式，默认list=FALSE</span></span><br><span class="line"></span><br><span class="line">sms_raw_train &lt;- sms_raw[train_index, ]</span><br><span class="line">sms_raw_test &lt;- sms_raw[-train_index, ]</span><br><span class="line"></span><br><span class="line"><span class="comment">#################</span></span><br><span class="line"><span class="comment"># check results #</span></span><br><span class="line"><span class="comment">#################</span></span><br><span class="line">dim(sms_raw_train)</span><br><span class="line"><span class="comment"># [1] 4182    2</span></span><br><span class="line"></span><br><span class="line">dim(sms_raw_test)</span><br><span class="line"><span class="comment"># [1] 1392    2</span></span><br><span class="line"></span><br><span class="line">dim(sms_raw_train)[<span class="number">1</span>]/dim(sms_raw_test)[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># [1] 3.00431</span></span><br><span class="line"></span><br><span class="line">table(sms_raw_train[, type]) %&gt;% prop.table()</span><br><span class="line"><span class="comment">#       ham      spam </span><span class="comment"># 0.8658537 0.1341463 </span></span><br><span class="line"></span><br><span class="line">table(sms_raw_test[, type]) %&gt;% prop.table()</span><br><span class="line"><span class="comment">#       ham      spam </span><span class="comment"># 0.8663793 0.1336207</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以看到训练集和测试集两个分类的比例几乎相同</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################</span></span><br><span class="line"><span class="comment">## Part 3. Text processing    ##</span></span><br><span class="line"><span class="comment">################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##############################</span></span><br><span class="line"><span class="comment"># Text processing functions  #</span></span><br><span class="line"><span class="comment">##############################</span></span><br><span class="line">corpus &lt;- <span class="keyword">function</span>(x) VectorSource(x) %&gt;% VCorpus(readerControl = list(reader = readPlain))</span><br><span class="line">clean &lt;- <span class="keyword">function</span>(x) &#123;</span><br><span class="line">    x %&gt;%</span><br><span class="line">        tm_map(content_transformer(tolower)) %&gt;%</span><br><span class="line">        tm_map(content_transformer(removeNumbers)) %&gt;%</span><br><span class="line">        tm_map(content_transformer(removeWords), stopwords()) %&gt;%</span><br><span class="line">        tm_map(content_transformer(removePunctuation)) %&gt;%</span><br><span class="line">        tm_map(content_transformer(stripWhitespace))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sms_corpus_train &lt;- corpus(sms_raw_train[, text]) %&gt;% clean</span><br><span class="line">sms_corpus_test &lt;- corpus(sms_raw_test[, text]) %&gt;% clean</span><br><span class="line"></span><br><span class="line"><span class="comment">##################################</span></span><br><span class="line"><span class="comment"># check text processing results  #</span></span><br><span class="line"><span class="comment">##################################</span></span><br><span class="line"></span><br><span class="line">sms_raw_train[<span class="number">1</span>, text]</span><br><span class="line">inspect(sms_corpus_train[[<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">sms_raw_test[<span class="number">1</span>, text]</span><br><span class="line">inspect(sms_corpus_test[[<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">##################################</span></span><br><span class="line"><span class="comment">#   construct document matrix    #</span></span><br><span class="line"><span class="comment">##################################</span></span><br><span class="line"></span><br><span class="line">sms_dtm_train_all &lt;- DocumentTermMatrix(sms_corpus_train)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Finding frequent words</span></span><br><span class="line">sms_dict &lt;- findFreqTerms(sms_dtm_train_all, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">sms_dtm_train &lt;- DocumentTermMatrix(</span><br><span class="line">    sms_corpus_train, control = list(dictionary = sms_dict)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">sms_dtm_test &lt;- DocumentTermMatrix(</span><br><span class="line">    sms_corpus_test, control = list(dictionary = sms_dict)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## function to convert counts to Yes/No strings</span></span><br><span class="line">convert_counts &lt;- <span class="keyword">function</span>(x) &#123;</span><br><span class="line">              x &lt;- ifelse(x &gt; <span class="number">0</span>, <span class="string">"Yes"</span>, <span class="string">"No"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">sms_train &lt;- sms_dtm_train %&gt;% </span><br><span class="line">    apply(MARGIN = <span class="number">2</span>, convert_counts)</span><br><span class="line"></span><br><span class="line">sms_test &lt;- sms_dtm_test %&gt;% </span><br><span class="line">    apply(MARGIN = <span class="number">2</span>, convert_counts)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">########################################</span></span><br><span class="line"><span class="comment">## Part 3. Train the model on the data #</span></span><br><span class="line"><span class="comment">########################################</span></span><br><span class="line"></span><br><span class="line">sms_classifier_1 &lt;- naiveBayes(sms_train, sms_raw_train$type)</span><br><span class="line">sms_test_pred_1 &lt;- predict(sms_classifier_1, sms_test)</span><br><span class="line"></span><br><span class="line"><span class="comment">#########################################</span></span><br><span class="line"><span class="comment">## Part 4. Evaluating model performance #</span></span><br><span class="line"><span class="comment">#########################################</span></span><br><span class="line"></span><br><span class="line">install.packages(<span class="string">"gmodels"</span>)</span><br><span class="line"><span class="keyword">library</span>(gmodels)</span><br><span class="line"></span><br><span class="line">CrossTable(x = sms_raw_test$type, y = sms_test_pred_1, prop.chisq=<span class="literal">FALSE</span>)</span><br><span class="line">mean(sms_test_pred_1==sms_raw_test$type)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#########################################</span></span><br><span class="line"><span class="comment">## Part 5. Improving model performance  #</span></span><br><span class="line"><span class="comment">#########################################</span></span><br><span class="line"></span><br><span class="line">sms_classifier_2 &lt;- naiveBayes(sms_train, sms_raw_train$type, laplace = <span class="number">1</span>)</span><br><span class="line">sms_test_pred_2 &lt;- predict(sms_classifier_2, sms_test)</span><br><span class="line">CrossTable(x = sms_raw_test$type, y = sms_test_pred_2, prop.chisq = <span class="literal">FALSE</span>)</span><br><span class="line">mean(sms_test_pred_2==sms_raw_test$type)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;基础操作&quot;&gt;&lt;a href=&quot;#基础操作&quot; class=&quot;headerlink&quot; title=&quot;基础操作&quot;&gt;&lt;/a&gt;基础操作&lt;/h1&gt;&lt;p&gt;查看数据类型：&lt;/p&gt;
&lt;figure class=&quot;highlight r&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;
      
    
    </summary>
    
      <category term="编程语言" scheme="https://hhyz.me/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="R语言" scheme="https://hhyz.me/tags/R%E8%AF%AD%E8%A8%80/"/>
    
  </entry>
  
  <entry>
    <title>聚类</title>
    <link href="https://hhyz.me/2019/04/25/Clustering/"/>
    <id>https://hhyz.me/2019/04/25/Clustering/</id>
    <published>2019-04-24T16:00:00.000Z</published>
    <updated>2019-07-05T06:57:06.686Z</updated>
    
    <content type="html"><![CDATA[<h2 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h2><p>我们的问题为：</p><p>$$<br>\min _{C_{1}, \ldots, C_{K}} \sum_{k=1}^{K} W\left(C_{k}\right)<br>$$</p><p>$$<br>W\left(C_{k}\right)=\frac{1}{\left|C_{k}\right|} \sum_{i, j \in C_{k}}\Vert x_{i}-x_{j}\Vert _{2}^{2}<br>$$</p><p>将以上两个式子结合一下，可以得到最优化问题为：</p><p>$$<br>\min _{C_{1}, \ldots, C_{K}} \sum_{k=1}^{K} \frac{1}{\left|C_{k}\right|} \sum_{i, j \in C_{k}}\Vert x_{i}-x_{j}\Vert _{2}^{2}<br>$$</p><p><strong>Centroid</strong></p><p>让 \(\mu_{k}=\frac{1}{\left|C_{k}\right|} \sum_{i \in C_{k}} x_{i}\) 为 \( C_{k}\) 的 mean/centroid。</p><p>$$<br>\frac{1}{\left|C_{k}\right|} \sum_{i, j \in \mathcal{C}_{k}}\Vert x_{i}-x_{j}\Vert _{2}^{2}=2 \sum_{i \in C_{k}}\Vert x_{i}-\mu_{k}\Vert _{2}^{2}<br>$$ </p><p>代入回之前的式子，优化问题等同于：</p><p>$$<br>\min _{C_{1}, \ldots, C_{K}} \sum_{k=1}^{K}\left\{\sum_{i \in C_{k}}\Vert x_{i}-\mu_{k}\Vert _{2}^{2}\right\}<br>$$</p><h3 id="迭代法"><a href="#迭代法" class="headerlink" title="迭代法"></a>迭代法</h3><p> \(\text { binary matrix } R=\left[r_{n k}\right] \in R^{N \times K}<br>\)<br> \(\text { 如果 } x_{n} \text { is assigned to cluster } k, \text { 那么 }r_{n k}=1 \text { 并且 } r_{n j}=0, j \neq k\) </p><br>**目标：**<p>找到  \(\left\{\mu_{k}\right\}\)，并把每一个数据点分配到一类，使 objective function 最小化。</p><p>$$<br>J\left(R,\left\{\mu_{k}\right\}\right)=\sum_{n=1}^{N}\left[\sum_{k=1}^{K} r_{n k}\Vert x_{n}-\mu_{k}\Vert ^{2}\right]<br>$$ </p><ul><li>\(\mu_{k}=\frac{1}{\left|C_{k}\right|} \sum_{i \in C_{k}} x_{i}\) 为 \( C_{k}\) 的 mean/centroid。</li><li>\(R=\left[r_{n k}\right] \in R^{N \times K}\)</li></ul><p><strong>重复以下两步：</strong></p><ol><li>step 1：固定 \(\{\mu_{k}\}\)，最优化 \(R\)  </li><li>step 2：固定 \(R\)，最优化 \(\{\mu_{k}\}\)</li></ol><p><strong>具体如下：</strong></p><h4 id="step-1"><a href="#step-1" class="headerlink" title="step 1"></a>step 1</h4><p>对于某一个具体的 n，我们选择 \(r_{nj}\) 最小化 </p><p>$$\sum_{k=1}^{K} r_{n k}\Vert x_{n}-\mu_{k}\Vert ^{2}$$ </p><p><img src="/img/2019-04-25-Clustering-1.png" alt></p><p><strong>也就是说，将 \(x_n\) 分配给最接近的 centroid。</strong></p><h4 id="step-2"><a href="#step-2" class="headerlink" title="step 2"></a>step 2</h4><p>对于固定的  \(R\),   \(J\left(R,\left\{\mu_{k}\right\}\right)\) 是 convex，quadratic 的，因此，将关于  \(u_{k}\) 的梯度设为 0：</p><p>$$<br>2 \sum_{n=1}^{N} r_{n k}\left(\mu_{k}-x_{n}\right)=0 \Rightarrow \mu_{k}=\frac{\sum_{n=1}^{N} r_{n k} x_{n}}{\sum_{n=1}^{N} r_{n k}}<br>$$</p><p>让 \(u_{k}\) 等于属于 cluster k 的所有数据点 \(x_n\) 的均值</p><p><img src="/img/2019-04-25-Clustering-2.png" alt></p><blockquote><p>k-means 算法对于异常值十分敏感，因为具有极大值的对象可能会产生严重扭曲的数据分布  </p></blockquote><h2 id="k-medoids"><a href="#k-medoids" class="headerlink" title="k-medoids"></a>k-medoids</h2><p>k-means 与 k-medoids 之间的差异就是可以理解为对于数据样本的平均值和中位数之间的差异：前者的取值范围可以是连续空间中的任意值，而后者的取值却只能是数据样本范围中的样本。这个原因就是 k-means 对于数据样本的要求太高了，要求所有数据样本处在一个欧式空间中，对于有很多噪声的数据就会造成极大的误差。同时对于非数值型数据样本，不能够计算平均值等实数型变量。</p><p>与 K-means 算法类似，区别在于中心点的选取，K-means 中选取的中心点为当前类中所有点的重心，而 K-medoids 法选取的中心点为当前 cluster 中存在的一点，准则函数是当前 cluster 中所有其他点到该中心点的距离之和最小，这就在一定程度上削弱了异常值的影响，但缺点是计算较为复杂，耗费的计算机时间比 K-means 多。</p><p>$$<br>\widehat{J}\left(R,\left\{\mu_{k}\right\}\right)=\sum_{n=1}^{N} \sum_{k=1}^{K} r_{n k} V\left(x_{n}, \mu_{k}\right)<br>$$</p><ul><li>其中 \(V\left(x_{n}, \mu_{k}\right)\) 表示 \(x_n\) 和 \(\mu_k\) 的 dissimilarity</li></ul><h2 id="Hierarchical-Clustering"><a href="#Hierarchical-Clustering" class="headerlink" title="Hierarchical Clustering"></a>Hierarchical Clustering</h2><p>K-means 需要提前确定 k 的值，而 Hierarchical Clustering 不需要</p><p>假设有N个待聚类的样本，对于层次聚类来说，其步骤为：</p><ol><li>初始化：把每个样本各自归为一类（每个样本自成一类），计算每两个类之间的距离，在这里也就是样本与样本之间的相似度（本质还是计算类与类之间的距离）。</li><li>寻找各个类之间最近的两个类，把它们归为一类（这样，类的总数就减少了一个）</li><li>重新计算新生成的这个类与各个旧类之间的距离（相似度）</li><li>重复 2、3 步，直到所有的样本都归为一类，结束。</li></ol><p>2、详细描述：</p><p>整个聚类过程其实是建立了一棵树，在建立过程中，可以通过第二步上设置一个阈值，当最近的两个类的距离大于这个阈值，则认为迭代终止。</p><p>另外，关键的一步是第三步，如何判断两个类之间的相似度有不少种方法，下面介绍三种：</p><ol><li><p><strong>Single Linkage</strong>：又叫做 nearest-neighbor，就是取两个类中最近的两个样本之间的距离作为两个集合的距离，即：最近的两个样本之间的距离越小，</p><p> 这两个类之间相似度越大，容易造成一种叫做 Chaining 的效果，两个类明明从“大局”上离的比较远，但由于其中个别点距离比较近就被合并了。</p><p> 这种合并之后 Chaining 效应会进一步扩大，最后得到比较松散的聚类 cluster。</p></li><li><p><strong>Complete Linkage</strong>：完全是 Single Linkage 的反面极端，取两个集合距离最远的两个点的距离作为两个集合的距离，其效果也刚好相反，限制非常大。</p><p> 两个聚类 cluster 即使已经很接近了，但是只要有不配合的带你存在，就顽固到底，老死不相合并，也是不太好的办法，这两种相似度定义方法共同问题就是：</p><p> 只考虑了某个特有的数据，而没有考虑类数据整体的特点。</p></li><li><p><strong>Average Linkage</strong>：这种方法就是把两个集合中的点两两距离全部放在一起求平均值，相应的能得到一点合适的结果。</p><p> Average Linkage 的一个变种就是取两两距离的中值，与取平均值相比更加能够解除个别偏离样本对结果的干扰。</p></li></ol><p><img src="/img/2019-04-25-Clustering-5.png" alt><br><img src="/img/2019-04-25-Clustering-6.png" alt></p><p><img src="/img/2019-04-25-Clustering-7.png" alt></p><div align="center"><center><small><font color="gray">  Complete Linkage   </font></small></center></div><p><img src="/img/2019-04-25-Clustering-4.png" alt><br><img src="/img/2019-04-25-Clustering-3.png" alt></p><p>对于 Hierarchical Clustering 的实际应用，有以下问题需要考虑：</p><ul><li>应该使用哪一种 dissimilarity measure？</li><li>应该使用哪一种 linkage？</li><li>什么时候应该切断树状结构？</li></ul><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;K-means&quot;&gt;&lt;a href=&quot;#K-means&quot; class=&quot;headerlink&quot; title=&quot;K-means&quot;&gt;&lt;/a&gt;K-means&lt;/h2&gt;&lt;p&gt;我们的问题为：&lt;/p&gt;
&lt;p&gt;$$&lt;br&gt;\min _{C_{1}, \ldots, C_{K}} 
      
    
    </summary>
    
      <category term="机器学习" scheme="https://hhyz.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://hhyz.me/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>主成分分析</title>
    <link href="https://hhyz.me/2019/04/22/PCA/"/>
    <id>https://hhyz.me/2019/04/22/PCA/</id>
    <published>2019-04-21T16:00:00.000Z</published>
    <updated>2019-07-02T17:05:20.731Z</updated>
    
    <content type="html"><![CDATA[<p>PCA的思想就是将n维特征映射到k维上(k&lt;n)，这k维是重新构造出来的全新维度特征，而不是简单的从n维特征去除n-k维特征，这k维就是主成分。</p><p>在三维坐标系中，四种颜色的标记界限并不直观，但是在重新定义的二维坐标系中，四种颜色标记界限非常直观，这就是一个典型的PCA，在降维的同时最大程度的保留了数据的特征，为后续的分析提供更直观的支持。</p><p><img src="/img/2019-04-22-PCA-1.jpg" alt></p><h2 id="基础数学知识"><a href="#基础数学知识" class="headerlink" title="基础数学知识"></a>基础数学知识</h2><h3 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h3><p>$$<br>{Var}(X)=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2}<br>$$</p><h3 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h3><p>$$<br>{Cov}(X, Y)=\frac{1}{n} \sum_{i=1}^{n} X_{i} Y_{i}=\frac{1}{n} X \cdot Y<br>$$</p><p>当 \(Cov(a,b)=0\)  时，表示两个字段完全独立，这也是我们的优化目标。</p><h3 id="特征值分解"><a href="#特征值分解" class="headerlink" title="特征值分解"></a>特征值分解</h3><p>如果说一个向量 \(v\) 是方阵 \(A\) 的特征向量，将一定可以表示成下面的形式：</p><p>$$<br>A v=\lambda v<br>$$</p><p>这时候 \(λ\) 就被称为特征向量 \(v\) 对应的特征值。<br>一个矩阵的一组<strong>特征向量</strong>是一组<strong>正交向量</strong>。</p><p><strong>特征分解：</strong></p><p>$$<br>A=Q \Sigma Q^{-1}<br>$$</p><p>其中 \(Q\) 是这个矩阵 \(A\) 的特征向量组成的矩阵，正交矩阵是可逆的。</p><p>\(\Sigma=diag\left(\lambda_{1}, \lambda_{2}, \dots, \lambda_{n}\right)\) 是一个对角阵，每一个对角线上的元素就是一个特征值。</p><blockquote><p>一个矩阵其实就是一个线性变换，因为一个矩阵乘以一个向量后得到的向量，其实就相当于将这个向量进行了线性变换。    </p><p><strong>特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么</strong></p></blockquote><h3 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h3><p>特征值分解是一个提取矩阵特征很不错的方法，但是它只<strong>适用于方阵</strong>。而在现实的世界中，我们看到的大部分矩阵都不是方阵，而奇异值分解是一个能<strong>适用于任意的矩阵</strong>的一种分解的方法。</p><p>奇异值分解实际上把矩阵的变换分为了三部分：</p><ul><li>旋转</li><li>拉伸</li><li>投影</li></ul><p><img src="/img/2019-04-22-PCA-2.jpg" alt></p><p>$$<br>A=U \Sigma V^{T}<br>$$</p><p><img src="/img/2019-04-22-PCA-3.jpg" alt></p><ul><li>\(A\) 是一个 \(M * N\) 的矩阵</li><li>\(U\) 是一个 \(M * M\) 的方阵（里面的向量是正交的，U里面的向量称为<strong>左奇异向量</strong>），</li><li>\(Σ\) 是一个 \(M * N\) 的实数对角矩阵（对角线以外的元素都是 \(0\) ，对角线上的元素称为<strong>奇异值</strong>），</li><li>\(V^{T}\) 是一个 \(N * N\) 的矩阵，里面的向量也是正交的， \(V\) 里面的向量称为<strong>右奇异向量</strong>）</li></ul><p>奇异值和特征值是怎么对应起来的呢？</p><p>首先，我们将一个矩阵A的转置 \(\mathrm{A}^{\top} * \mathrm{A}\)，将会得到 \(\mathrm{A}^{\top} \mathrm{A}\) 是一个方阵，我们用这个方阵求特征值可以得到：</p><p>$$<br>\left(A^{T} A\right) v_{i}=\lambda_{i} v_{i}<br>$$<br>并且有：<br>$$<br>\sigma_{i}=\sqrt{\lambda_{i}}<br>$$<br>$$<br>u_{i}=\frac{1}{\sigma_{i}} A v_{i}<br>$$</p><ul><li>\(σ_i\)  是就是奇异值， </li><li>\(u_i\) 是左奇异向量</li><li>\(v\) 是右奇异向量</li></ul><p>常见的做法是将奇异值由大而小排列。如此 \(Σ\) 便能由 \(M\) 唯一确定。</p><p>$$<br>A=U \Sigma V^{T} \Rightarrow A V=U \Sigma V^{T} V \Rightarrow A V=U \Sigma \Rightarrow A v_{i}=\sigma_{i} u_{i} \Rightarrow \sigma_{i}=A v_{i} / u_{i}<br>$$</p><p>奇异值 \(σ\) 跟特征值类似，在矩阵 \(Σ\) 中也是从大到小排列，而且 \(σ\) 的减少特别的快，在很多情况下，前 10% 甚至 1% 的奇异值的和就占了全部的奇异值之和的 99% 以上了。</p><p>也就是说，我们也可以用前 \(r\) 大的奇异值来近似描述矩阵</p><p><strong>部分奇异值分解：</strong></p><p>$$<br>A_{m \times n} \approx U_{m \times r} \Sigma_{r \times r} V_{r \times n}^{T}<br>$$</p><p> \(r\) 是一个远小于 \(m\)、 \(n\) 的数，这样矩阵的乘法看起来像是下面的样子：</p><p><img src="/img/2019-04-22-PCA-4.jpg" alt></p><p>右边的三个矩阵相乘的结果将会是一个接近于 \(A\) 的矩阵，在这儿， \(r\) 越接近于 \(n\) ，则相乘的结果越接近于 \(A\) 。而这三个矩阵的面积之和（在存储观点来说，矩阵面积越小，存储量就越小）要远远小于原始的矩阵 \(A\)。 </p><p>如果想要压缩空间来表示原矩阵 \(A\)，只需存下三个矩阵：\(U、Σ、V\)。</p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"></script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;PCA的思想就是将n维特征映射到k维上(k&amp;lt;n)，这k维是重新构造出来的全新维度特征，而不是简单的从n维特征去除n-k维特征，这k维就是主成分。&lt;/p&gt;
&lt;p&gt;在三维坐标系中，四种颜色的标记界限并不直观，但是在重新定义的二维坐标系中，四种颜色标记界限非常直观，这就是一
      
    
    </summary>
    
      <category term="机器学习" scheme="https://hhyz.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://hhyz.me/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机</title>
    <link href="https://hhyz.me/2019/04/10/SVM/"/>
    <id>https://hhyz.me/2019/04/10/SVM/</id>
    <published>2019-04-09T16:00:00.000Z</published>
    <updated>2019-07-02T17:05:20.728Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://zhuanlan.zhihu.com/p/31652569" target="_blank" rel="noopener">从零推导支持向量机(SVM)</a><br><a href="https://github.com/HaoMood/File/blob/master/%E4%BB%8E%E9%9B%B6%E6%8E%A8%E5%AF%BC%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA(SVM).pdf" target="_blank" rel="noopener">Github PDF  地址</a></p></blockquote><h2 id="1-线性二分类模型"><a href="#1-线性二分类模型" class="headerlink" title="1. 线性二分类模型"></a>1. 线性二分类模型</h2><p><img src="/img/2019-04-10-SVM-1.jpg" alt></p><h2 id="2-线性支持向量机"><a href="#2-线性支持向量机" class="headerlink" title="2. 线性支持向量机"></a>2. 线性支持向量机</h2><p><img src="/img/2019-04-10-SVM-2.jpg" alt></p><h3 id="2-1-间隔"><a href="#2-1-间隔" class="headerlink" title="2.1 间隔"></a>2.1 间隔</h3><p><img src="/img/2019-04-10-SVM-3.jpg" alt></p><h3 id="2-2-线性支持向量机基本型"><a href="#2-2-线性支持向量机基本型" class="headerlink" title="2.2 线性支持向量机基本型"></a>2.2 线性支持向量机基本型</h3><p><img src="/img/2019-04-10-SVM-4.jpg" alt><br><img src="/img/2019-04-10-SVM-5.jpg" alt><br><img src="/img/2019-04-10-SVM-6.jpg" alt></p><h2 id="3-核函数"><a href="#3-核函数" class="headerlink" title="3. 核函数"></a>3. 核函数</h2><p><img src="/img/2019-04-10-SVM-7.jpg" alt></p><h3 id="3-1-非线性可分问题"><a href="#3-1-非线性可分问题" class="headerlink" title="3.1 非线性可分问题"></a>3.1 非线性可分问题</h3><p><img src="/img/2019-04-10-SVM-8.jpg" alt></p><p><img src="/img/2019-04-10-SVM-9.jpg" alt></p><h2 id="4-软间隔"><a href="#4-软间隔" class="headerlink" title="4.  软间隔"></a>4.  软间隔</h2><p><img src="/img/2019-04-10-SVM-10.jpg" alt><br><img src="/img/2019-04-10-SVM-11.jpg" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/31652569&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;从零推导支持向量机(SVM)&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://github
      
    
    </summary>
    
      <category term="机器学习" scheme="https://hhyz.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://hhyz.me/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>CART (Classification And Regression Tree)</title>
    <link href="https://hhyz.me/2019/03/30/CART/"/>
    <id>https://hhyz.me/2019/03/30/CART/</id>
    <published>2019-03-29T16:00:00.000Z</published>
    <updated>2019-07-05T06:03:23.925Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>求  \(R_m\)： </p><ol><li><p>扩张树：用贪心，上至下递归分区方法    </p></li><li><p>split function 选择最好的特征  \(j*\) 和该特征最好的值 \(t*\) </p><p> <img src="/img/2019-03-30-CART-1.jpg" alt></p></li></ol><p>Split s divide the current node into two children.<br>比如，对于 t，\(\text{Left-Child }= {(y_i, x_i) : x_{ij} ≤ t}\ 而\ \text{Right-Child }= {(y_i, x_i) : x_{ij} &gt; t}\) </p><p><strong>2D 的例子</strong></p><p><img src="/img/2019-03-30-CART-2.jpg" alt></p><h2 id="Splitting-规则"><a href="#Splitting-规则" class="headerlink" title="Splitting 规则"></a>Splitting 规则</h2><h3 id="1-Regression"><a href="#1-Regression" class="headerlink" title="1. Regression"></a>1. Regression</h3><p><img src="/img/2019-03-30-CART-3.jpg" alt></p><h3 id="2-Classification"><a href="#2-Classification" class="headerlink" title="2. Classification"></a>2. Classification</h3><p><img src="/img/2019-03-30-CART-4.jpg" alt></p><p><strong>不纯度函数(impurity function)</strong></p><ol><li><p>当所有样本都属于同一类时候  \(I\)  取最小值. 即 \(I\) 在点  \((1,0,…,0),(0,1,…,0),…,(0,..,0,1)\)  取最小值.</p></li><li><p>当样本中每个类目下样本个数相同时I取最大值. 即 \(I\) 在点 \((1/k,..,1/k)\) 取最大值.</p></li></ol><h2 id="Prunning-剪枝"><a href="#Prunning-剪枝" class="headerlink" title="Prunning 剪枝"></a>Prunning 剪枝</h2><p>代价复杂性剪枝 (cost complexity pruning)</p><p>$$min \ \ \frac {1}{N} \sum^{\vert T \vert}_{m=1} \sum_{x_i \in {R_m}} L(y_i, w_m) + \alpha \vert T\vert$$</p><p> \(\vert T \vert\) 是 termainal nodes 的总数</p><p>  \(L(·, ·)\) 是 loss function, 例如 \(L(yi, f (x_i)) = L(x_i,w_m) = (y_i − w_m)^2\) </p><p>  \(w_m\) 是与 \(R_m\) 对应的预测值  \(\rightarrow\)  也就是 \(R_m\) 中训练集的平均值</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p><img src="/img/2019-03-30-CART-5.jpg" alt></p><p><img src="/img/2019-03-30-CART-6.jpg" alt></p><p><img src="/img/2019-03-30-CART-7.jpg" alt></p><p>Multiple trees: </p><ul><li>bagging 袋装法</li><li>random forests 随机森林</li><li>boosting 提升法</li></ul><h2 id="boosting"><a href="#boosting" class="headerlink" title="boosting"></a>boosting</h2><blockquote><p><strong>基本思想</strong>：<br>在分类问题中，通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类器的性能。</p></blockquote><p><strong>历史</strong>：</p><ul><li>PAC learning framework (1990)</li><li>AdaBoost methods (1996)</li><li>gradient boosting (2000)</li></ul><p><strong>weak learner</strong>:<br>classifiers whose error rate is slightly better than random guessing</p><p>Boosting 改变训练样本的权重，产生一系列的分类器：<br><img src="/img/2019-03-30-CART-8.jpg" alt></p><p>最终的分类器可以表示为：<br><img src="/img/2019-03-30-CART-9.jpg" alt></p><p> \(\alpha_m\)：分类系数（由 boosting 算法计算得出）</p><h3 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h3><p><img src="/img/2019-03-30-CART-10.jpg" alt></p><p><img src="/img/2019-03-30-CART-11.jpg" alt></p><p><img src="/img/2019-03-30-CART-12.jpg" alt></p><h2 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h2><p>你可能会问为什么不直接使用一个决策树？这种分类器堪称完美，因为根本不会犯任何错误！但要记住一个重点：决策树只是不会在训练数据上犯错。</p><p>随机森林是由许多决策树构成的模型。这不仅仅是森林，而且是随机的，这涉及到两个概念：</p><ol><li><p>随机采样数据点</p></li><li><p>基于特征的子集分割节点</p></li></ol><p>随机森林由LeoBreiman（2001）提出，它通过自助法（bootstrap）重采样技术，从原始训练样本集N中有放回地重复随机抽取k个样本生成新的训练样本集合，然后根据自助样本集生成k个分类树组成随机森林，新数据的分类结果按分类树投票多少形成的分数而定。其实质是对决策树算法的一种改进，将多个决策树合并在一起，每棵树的建立依赖于一个独立抽取的样品，森林中的每棵树具有相同的分布，分类误差取决于每一棵树的分类能力和它们之间的相关性。特征选择采用随机的方法去分裂每一个节点，然后比较不同情况下产生的误差。能够检测到的内在估计误差、分类能力和相关性决定选择特征的数目。单棵树的分类能力可能很小，但在随机产生大量的决策树后，一个测试样品可以通过每一棵树的分类结果经统计后选择最可能的分类。</p><p><strong>随机森林的构建过程</strong></p><p>决策树相当于一个大师，通过自己在数据集中学到的知识对于新的数据进行分类。但是俗话说得好，一个诸葛亮，玩不过三个臭皮匠。随机森林就是希望构建多个臭皮匠，希望最终的分类效果能够超过单个大师的一种算法。</p><p>那随机森林具体如何构建呢？有两个方面：数据的随机性选取，以及待选特征的随机选取。</p><p><strong>数据的随机选取</strong></p><p>首先，从原始的数据集中采取有放回的抽样，构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。第二，利用子数据集来构建子决策树，将这个数据放到每个子决策树中，每个子决策树输出一个结果。最后，如果有了新的数据需要通过随机森林得到分类结果，就可以通过对子决策树的判断结果的投票，得到随机森林的输出结果了。</p><p>随机森林采用的是bagging的思想，bagging又称为bootstrap aggreagation，通过在训练样本集中进行有放回的采样得到多个采样集，基于每个采样集训练出一个基学习器，再将基学习器结合。随机森林在对决策树进行bagging的基础上，在决策树的训练过程中引入了随机属性选择。传统决策树在选择划分属性的时候是在当前节点属性集合中选择最优属性，而随机森林则是对结点先随机选择包含k个属性的子集，再选择最有属性，k作为一个参数控制了随机性的引入程度。</p><p>另外，GBDT训练是基于Boosting思想，每一迭代中根据错误更新样本权重，因此是串行生成的序列化方法，而随机森林是bagging的思想，因此是并行化方法。</p><p>尽管有剪枝等等方法，一棵树的生成肯定还是不如多棵树，因此就有了随机森林，解决决策树泛化能力弱的缺点。（可以理解成三个臭皮匠顶过诸葛亮）</p><p>而同一批数据，用同样的算法只能产生一棵树，这时Bagging策略可以帮助我们产生不同的数据集。Bagging策略来源于bootstrap aggregation：从样本集（假设样本集N个数据点）中重采样选出Nb个样本（有放回的采样，样本数据点个数仍然不变为N），在所有样本上，对这n个样本建立分类器（ID3\C4.5\CART\SVM\LOGISTIC），重复以上两步m次，获得m个分类器，最后根据这m个分类器的投票结果，决定数据属于哪一类。</p><p>随机森林在bagging的基础上更进一步：</p><ol><li><p>样本的随机：从样本集中用Bootstrap随机选取n个样本</p></li><li><p>特征的随机：从所有属性中随机选取K个属性，选择最佳分割属性作为节点建立CART决策树（泛化的理解，这里面也可以是其他类型的分类器，比如SVM、Logistics）</p></li><li><p>重复以上两步m次，即建立了m棵CART决策树</p></li><li><p>这m个CART形成随机森林，通过投票表决结果，决定数据属于哪一类（投票机制有一票否决制、少数服从多数、加权多数）</p></li></ol><p>假设随机森林中有3棵子决策树，2棵子树的分类结果是A类，1棵子树的分类结果是B类，那么随机森林的分类结果就是A类。</p><p>关于调参：</p><ol><li>如何选取K，可以考虑有N个属性，取K=根号N</li><li>最大深度（不超过8层）</li><li>棵数</li><li>最小分裂样本树</li><li>类别比例</li></ol><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;求  \(R_m\)： &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;扩张树：用贪心，上至下递归分区方法    &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;sp
      
    
    </summary>
    
      <category term="机器学习" scheme="https://hhyz.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://hhyz.me/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>机器学习汇总</title>
    <link href="https://hhyz.me/2019/03/25/ml/"/>
    <id>https://hhyz.me/2019/03/25/ml/</id>
    <published>2019-03-24T16:00:00.000Z</published>
    <updated>2019-07-02T17:05:20.724Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><h2 id="1-参数法与非参数法"><a href="#1-参数法与非参数法" class="headerlink" title="1. 参数法与非参数法"></a>1. 参数法与非参数法</h2><p>机器学习上的方法分：</p><ul><li><p><strong>参数方法</strong></p><p>  根据先验知识假定模型服从某种分布，然后利用训练集估计出模型参数，也就弄清楚了整个模型。   </p><p>  假设了一个在整个输入空间上有效的模型，将问题归结为在样本上估计少量参数，(如：线性模型估计w，高斯分布估计mu和sigma)，参数学习方法假定了一个模型,当模型假定不成立或样本不是一个分组可能导致很大的误差。</p><p>  例如感知器</p></li><li><p><strong>非参数方法</strong></p><p>  不需要知道数据的概率分布，只需要假设：相似的输入具有相似的输出。基于记忆训练集，然后根据训练集预测。</p><p>  非参数方法使用合适的距离度量相似性，对于输入样本，从训练集中找出它们的相似示例(输入样本的邻域)，并由相似的实例插值得到正确的输入。</p><p>  例如kNN</p></li></ul><h2 id="2-维度灾难"><a href="#2-维度灾难" class="headerlink" title="2. 维度灾难"></a>2. 维度灾难</h2><blockquote><p>Curse of Dimensionality</p></blockquote><p>维度灾难是在数字图像处理中，对于已知样本数目，存在一个特征数目的最大值，当实际使用的特征数目超过这个最大值时，分类器的性能不是得到改善，而是退化。</p><p>这种现象正是在识别模式中被称为“维度灾难”的一种表现形式。此外，提取特征向量的维度过<br>高会增加计算的复杂度，给后续的分类问题造成负面影响。</p><p>非参数方法在特征数 p 很大时表现不好。</p><h2 id="3-分类器性能指标"><a href="#3-分类器性能指标" class="headerlink" title="3. 分类器性能指标"></a>3. 分类器性能指标</h2><h3 id="3-1-ROC-曲线"><a href="#3-1-ROC-曲线" class="headerlink" title="3.1 ROC 曲线"></a>3.1 ROC 曲线</h3><blockquote><p>receiver operating characteristics 接收者操作特征</p></blockquote><p><img src="/img/2019-03-25-ml-1.jpg" alt></p><p>由上表可得出横，纵轴的计算公式：</p><ol><li><p>真正类率 (True Postive Rate)： \(TPR = \frac{TP}{TP+FN}\)， 代表分类器预测的正类中实际正实例占所有正实例的比例。Sensitivity</p></li><li><p>负正类率 (False Postive Rate) ： \(FPR = \frac{FP}{FP+TN}\)，代表分类器预测的正类中实际负实例占所有负实例的比例。1-Specificity</p></li><li><p>真负类率 (True Negative Rate) ： \(TNR = \frac{TN}{FP+TN}\)，代表分类器预测的负类中实际负实例占所有负实例的比例， \(TNR=1-FPR\) 。Specificity</p></li></ol><p>假设采用逻辑回归分类器，其给出针对每个实例为正类的概率，那么通过设定一个阈值如0.6，概率大于等于0.6的为正类，小于0.6的为负类。对应的就可以算出一组(FPR,TPR),在平面中得到对应坐标点。</p><p>随着阈值的逐渐减小，越来越多的实例被划分为正类，但是这些正类中同样也掺杂着真正的负实例，即TPR和FPR会同时增大。</p><p><em>阈值最大时，对应坐标点为 \((0,0)\) ,阈值最小时，对应坐标点 \((1,1)\) 。</em></p><p><img src="/img/2019-03-25-ml-2.jpg" alt></p><p><strong>横轴</strong>：负正类率 (false postive rate FPR) 特异度，<em>FPR越大，预测正类中实际负类越多。</em><br><strong>纵轴</strong>：真正类率 (true postive rate TPR )灵敏度，<em>TPR越大，预测正类中实际正类越多。</em></p><p><strong>理想目标</strong>：     </p><p>\(TPR=1，FPR=0\)，即图中 \((0,1)\) 点，故ROC曲线越靠拢 \((0,1)\) 点，越偏离 45 度对角线越好，Sensitivity、Specificity 越大效果越好。</p><h3 id="3-2-AUC"><a href="#3-2-AUC" class="headerlink" title="3.2 AUC"></a>3.2 AUC</h3><blockquote><p>area under the curve</p></blockquote><p>常常用AUC来评估二分类模型的性能</p><p>Roc 曲线下的面积，介于 0.1 和 1 之间。Auc 作为数值可以直观的评价分类器的好坏，值越大越好。</p><p>首先 AUC 值是一个概率值，当你随机挑选一个正样本以及负样本，<strong>当前的分类算法根据计算得到的Score 值将这个正样本排在负样本前面的概率就是 AUC 值，AUC 值越大，当前分类算法越有可能将正样本排在负样本前面，从而能够更好地分类。</strong></p><p>AUC 可以看做随机从正负样本中选取一对正负样本，其中正样本的得分大于负样本的概率！</p><p><strong>AUC 对正负样本比例不敏感</strong></p><p>利用概率解释，还可以得到AUC另外一个性质，对正负样本比例不敏感。 在训练模型的时候，如果正负比例差异比较大，例如正负比例为1:1000，训练模型的时候通常要对负样本进行下采样。当一个模型训练完了之后，用负样本下采样后的测试集计算出来的AUC和未采样的测试集计算的AUC基本一致，或者说前者是后者的无偏估计！ </p><h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><h2 id="1-梯度法"><a href="#1-梯度法" class="headerlink" title="1. 梯度法"></a>1. 梯度法</h2><p>次梯度法（subgradient method）是传统梯度下降方法的拓展，用来处理不可微（non-differentiable ）的凸函数。它的优势是比传统方法处理问题范围大，但劣势是算法收敛速度慢。而传统的梯度下降方法只能处理可导函数。</p><p><img src="/img/2019-03-25-ml-3.jpg" alt></p><blockquote><p>其实无论是梯度法还是次梯度法，本质上我们都在使用一阶泰勒展开式的原理去逼近在某点的原函数。正如泰勒展开式的思想所述，将目标函数在某点附近展开为泰勒(Taylor)多项式来逼近原函数。</p></blockquote><h2 id="1-朴素贝叶斯"><a href="#1-朴素贝叶斯" class="headerlink" title="1. 朴素贝叶斯"></a>1. 朴素贝叶斯</h2><p><img src="/img/2019-03-25-ml-4.jpg" alt></p> <div align="center"> ![](/img/2019-03-25-ml-101.jpg)<center><small><font color="gray">    </font></small></center></div><p><strong>实例解析</strong></p><p><img src="/img/2019-03-25-ml-5.jpg" alt></p><blockquote><p>现在给我们的问题是，如果一对男女朋友，男生想女生求婚，男生的四个特点分别是不帅，性格不好，身高矮，不上进，请你判断一下女生是嫁还是不嫁？</p></blockquote><p>这是典型的二分类问题，按照朴素贝叶斯的求解，转换为 </p><ul><li>\(P(嫁 | 不帅、性格不好、矮、不上进)\) </li><li>\(P(不嫁 | 不帅、性格不好、矮、不上进)\) </li></ul><p>的概率，最终选择嫁与不嫁的答案。</p><p>这里我们根据贝特斯公式:</p><p><img src="/img/2019-03-25-ml-6.jpg" alt></p><p>$$P(不帅、性格不好、矮、不上进)=P(嫁)P(不帅|嫁)P(性格不好|嫁)P(矮|嫁)P(不上进|嫁)$$<br>$$+P(不嫁)P(不帅|不嫁)P(性格不好|不嫁)P(矮|不嫁)P(不上进|不嫁)$$  </p><p>$$P(不帅、性格不好、矮、不上进|嫁)=P(不帅|嫁)P(性格不好|嫁)P(矮|嫁)P(不上进|嫁)$$</p><br>将上面的公式整理一下可得:<p><img src="/img/2019-03-25-ml-7.jpg" alt></p><p>但是由贝叶斯公式可得:对于目标求解为不同的类别，贝叶斯公式的分母总是相同的。所以，只求解分子即可：</p><p>$$P(嫁)P(不帅|嫁)P(性格不好|嫁)P(矮|嫁)P(不上进|嫁)=1/2 * 1/2 * 1/6 * 1/6 * 1/6=1/864$$   </p><p>对于类别“不嫁”的贝叶斯分子为:</p><p>$$P(不嫁)P(不帅|不嫁)P(性格不好|不嫁)P(矮|不嫁)P(不上进|不嫁)=1/2 * 1/3 * 1/2 * 1* 2/3=1/18$$</p><p>经代入贝叶斯公式可得：</p><p>$$P(嫁|不帅、性格不好、矮、不上进)=(1/864) / (1/864+1/18)=1/49=2.04%$$</p><p>$$P(不嫁|不帅、性格不好、矮、不上进)=(1/18) / (1/864+1/18)=48/49=97.96%$$</p><p>则 \(P(不嫁|不帅、性格不好、矮、不上进) &gt; P(嫁|不帅、性格不好、矮、不上进)\) ，则该女子选择不嫁！</p><br><p><strong>优点</strong>：  </p><ul><li>算法逻辑简单,易于实现（算法思路很简单，只要使用贝叶斯公式转化即可！）</li><li>分类过程中时空开销小（假设特征相互独立，只会涉及到二维存储）</li></ul><p><strong>缺点</strong>：</p><ul><li>朴素贝叶斯假设属性之间相互独立，这种假设在实际过程中往往是不成立的。在属性之间相关性越大，分类误差也就越大。</li></ul><blockquote><p><a href="https://blog.csdn.net/fisherming/article/details/79509025" target="_blank" rel="noopener">带你彻彻底底搞懂朴素贝叶斯公式</a><br><a href="https://www.cnblogs.com/pinard/p/6069267.html" target="_blank" rel="noopener">朴素贝叶斯算法原理小结</a></p></blockquote><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;基础知识&quot;&gt;&lt;a href=&quot;#基础知识&quot; class=&quot;headerlink&quot; title=&quot;基础知识&quot;&gt;&lt;/a&gt;基础知识&lt;/h1&gt;&lt;h2 id=&quot;1-参数法与非参数法&quot;&gt;&lt;a href=&quot;#1-参数法与非参数法&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
      <category term="机器学习" scheme="https://hhyz.me/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://hhyz.me/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>c++ 基础汇总</title>
    <link href="https://hhyz.me/2019/03/17/cpp/"/>
    <id>https://hhyz.me/2019/03/17/cpp/</id>
    <published>2019-03-16T16:00:00.000Z</published>
    <updated>2019-07-09T17:21:20.482Z</updated>
    
    <content type="html"><![CDATA[<h1 id="类"><a href="#类" class="headerlink" title="类"></a>类</h1><h2 id="1-类成员函数"><a href="#1-类成员函数" class="headerlink" title="1. 类成员函数"></a>1. 类成员函数</h2><p>类的成员函数是指那些<strong>把定义和原型写在类定义内部的函数，就像类定义中的其他变量一样</strong>。</p><p>类成员函数是类的一个成员，它可以操作类的任意对象，可以访问对象中的所有成员。</p><p>范围解析运算符 <code>::</code></p><p>在类里面不写是什么类型，默认是 private 的。</p><h2 id="2-构造函数-constructor"><a href="#2-构造函数-constructor" class="headerlink" title="2. 构造函数 constructor"></a>2. 构造函数 constructor</h2><p>类的构造函数是类的一种特殊的成员函数，<strong>它会在每次创建类的新对象时执行。</strong></p><p>构造函数的名称与类的名称是完全相同的，并且不会返回任何类型，也不会返回 void。</p><p><strong>构造函数可用于为某些成员变量设置初始值。</strong></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Line</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">      <span class="function"><span class="keyword">void</span> <span class="title">setLength</span><span class="params">( <span class="keyword">double</span> len )</span></span>;</span><br><span class="line">      <span class="function"><span class="keyword">double</span> <span class="title">getLength</span><span class="params">( <span class="keyword">void</span> )</span></span>;</span><br><span class="line">      Line();  <span class="comment">// 这是构造函数</span></span><br><span class="line"> </span><br><span class="line">   <span class="keyword">private</span>:</span><br><span class="line">      <span class="keyword">double</span> length;</span><br><span class="line">&#125;;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 成员函数定义，包括构造函数</span></span><br><span class="line">Line::Line(<span class="keyword">void</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Object is being created"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="带参数的构造函数"><a href="#带参数的构造函数" class="headerlink" title="带参数的构造函数"></a>带参数的构造函数</h3><p>默认的构造函数没有任何参数，但如果需要，构造函数也可以带有参数。这样在创建对象时就会给对象赋初始值，如下面的例子所示：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Line</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">      <span class="function"><span class="keyword">void</span> <span class="title">setLength</span><span class="params">( <span class="keyword">double</span> len )</span></span>;</span><br><span class="line">      <span class="function"><span class="keyword">double</span> <span class="title">getLength</span><span class="params">( <span class="keyword">void</span> )</span></span>;</span><br><span class="line">      Line(<span class="keyword">double</span> len);  <span class="comment">// 这是构造函数</span></span><br><span class="line"> </span><br><span class="line">   <span class="keyword">private</span>:</span><br><span class="line">      <span class="keyword">double</span> length;</span><br><span class="line">&#125;;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// 成员函数定义，包括构造函数</span></span><br><span class="line">Line::Line( <span class="keyword">double</span> len)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"Object is being created, length = "</span> &lt;&lt; len &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    length = len;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">void</span> Line::setLength( <span class="keyword">double</span> len )</span><br><span class="line">&#123;</span><br><span class="line">    length = len;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">double</span> Line::getLength( <span class="keyword">void</span> )</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> length;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 程序的主函数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">( )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="function">Line <span class="title">line</span><span class="params">(<span class="number">10.0</span>)</span></span>;</span><br><span class="line"> </span><br><span class="line">   <span class="comment">// 获取默认设置的长度</span></span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; <span class="string">"Length of line : "</span> &lt;&lt; line.getLength() &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">   <span class="comment">// 再次设置长度</span></span><br><span class="line">   line.setLength(<span class="number">6.0</span>); </span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; <span class="string">"Length of line : "</span> &lt;&lt; line.getLength() &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"> </span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="使用初始化列表来初始化字段"><a href="#使用初始化列表来初始化字段" class="headerlink" title="使用初始化列表来初始化字段"></a>使用初始化列表来初始化字段</h3><p>使用初始化列表来初始化字段：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Line::Line( <span class="keyword">double</span> len): length(len) </span><br><span class="line">&#123; </span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Object is being created, length = "</span> &lt;&lt; len &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的语法等同于如下语法：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Line::Line( <span class="keyword">double</span> len) </span><br><span class="line">&#123; </span><br><span class="line">length = len; </span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"Object is being created, length = "</span> &lt;&lt; len &lt;&lt; <span class="built_in">endl</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>假设有一个类 C，具有多个字段 X、Y、Z 等需要进行初始化，同理地，您可以使用上面的语法，只需要在不同的字段使用逗号进行分隔，如下所示：</p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">C::C<span class="comment">( double a, double b, double c)</span>: X<span class="comment">(a)</span>, Y<span class="comment">(b)</span>, Z<span class="comment">(c)</span> </span><br><span class="line">&#123; </span><br><span class="line">.... </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="3-析构函数-destructor"><a href="#3-析构函数-destructor" class="headerlink" title="3. 析构函数 destructor"></a>3. 析构函数 destructor</h2><p>类的析构函数是类的一种特殊的成员函数，它会在每次删除所创建的对象时执行。</p><p>析构函数的名称与类的名称是完全相同的，只是在前面加了个波浪号（~）作为前缀，它不会返回任何值，也不能带有任何参数。</p><p><strong>析构函数有助于在跳出程序（比如关闭文件、释放内存等）前释放资源。</strong></p><h2 id="4-友元函数"><a href="#4-友元函数" class="headerlink" title="4. 友元函数"></a>4. 友元函数</h2><p>类的友元函数是定义在类外部，但<strong>有权访问类的所有私有（private）成员和保护（protected）成员</strong>。</p><p>尽管友元函数的原型有在类的定义中出现过，但是友元函数并不是成员函数。</p><p><code>friend class ClassTwo;</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#include &lt;iostream&gt;</span></span><br><span class="line"> </span><br><span class="line">using namespace std;</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Box</span></span></span><br><span class="line">&#123;</span><br><span class="line">   double width;</span><br><span class="line">public:</span><br><span class="line">   friend void printWidth( Box box );</span><br><span class="line">   void setWidth( double wid );</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">// 成员函数定义</span><br><span class="line">void Box::setWidth( double wid )</span><br><span class="line">&#123;</span><br><span class="line">    width = wid;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 请注意：printWidth() 不是任何类的成员函数</span><br><span class="line">void printWidth( Box box )</span><br><span class="line">&#123;</span><br><span class="line">   /* 因为 printWidth() 是 Box 的友元，它可以直接访问该类的任何成员 */</span><br><span class="line">   cout &lt;&lt; <span class="string">"Width of box : "</span> &lt;&lt; box.width &lt;&lt;endl;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">// 程序的主函数</span><br><span class="line">int main( )</span><br><span class="line">&#123;</span><br><span class="line">   Box box;</span><br><span class="line"> </span><br><span class="line">   // 使用成员函数设置宽度</span><br><span class="line">   box.setWidth(<span class="number">10.0</span>);</span><br><span class="line">   </span><br><span class="line">   // 使用友元函数输出宽度</span><br><span class="line">   printWidth( box );</span><br><span class="line"> </span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>输出：  </p><p><code>Width of box : 10</code></p><h2 id="5-内联函数"><a href="#5-内联函数" class="headerlink" title="5. 内联函数"></a>5. 内联函数</h2><p>C++ 内联函数是通常与类一起使用。如果一个函数是内联的，那么在编译时，<strong>编译器会把该函数的代码副本放置在每个调用该函数的地方。</strong></p><p>对内联函数进行任何修改，都需要重新编译函数的所有客户端，因为编译器需要重新更换一次所有的代码，否则将会继续使用旧的函数。</p><p>如果想把一个函数定义为内联函数，则需要在函数名前面放置关键字 inline，在调用函数之前需要对函数进行定义。如果已定义的函数多于一行，编译器会忽略 inline 限定符。</p><p>在类定义中的定义的函数都是内联函数，即使没有使用 inline 说明符。</p><p>下面是一个实例，使用内联函数来返回两个数中的最大值：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">int</span> <span class="title">Max</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="keyword">return</span> (x &gt; y)? x : y;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 程序的主函数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">( )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"></span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; <span class="string">"Max (20,10): "</span> &lt;&lt; Max(<span class="number">20</span>,<span class="number">10</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; <span class="string">"Max (0,200): "</span> &lt;&lt; Max(<span class="number">0</span>,<span class="number">200</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">   <span class="built_in">cout</span> &lt;&lt; <span class="string">"Max (100,1010): "</span> &lt;&lt; Max(<span class="number">100</span>,<span class="number">1010</span>) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="stream"><a href="#stream" class="headerlink" title="stream"></a>stream</h1><p>在C++中，有一个stream这个类，所有的I/O都以这个“流”类为基础的，包括我们要认识的文件I/O.</p><p><img src="/img/2019-03-26-stream-1.jpg" alt></p><p><img src="/img/2019-03-26-stream-2.jpg" alt></p><p>I/O 对象无拷贝或者赋值</p><p>stream 这个类有两个重要的运算符：</p><ol><li>插入器 <code>&lt;&lt;</code></li></ol><p>　　向流输出数据。比如说系统有一个默认的标准输出流 <code>cout</code>，一般情况下就是指的显示器，所以，<code>cout&lt;&lt;&quot;Write Stdout&quot;&lt;&lt;&#39;\n&#39;</code> 就表示把字符串”Write Stdout”和换行字符(‘\n’)输出到标准输出流。</p><ol start="2"><li>析取器 <code>&gt;&gt;</code></li></ol><p>　　从流中输入数据。比如说系统有一个默认的标准输入流(cin)，一般情况下就是指的键盘，所以，cin&gt;&gt;x;就表示从标准输入流中读取一个指定类型的数据。</p><h2 id="iostream"><a href="#iostream" class="headerlink" title="iostream"></a><code>iostream</code></h2><ul><li><code>istream</code> 输入流 (<code>cin</code>)</li><li><code>ostream</code> 输出流 (<code>cout</code>)</li></ul><h2 id="fstream"><a href="#fstream" class="headerlink" title="fstream"></a><code>fstream</code></h2><p>　　在 C++ 中，对文件的操作是通过stream的子类fstream(file stream)来实现的，所以，要用这种方式操作文件，就必须加入头文件fstream.h。<br>　　<br>　　<br>ofstream是从内存到硬盘，ifstream是从硬盘到内存，其实所谓的流缓冲就是内存空间</p><h1 id="指针"><a href="#指针" class="headerlink" title="指针"></a>指针</h1><h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><p><code>int* pN = &amp;N;</code></p><p><code>pN</code> 和 <code>&amp;N</code> 是 <code>int *</code> 类型</p><p><code>*pN</code> 是 <code>int</code> 类型</p><h2 id="数组的指针"><a href="#数组的指针" class="headerlink" title="数组的指针"></a>数组的指针</h2><p><strong>数组名实际上就是数组数据所在内存区域的首地址</strong>，表示数组在内存中的起始位置。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> nArray[<span class="number">3</span>] = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;; <span class="comment">// 定义一个数组</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; nArray;</span><br></pre></td></tr></table></figure><p>输出为：<code>0x7ffeeeb00ffc</code></p><p>可以通过把首地址赋值给指针，然后对该指针进行加减运算，使指针发生偏转指向数组中的其他元素，从而遍历整个数组。例如：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> nArray[<span class="number">3</span>] = &#123; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span> &#125;;   <span class="comment">// 定义一个数组</span></span><br><span class="line"><span class="keyword">int</span>* pIndex = nArray;          <span class="comment">// 将数组的起始地址赋值给指针pIndex</span></span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;<span class="string">"指针指向的地址是："</span>&lt;&lt;pIndex&lt;&lt;<span class="built_in">endl</span>;       <span class="comment">// 输出指针指向的地址</span></span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;<span class="string">"指针所指向的数据的值是："</span>&lt;&lt;*pIndex&lt;&lt;<span class="built_in">endl</span>; <span class="comment">// 输出这个位置上的数据</span></span><br><span class="line"></span><br><span class="line">pIndex++;   <span class="comment">// 对指针进行加运算，使其指向数组中的下一个值</span></span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;<span class="string">"指针指向的地址是："</span>&lt;&lt;pIndex&lt;&lt;<span class="built_in">endl</span>;        <span class="comment">// 输出指针指向的地址</span></span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;<span class="string">"指针所指向的数据的值是："</span>&lt;&lt;*pIndex&lt;&lt;<span class="built_in">endl</span>;  <span class="comment">// 输出数据</span></span><br></pre></td></tr></table></figure><p>输出为 </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">指针指向的地址是：<span class="number">0016F</span>A38</span><br><span class="line"></span><br><span class="line">指针所指向的数据的值是：<span class="number">1</span></span><br><span class="line"></span><br><span class="line">指针指向的地址是：<span class="number">0016F</span>A3C</span><br><span class="line"></span><br><span class="line">指针所指向的数据的值是：<span class="number">2</span></span><br></pre></td></tr></table></figure><p>从输出结果中可以看到，pIndex 指针初始指向的地址是 <code>0016FA38</code>，也就是 nArray 这个数组的首地址。换句话说，也就是 pIndex 指向的是数组中的第一个数据，所以输出 <code>*pIndex</code> 的值是 1。    </p><p>而在对指针进行加 1 运算后，指针指向的地址变为 <code>0016FA3C</code>，它向地址增大的方向偏移了 <strong>4</strong> 个字节，指向了数组中的第二个数据，输出 <code>*pIndex</code>的值自然也就变成了 2。</p><p><img src="/img/2019-03-17-pointer-1.jpg" alt></p><p>除了指针的加减算术运算之外，常用到的还有指针的关系运算。指针的关系运算通常用 <code>==</code> 或 <code>!=</code> 来判断两个相同类型的指针是否相等，也就是判断它们是否指向同一地址上的同一数据，以此作为条件或循环结构中的条件判断语句。例如：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> nArray[<span class="number">3</span>] = &#123; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span> &#125;;    <span class="comment">// 定义一个数组</span></span><br><span class="line"><span class="keyword">int</span>* pIndex = nArray;           <span class="comment">// 将数组的起始地址赋值给指针pIndex</span></span><br><span class="line"><span class="keyword">int</span>* pEnd = nArray + <span class="number">3</span>;         <span class="comment">// 计算数组的结束地址并赋值给pEnd</span></span><br><span class="line"><span class="keyword">while</span>( pIndex != pEnd )         <span class="comment">// 在while的条件语句中判断两个指针是否相等，</span></span><br><span class="line">                               <span class="comment">// 也就是判断当前指针是否已经偏转到结束地址</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;*pIndex&lt;&lt;<span class="built_in">endl</span>;        <span class="comment">// 输出当前指针指向的数据</span></span><br><span class="line">    <span class="comment">// 对指针进行加1 运算，</span></span><br><span class="line">   <span class="comment">// 使其偏移到下一个内存位置，指向数组中的下一个数据</span></span><br><span class="line">    ++pIndex;                  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="gt"><a href="#gt" class="headerlink" title="-&gt;"></a>-&gt;</h2><p>c++ 中当定义类对象是指针对象时候，就需要用到 <code>-&gt;</code> 指向类中的成员；当定义一般对象时候时就需要用到 <code>.</code> 指向类中的成员。</p><p>例如：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">　　<span class="keyword">public</span></span><br><span class="line">　　play();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果定义如下：</p><p><code>A *p</code> 则使用：<code>p-&gt;play();</code> 左边是结构指针。</p><p><code>A p</code> 则使用：<code>p.paly();</code> 左边是结构变量。</p><p>总结：</p><p>箭头（<code>-&gt;</code>）：左边必须为指针；</p><p>点号（<code>.</code>）：左边必须为实体。</p><h2 id="this-指针"><a href="#this-指针" class="headerlink" title="this 指针"></a>this 指针</h2><p>在 C++ 中，每一个对象都能通过 this 指针来访问自己的<strong>地址</strong>。this 指针是所有成员函数的隐含参数。因此，在成员函数内部，它可以用来指向调用对象。</p><blockquote><p>友元函数没有 this 指针，因为友元不是类的成员。只有成员函数才有 this 指针。</p></blockquote><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Box</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">      <span class="comment">// 构造函数定义</span></span><br><span class="line">      Box(<span class="keyword">double</span> l=<span class="number">2.0</span>, <span class="keyword">double</span> b=<span class="number">2.0</span>, <span class="keyword">double</span> h=<span class="number">2.0</span>)</span><br><span class="line">      &#123;</span><br><span class="line">         <span class="built_in">cout</span> &lt;&lt;<span class="string">"Constructor called."</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">         length = l;</span><br><span class="line">         breadth = b;</span><br><span class="line">         height = h;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="function"><span class="keyword">double</span> <span class="title">Volume</span><span class="params">()</span></span></span><br><span class="line"><span class="function">      </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> length * breadth * height;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="function"><span class="keyword">int</span> <span class="title">compare</span><span class="params">(Box box)</span></span></span><br><span class="line"><span class="function">      </span>&#123;</span><br><span class="line">         <span class="keyword">return</span> <span class="keyword">this</span>-&gt;Volume() &gt; box.Volume();</span><br><span class="line">      &#125;</span><br><span class="line">   <span class="keyword">private</span>:</span><br><span class="line">      <span class="keyword">double</span> length;     <span class="comment">// Length of a box</span></span><br><span class="line">      <span class="keyword">double</span> breadth;    <span class="comment">// Breadth of a box</span></span><br><span class="line">      <span class="keyword">double</span> height;     <span class="comment">// Height of a box</span></span><br><span class="line">&#125;;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="function">Box <span class="title">Box1</span><span class="params">(<span class="number">3.3</span>, <span class="number">1.2</span>, <span class="number">1.5</span>)</span></span>;    <span class="comment">// Declare box1</span></span><br><span class="line">   <span class="function">Box <span class="title">Box2</span><span class="params">(<span class="number">8.5</span>, <span class="number">6.0</span>, <span class="number">2.0</span>)</span></span>;    <span class="comment">// Declare box2</span></span><br><span class="line"> </span><br><span class="line">   <span class="keyword">if</span>(Box1.compare(Box2))</span><br><span class="line">   &#123;</span><br><span class="line">      <span class="built_in">cout</span> &lt;&lt; <span class="string">"Box2 is smaller than Box1"</span> &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">else</span></span><br><span class="line">   &#123;</span><br><span class="line">      <span class="built_in">cout</span> &lt;&lt; <span class="string">"Box2 is equal to or larger than Box1"</span> &lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h1><p>引用变量是一个别名，也就是说，<strong>它是某个已存在变量的另一个名字</strong>。一旦把引用初始化为某个变量，就可以使用该引用名称或变量名称来指向变量。</p><blockquote><p>C++ 引用 vs 指针<br>引用很容易与指针混淆，它们之间有以下不同：  </p><ul><li>不存在空引用。引用必须连接到一块合法的内存。</li><li>一旦引用被初始化为一个对象，就不能被指向到另一个对象。</li><li>指针可以在任何时候指向到另一个对象。</li><li>引用必须在创建时被初始化。指针可以在任何时间被初始化。</li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;类&quot;&gt;&lt;a href=&quot;#类&quot; class=&quot;headerlink&quot; title=&quot;类&quot;&gt;&lt;/a&gt;类&lt;/h1&gt;&lt;h2 id=&quot;1-类成员函数&quot;&gt;&lt;a href=&quot;#1-类成员函数&quot; class=&quot;headerlink&quot; title=&quot;1. 类成员函数&quot;&gt;&lt;/a&gt;1
      
    
    </summary>
    
      <category term="编程语言" scheme="https://hhyz.me/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="c++" scheme="https://hhyz.me/tags/c/"/>
    
  </entry>
  
  <entry>
    <title>make/cmake/qmake</title>
    <link href="https://hhyz.me/2019/01/22/make/"/>
    <id>https://hhyz.me/2019/01/22/make/</id>
    <published>2019-01-21T16:00:00.000Z</published>
    <updated>2019-07-02T17:05:20.718Z</updated>
    
    <content type="html"><![CDATA[<h1 id="make-cmake-qmake"><a href="#make-cmake-qmake" class="headerlink" title="make/cmake/qmake"></a>make/cmake/qmake</h1><blockquote><p>Makefile是类unix环境下(比如Linux)的类似于批处理的”脚本”文件。</p></blockquote><p>gcc是GNU Compiler Collection（就是GNU编译器套件），也可以简单认为是编译器，它可以编译很多种编程语言（括C、C++、Objective-C、Fortran、Java等等</p><ol><li>make 是用来执行Makefile的  </li><li>Makefile是类unix环境下(比如Linux)的类似于批处理的”脚本”文件。其基本语法是: <strong>目标+依赖+命令</strong>，只有在<strong>目标</strong>文件不存在，或<strong>目标</strong>比<strong>依赖</strong>的文件更旧，<strong>命令</strong>才会被执行。由此可见，Makefile和make可适用于任意工作，不限于编程。比如，可以用来管理latex。  </li><li>Makefile+make可理解为类unix环境下的项目管理工具，但它太基础了，抽象程度不高，而且在windows下不太友好(针对visual studio用户)，于是就有了跨平台项目管理工具cmake  </li><li>cmake是跨平台项目管理工具，它用更抽象的语法来组织项目。虽然，仍然是目标，依赖之类的东西，但更为抽象和友好，比如你可用math表示数学库，而不需要再具体指定到底是math.dll还是libmath.so，在windows下它会支持生成visual studio的工程，在linux下它会生成Makefile，甚至它还能生成eclipse工程文件。也就是说，从同一个抽象规则出发，它为各个编译器定制工程文件。  </li><li>cmake是抽象层次更高的项目管理工具，cmake命令执行的CMakeLists.txt文件  </li><li>qmake是Qt专用的项目管理工具，对应的工程文件是<em>.pro，在Linux下面它也会生成Makefile，当然，在命令行下才会需要手动执行qmake，完全可以在qtcreator这个专用的IDE下面打开</em>.pro文件，使用qmake命令的繁琐细节不用你管了。</li></ol><p>总结一下，make用来执行Makefile，cmake用来执行CMakeLists.txt，qmake用来处理*.pro工程文件。Makefile的抽象层次最低，cmake和qmake在Linux等环境下最后还是会生成一个Makefile。cmake和qmake支持跨平台，cmake的做法是生成指定编译器的工程文件，而qmake完全自成体系。</p><p>具体使用时，Linux下，小工程可手动写Makefile，大工程用automake来帮你生成Makefile，要想跨平台，就用cmake。如果GUI用了Qt，也可以用qmake+*.pro来管理工程，这也是跨平台的。当然，cmake中也有针对Qt的一些规则，并代替qmake帮你将qt相关的命令整理好了。</p><p>另外，需要指出的是，make和cmake主要命令只有一条，make用于处理Makefile，cmake用来转译CMakeLists.txt，而qmake是一个体系，用于支撑一个编程环境，它还包含除qmake之外的其它多条命令(比如uic，rcc,moc)。</p><p>上个简图，其中cl表示visual studio的编译器，gcc表示linux下的编译器</p><div align="center">![](/img/2019-01-22-make-1.jpg)<center><small><font color="gray">    </font></small></center></div><ol><li><p>gcc是GNU Compiler Collection（就是GNU编译器套件），也可以简单认为是编译器，它可以编译很多种编程语言（括C、C++、Objective-C、Fortran、Java等等）。</p></li><li><p>当你的程序只有一个源文件时，直接就可以用gcc命令编译它。</p></li><li><p>但是当你的程序包含很多个源文件时，用gcc命令逐个去编译时，你就很容易混乱而且工作量大</p></li><li><p>所以出现了make工具<br>make工具可以看成是一个智能的批处理工具，它本身并没有编译和链接的功能，而是用类似于批处理的方式—通过调用makefile文件中用户指定的命令来进行编译和链接的。</p></li><li><p>makefile是什么？简单的说就像一首歌的乐谱，make工具就像指挥家，指挥家根据乐谱指挥整个乐团怎么样演奏，make工具就根据makefile中的命令进行编译和链接的。</p></li><li><p>makefile命令中就包含了调用gcc（也可以是别的编译器）去编译某个源文件的命令。</p></li><li><p>makefile在一些简单的工程完全可以人工手下，但是当工程非常大的时候，手写makefile也是非常麻烦的，如果换了个平台makefile又要重新修改。</p></li><li><p>这时候就出现了Cmake这个工具，cmake就可以更加简单的生成makefile文件给上面那个make用。当然cmake还有其他功能，就是可以跨平台生成对应平台能用的makefile，你不用再自己去修改了。</p></li><li><p>可是cmake根据什么生成makefile呢？它又要根据一个叫CMakeLists.txt文件（学名：组态档）去生成makefile。</p></li><li><p>到最后CMakeLists.txt文件谁写啊？亲，是你自己手写的。</p></li><li><p>当然如果你用IDE，类似VS这些一般它都能帮你弄好了，你只需要按一下那个三角形</p></li><li><p>接着是qmake，qmake是什么，先说一下Qt这个东西。Qt是跨平台C++图形用户界面应用程序开发框架。它既可以开发GUI程序，也可用于开发非GUI程序，比如控制台工具和服务器。简单的说就是C++的第三方库，使用这个库你可以很容易生成windows，Linux，MAC os等等平台的图形界面。现在的Qt还包含了开发各种软件一般需要用到的功能模块（网络，数据库，XML，多线程啊等等），比你直接用C++（只带标准内裤那种）要方便和简单。</p></li><li><p>你可以用Qt简简单单就实现非常复杂的功能，是因为Qt对C++进行了扩展，你写一行代码，Qt在背后帮你写了几百上千行，而这些多出来的代码就是靠Qt专有的moc编译器（The Meta-Object Compiler）和uic编译器（User Interface Complier）来重新翻译你那一行代码。问题来了，你在进行程序编译前就必须先调用moc和uic对Qt源文件进行预处理，然后再调用编译器进行编译。上面说的那种普通makefile文件是不适用的，它没办法对qt源文件进行预处理。所以qmake就产生了。</p></li><li><p>qmake工具就是Qt公司制造出来，用来生成Qt 专用makefile文件，这种makefile文件就能自动智能调用moc和uic对源程序进行预处理和编译。qmake当然必须也是跨平台的，跟cmake一样能对应各种平台生成对应makefile文件。</p></li><li><p>qmake是根据Qt 工程文件（.pro）来生成对应的makefile的。工程文件（.pro）相对来说比较简单，一般工程你都可以自己手写，但是一般都是由Qt的开发环境 Qt Creator自动生成的，你还是只需要按下那个邪恶三角形就完事了。</p></li><li><p>还没有完，由于qmake很简单很好用又支持跨平台，而且是可以独立于它的IDE，所以你也可以用在非Qt工程上面，照样可以生成普通的makefile，只要在pro文件中加入CONFIG -= qt 就可以了。</p></li><li><p>这样qmake和cmake有什么区别？<br>不好意思，cmake也是同样支持Qt程序的，cmake也能生成针对qt 程序的那种特殊makefile，<br>只是cmake的CMakeLists.txt 写起来相对与qmake的pro文件复杂点。<br>qmake 是为 Qt 量身打造的，使用起来非常方便，但是cmake功能比qmake强大。<br>一般的Qt工程你就直接使用qmake就可以了，cmake的强大功能一般人是用不到的。<br>当你的工程非常大的时候，又有qt部分的子工程，又有其他语言的部分子工程，据说用cmake会 方便，我也没试过。</p></li></ol><blockquote><p><a href="https://www.zhihu.com/question/27455963" target="_blank" rel="noopener">make makefile cmake qmake都是什么，有什么区别？</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;make-cmake-qmake&quot;&gt;&lt;a href=&quot;#make-cmake-qmake&quot; class=&quot;headerlink&quot; title=&quot;make/cmake/qmake&quot;&gt;&lt;/a&gt;make/cmake/qmake&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;M
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>GAN</title>
    <link href="https://hhyz.me/2018/09/19/gan/"/>
    <id>https://hhyz.me/2018/09/19/gan/</id>
    <published>2018-09-18T16:00:00.000Z</published>
    <updated>2019-07-13T08:50:33.366Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><blockquote><p>论文地址：<a href="https://arxiv.org/pdf/1406.2661v1.pdf" target="_blank" rel="noopener">Generative Adversarial Nets</a></p></blockquote><p>GAN是Ian Goodfellow提出的使用对抗过程来获得生成模型的新框架。</p><p>生成对抗网络主要由两个部分组成，</p><ul><li>生成器 \(G\) (Generator)</li><li>判别器 \(D\) (discriminator)</li></ul><p>生成器G的作用：<br>尽量去拟合 (cover) 真实数据分布，<strong>生成以假乱真的图片</strong>。它的输入参数是一个随机噪声 \(z，G(z)\) 代表其生成的一个样本 (fake data)。</p><p>判别器 \(D\) 的作用：<br>判断一张图片是否是“真实的”，<strong>即能判断出一张图片是真实数据(training data)还是生成器 \(G\) 生成的样本 (fake data)</strong>。它的输入参数是 \(x，x\) 代表一张图片， \(D(x)\) 代表 \(x\) 是真实图片的概率。</p><p><strong>具体过程：</strong></p><ol><li><p>对于从训练数据中取样出的真实图片x，判别器D希望D(x)的输出值接近1，即判定训练数据为真实图片。</p></li><li><p>给定一个随机噪声z，判别器D希望 \(D(G(z))\) 的输出值接近 \(0\) ，即认定生成器G生成的图片是假的；而生成器G希望 \(D(G(z))\) 的输出值接近1，即G希望能够欺骗D，让D将生成器G生成的样本误判为真实图片。这样 G 和 D 就构成了博弈的状态。</p></li><li><p>在博弈的过程中，生成器G和判别器D都不断的提升自己的能力，最后达到一个平衡的状态。G可以生成足以“以假乱真”的图片G(z)。对于D来说，它难以判定G生成的图片究竟是不是真实的，因此 \(D(G(z)) = 0.5\) 。这样我们的目的就达成了：我们得到了一个生成式的模型G，它可以用来生成真实图片。</p></li></ol><blockquote><p>一个很有意思例子：生成器G可以被比作假币制造者团队，试图生产出无法检测出真伪的假币；判别器D可以被比作警察，试图区分出真币和假币。在比拼竞争的过程中，双方都不断提升自己的方法，最终导致假币与真品无法区分。说明我们得到了一个效果非常好的生成器G</p></blockquote><p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fvf1vsgjyrj30mt09uaaa.jpg" alt><br><img src="https://ws4.sinaimg.cn/large/006tNbRwgy1fvf33sz4l9j30hs0by74y.jpg" alt></p><center><small><font color="gray">  这里的G网络的输入是一个符合简单分布如高斯分布或者均匀分布的随机噪声  </font></small></center><p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fvf4nrc9mfj31980g6wh5.jpg" alt></p><center><small><font color="gray">  黑色的线表示数据x的实际分布，绿色的线表示数据的生成分布，蓝色的线表示生成的数据对应在判别器中的分布效果  </font></small></center><ul><li>对于图a，D还刚开始训练，本身分类的能力还很有限，有波动，但是初步区分实际数据和生成数据还是可以的。     </li><li>图b，D训练得比较好了，可以很明显的区分出生成数据。     </li><li>图c：绿色的线与黑色的线的偏移，蓝色的线下降了，也就是生成数据的概率下降了。那么，由于绿色的线的目标是提升概率，因此就会往蓝色线高的方向移动。那么随着训练的持续，由于G网络的提升，G也反过来影响D的分布。假设固定G网络不动，训练D，那么训练到最优， \(D^*_g(x) = p_{data}(x)/(p_{data}(x)+p_{g}(x))\) 。     </li><li>因此，随着 \(p_g(x)\) 趋近于 \(p_{data}(x),D^*_g(x)\) 会趋近于 \(0.5\) ，也就是到图d。而我们的目标就是希望绿色的线能够趋近于黑色的线，也就是让生成的数据分布与实际分布相同。图d符合我们最终想要的训练结果。到这里，G网络和D网络就处于纳什均衡状态，无法再进一步更新了。</li></ul><p><strong>GAN 模型的目标函数：</strong></p><p>$$\min_{G} \max_D V(G,D)=E_{x\sim P_{data}}[logD(x)] + E_{x\sim P_G}[log(1-D(x))]$$</p><p>或者我们可以直接理解：<br>G网络的loss是 \(log(1-D(G(z))\) ，<br>而D的loss是 \(-(log(D(x)) + log(1-D(G(z)))\) </p><h3 id="怎么训练"><a href="#怎么训练" class="headerlink" title="怎么训练"></a>怎么训练</h3><h4 id="交替训练。"><a href="#交替训练。" class="headerlink" title="交替训练。"></a>交替训练。</h4><ol><li>固定G时，训练D；<br>$$\max {E_{x\sim p(r)} log(D(x)) + E_{x\sim p(g)} log(1-D(x)) } $$（公式1）<br>转化成最小形式：<br>$$\min -[{E_{x\sim p(r)} log(D(x)) + E_{x\sim p(g)} log(1-D(x)) } ]$$       </li><li>固定D时，训练G。<br>可以设置超参数k, 表示训练k次D，再训练一次G.<br>$$min Loss_G =E_{x\sim P_G}[log(1-D(x))] $$（公式2）</li></ol><p><img src="https://ws1.sinaimg.cn/large/006tNbRwgy1fvf3xw32xtj31f20xuah2.jpg" alt></p><p>全局最优  \( p_g=p_{data}\)</p> <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/pdf/1406.2661v1.pdf&quot; targ
      
    
    </summary>
    
      <category term="深度学习" scheme="https://hhyz.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="GAN" scheme="https://hhyz.me/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>强化学习（二）</title>
    <link href="https://hhyz.me/2018/08/13/RL2/"/>
    <id>https://hhyz.me/2018/08/13/RL2/</id>
    <published>2018-08-12T16:00:00.000Z</published>
    <updated>2019-07-04T19:24:30.084Z</updated>
    
    <content type="html"><![CDATA[<br><div align="center">![](/img/2018-08-13-RL2-1.jpg)<center><small><font color="gray">    </font></small></center></div><p>强化学习（RL，基于MDP）的求解policy的方式一般分为三种：</p><ul><li>Value &lt;—critic</li><li>Policy &lt;—actor</li><li>Value + Policy &lt;— Actor-critic</li></ul><h2 id="策略梯度"><a href="#策略梯度" class="headerlink" title="策略梯度"></a>策略梯度</h2><p>强化学习是一个通过奖惩来学习正确行为的机制. 家族中有很多种不一样的成员, 有学习奖惩值, 根据自己认为的高价值选行为, 比如 Q learning, Deep Q Network, 也有不通过分析奖励值, 直接输出行为的方法, 这就是今天要说的 Policy Gradients 了. 甚至我们可以为 Policy Gradients 加上一个神经网络来输出预测的动作. 对比起以值为基础的方法, Policy Gradients 直接输出动作的最大好处就是, 它能在一个连续区间内挑选动作, 而基于值的, 比如 Q-learning, 它如果在无穷多的动作中计算价值, 从而选择行为, 这, 它可吃不消.</p><p>有了神经网络当然方便, 但是, 我们怎么进行神经网络的误差反向传递呢? Policy Gradients 的误差又是什么呢? 答案是! 哈哈, 没有误差! 但是他的确是在进行某一种的反向传递. 这种反向传递的目的是让这次被选中的行为更有可能在下次发生. 但是我们要怎么确定这个行为是不是应当被增加被选的概率呢? 这时候我们的老朋友, reward 奖惩正可以在这时候派上用场,</p><p><img src="/img/2018-08-13-RL2-2.jpg" alt></p><div align="center"><center><small><font color="gray">    </font></small></center></div><p>现在我们来演示一遍, 观测的信息通过神经网络分析, 选出了左边的行为, 我们直接进行反向传递, 使之下次被选的可能性增加, 但是奖惩信息却告诉我们, 这次的行为是不好的, 那我们的动作可能性增加的幅度 随之被减低. 这样就能靠奖励来左右我们的神经网络反向传递. 我们再来举个例子, 假如这次的观测信息让神经网络选择了右边的行为, 右边的行为随之想要进行反向传递, 使右边的行为下次被多选一点, 这时, 奖惩信息也来了, 告诉我们这是好行为, 那我们就在这次反向传递的时候加大力度, 让它下次被多选的幅度更猛烈! 这就是 Policy Gradients 的核心思想了. 很简单吧.</p><p>Policy gradient 是 RL 中另外一个大家族, 他不像 Value-based 方法 (Q learning, Sarsa), 但他也要接受环境信息 (observation), 不同的是他要输出不是 action 的 value, 而是具体的那一个 action, 这样 policy gradient 就跳过了 value 这个阶段. 而且个人认为 Policy gradient 最大的一个优势是: 输出的这个 action 可以是一个连续的值, 之前我们说到的 value-based 方法输出的都是不连续的值, 然后再选择值最大的 action. 而 policy gradient 可以在一个连续分布上选取 action.</p><p>介绍的 policy gradient 的第一个算法是一种基于 整条回合数据 的更新, 也叫 REINFORCE 方法. 这种方法是 policy gradient 的最基本方法, 有了这个的基础, 我们再来做更高级的.</p><p><img src="/img/2018-08-13-RL2-3.jpg" alt></p><div align="center"><center><small><font color="gray">    </font></small></center></div><p><code>log(Policy(s,a))*V</code> 中的 <code>log(Policy(s,a))</code> 表示在 状态 <code>s</code> 对所选动作 <code>a</code> 的吃惊度, 如果 <code>Policy(s,a)</code> 概率越小, 反向的 <code>log(Policy(s,a))</code> (即 <code>-log(P)</code>) 反而越大. 如果在 <code>Policy(s,a)</code> 很小的情况下, 拿到了一个 大的 <code>R</code>, 也就是 大的 <code>V</code>, 那 <code>-log(Policy(s, a))*V</code> 就更大, 表示更吃惊, (我选了一个不常选的动作, 却发现原来它能得到了一个好的 reward, 那我就得对我这次的参数进行一个大幅修改). 这就是 <code>log(Policy)*V</code> 的物理意义啦</p><h3 id="策略梯度-1"><a href="#策略梯度-1" class="headerlink" title="策略梯度"></a>策略梯度</h3><h2 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor Critic"></a>Actor Critic</h2><p>我们有了像 Q-learning 这么伟大的算法, 为什么还要瞎折腾出一个 Actor-Critic? 原来 Actor-Critic 的 Actor 的前生是 Policy Gradients, 这能让它毫不费力地在连续动作中选取合适的动作, 而 Q-learning 做这件事会瘫痪. 那为什么不直接用 Policy Gradients 呢? 原来 Actor Critic 中的 Critic 的前生是 Q-learning 或者其他的 以值为基础的学习法 , 能进行单步更新, 而传统的 Policy Gradients 则是回合更新, 这降低了学习效率.</p><p>结合了 Policy Gradient (Actor) 和 Function Approximation (Critic) 的方法. Actor 基于概率选行为, Critic 基于 Actor 的行为评判行为的得分, Actor 根据 Critic 的评分修改选行为的概率.</p><p><img src="/img/2018-08-13-RL2-101.jpg" alt></p><div align="center"><center><small><font color="gray">    </font></small></center></div><p>或者说详细点, 就是 Actor 在运用 Policy Gradient 的方法进行 Gradient ascent 的时候, 由 Critic 来告诉他, 这次的 Gradient ascent 是不是一次正确的 ascent, 如果这次的得分不好, 那么就不要 ascent 那么多.</p><h2 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG"></a>DDPG</h2><p><img src="/img/2018-08-13-RL2-102.jpg" alt></p><div align="center"><center><small><font color="gray">    </font></small></center></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;br&gt;
&lt;div align=&quot;center&quot;&gt;
![](/img/2018-08-13-RL2-1.jpg)
&lt;center&gt;&lt;small&gt;&lt;font color=&quot;gray&quot;&gt;    &lt;/font&gt;&lt;/small&gt;&lt;/center&gt;
&lt;/div&gt;


&lt;p&gt;强化学习（RL，
      
    
    </summary>
    
      <category term="强化学习" scheme="https://hhyz.me/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://hhyz.me/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>AlphaGO Zero 原理</title>
    <link href="https://hhyz.me/2018/08/08/AlphaGO-Zero/"/>
    <id>https://hhyz.me/2018/08/08/AlphaGO-Zero/</id>
    <published>2018-08-07T16:00:00.000Z</published>
    <updated>2019-07-02T17:05:20.712Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h2><p>简单来说，AlphaGo Zero 的训练可以分为三个同时进行的阶段：</p><ul><li>自我对战</li><li>再训练网络</li><li>评估网络</li></ul><p>在自我对战阶段， AlphaGo Zero 创建一个训练集合，自我完成对战 25000 次。<strong>棋局每变动一次，博弈、搜索可能性和胜出者的信息将被存储。</strong></p><p><strong>训练网络阶段，是神经网络权值得到优化的过程。</strong>在一次完整的训练循环中， AlphaGo Zero 将从 50 万局博弈中选取 2048 个移动位置作为样品，并对这些位置的神经网络进行训练。之后，通过损失函数，来对比神经网络预测与搜索可能性和实际胜出方的信息。每完成一千次这样的训练循环，就对神经网络进行一次评估。</p><p><strong>在评估网络阶段，测试新的神经网络是否得到优化。</strong>在这个过程中，博弈双方都通过各自的神经网络评估叶节点，并使用蒙特卡洛树搜索进行下一步棋路的选择。</p><div align="center">![](/img/2018-08-08-AlphaGO-Zero-1.jpg)<center><small><font color="gray">  AlphaGo Zero结构图  </font></small></center></div><br><div align="center">![](/img/2018-08-08-AlphaGO-Zero-2.jpg)<center><small><font color="gray">    </font></small></center></div><p><strong>a 部分是利用初始化的神经网络和MCTS进行自博弈，收集到对弈的数据以及胜负关系</strong></p><p>程序自我对弈完成一个棋局产生一个状态序列  \(s_1,…,s_T\)  ，在  \(T\)  时刻棋局结束，产生了获胜方，用  \(z\)  表示。在其中的每一个时刻  \(T\)  ，棋局状态用 \( s_t\)  表示，会在神经网络 \( f_{\theta} \) 的引导下执行一次 MCTS 搜索  \({\alpha}_{\theta}\)  ，通过 MCTS 搜索计算得到的概率  \(a_t \sim {\pi}_t\)  确定如何行为（在何处落子）</p><p><strong>b 部分是利用收集到的数据训练当前棋盘的价值和每一个走子的概率 (神经网络的训练过程)</strong>      </p><p>神经网络的输入是某时刻  \(t\)  的棋局状态  \(s_t\)  外加一些历史和额外信息，输出是一个行为概率向量 \( p_t\)  和一个标量  \(v_t\) </p><p>Alpha Zero 算法主体思想就是在策略迭代过程中重复使用上述两个工具：神经网络的参数得以更新，这样可以使得神经网络的输出  \((p, v) = f_{\theta}(s) \) ：移动概率和获胜奖励更接近与经过改善了的搜索得到的概率以及通过自我对弈得到的棋局结果，后者用 \( (\pi, z) \) 表示。得到的新参数可以在下一次自我对弈的迭代过程中让搜索变得更加强大。</p><ul><li>\(p\) (move probabilities) 在当前棋局状态下采取每种可能落子方式的概率</li><li>\(v\) 当前棋局状态 \(s\) 下棋手最终获胜还是落败（分别用 \(1\) 和 \(-1\) 表示）</li><li>\(\pi\) 表示经过神经网络改善了的蒙特卡洛树搜索输出的选择每一个 move 的概率</li><li>\(z\) 表示通过自我对弈得到的棋局结果</li></ul><blockquote><p>The vector of move probabilities  \(p\)  represents the probability of selecting each move a (including pass),  \(p_a = Pr(a|s)\) . The value  \(v\)  is a scalar evaluation, estimating the probability of the current player winning from position  \(s\) . </p><p>The MCTS search outputs probabilities π of playing each move. These search probabilities usually select much stronger moves than the raw move probabilities p of the neural network  \(f_θ(s)\) ; MCTS may therefore be viewed as a powerful policy improvement operator. Self­play with search—using the improved MCTS­based policy to select each move, then using the game winner  \(z\)  as a sample of the value—may be viewed as a powerful policy evaluation operator. </p></blockquote><br>## 2. 传统蒙特卡洛树搜索 MCTS<h3 id="2-1-树搜索"><a href="#2-1-树搜索" class="headerlink" title="2.1 树搜索"></a>2.1 树搜索</h3><p>围棋第一手有 \(361\) 种下法，第二手有 \(360\) 种，第三手有 \(359\) ，依次类推，即一共有  \(361!\)  种下法。这个一个天文数字，比目前可观测宇宙的所有原子数还要多。要进行完全树搜索，是不可能的。因此我们必须进行剪枝，并限制思考的深度。</p><div align="center">![](/img/2018-08-08-AlphaGO-Zero-3.jpg)<center><small><font color="gray">    </font></small></center></div><br>所谓**剪枝**，就是指没必要考虑每种下法，我们**只需考虑最有价值的几手下法**。所谓**限制思考的深度**，就是我们最多只思考5步，10步，20步。常见的算法是Alpha-beta剪枝算法。但是，剪枝算法也有它的缺陷，它很有可能**过早的剪掉了后期价值很大走法**。<br>### 2.2 蒙特卡洛方法<p>简而言之，蒙特卡洛方法(Monte Carlo method)，是一种“统计模拟方法”。</p><p>假设我们要计算一个不规则形状的面积，我们只需在包含这个不规则形状的矩形内，随机的掷出一个点，每掷出一个点，则 \(N+1\) ，如果这个点在不规则图形内则 \(W+1\) 。落入不规则图形的概率即为 \( W/N\) 。当掷出足够多的点之后，我们可以认为： \(不规则图形面积＝矩形面积＊W/N\) 。</p><p>要应用蒙特卡洛算法的问题，<strong>首先要将问题转化为概率问题</strong>，然后通过统计方法将其问题的解估计出来。</p><br>### 2.3 蒙特卡洛树搜索 MCTS<p>这种算法简而言之是用蒙特卡洛方法估算每一种走法的胜率。如果描述的再具体一些，通过不断的模拟每一种走法，直至终局，该走法的模拟总次数 \(N\) ，与胜局次数 \(W\) ，即可推算出该走法的胜率为  \(W/N\) 。</p><div align="center">![](/img/2018-08-08-AlphaGO-Zero-4.jpg)<center><small><font color="gray">    </font></small></center></div><p>该算法的每个循环包含4个步骤：</p><ul><li><strong>选择：</strong> 从根节点往下走，每次都选一个“最值得看的子节点”（具体规则稍后说），直到来到一个“存在未扩展的子节点”的节点，如图中的 \( 3/3 \) 节点。什么叫做“存在未扩展的子节点”，其实就是指这个局面存在未走过的后续着法。</li><li><strong>扩展：</strong> 我们给这个节点加上一个 \( 0/0 \) 子节点，对应之前所说的“未扩展的子节点”，就是还没有试过的一个着法。</li><li><strong>仿真：</strong> 从上面这个没有试过的着法开始，用快速走子策略（Rollout policy）走到底，得到一个胜负结果。按照普遍的观点，快速走子策略适合选择一个棋力很弱但走子很快的策略。因为如果这个策略走得慢（比如用 AlphaGo 的策略网络走棋），虽然棋力会更强，结果会更准确，但由于耗时多了，在单位时间内的模拟次数就少了，所以不一定会棋力更强，有可能会更弱。这也是为什么我们一般只模拟一次，因为如果模拟多次，虽然更准确，但更慢。</li><li><strong>回溯：</strong> 把模拟的结果加到它的所有父节点上。例如第三步模拟的结果是  \(0/1\) （代表黑棋失败），那么就把这个节点的所有父节点加上  \(0/1\) 。</li></ul><br>### 2.4 上限置信区间算法 UCT<p>怎么选择节点？和从前一样：如果轮到黑棋走，就选对于黑棋有利的；如果轮到白棋走，就选对于黑棋最不利的。但不能太贪心，不能每次都只选择“最有利的/最不利的”，因为这会意味着搜索树的广度不够，容易忽略实际更好的选择。</p><p>为了在最大胜率和新节点探索上保持平衡，<strong>UCT（Upper Confidence Bound，上限置信区间算法）</strong>被引入。所谓置信区间，就是概率计算结果的可信度。打个比方，如果掷了3次硬币，都是正面朝上，我们就认为掷硬币正面朝上概率是100%，那肯定是错误的，因为我们的样本太少了。所以UCT就是用来修正这个样本太少的问题。</p><p><strong>具体公式如下：</strong></p><p>$$\text{score = }\ \frac{w_i}{n_i}+c\sqrt{\frac{\ln N_i}{n_i}}$$</p><ul><li>\(w_i\)  是 \(i\) 节点的胜利次数</li><li>\(n_i\) 是i节点的模拟次数</li><li>\(N_i\) 是所有模拟次数 </li><li>\(c\) 是探索常数，理论值为 \(\sqrt{2}\) ，可根据经验调整，\(c\) 越大就越偏向于广度搜索，\(c\) 越小就越偏向于深度搜索</li></ul><p>我们看例子说明这是什么意思，就看之前的图吧。</p><div align="center">![](/img/2018-08-08-AlphaGO-Zero-5.jpg)<center><small><font color="gray">    </font></small></center></div><p>假设根节点是轮到黑棋走。那么我们首先需要在  \(7/10、5/8、0/3\)  之间选择 (即第二排)：</p><ul><li>其中 \( 7/10 \) 对应的分数为  \(7/10 + C \cdot \sqrt{\frac{\log(21)}{10}}  \approx 0.7 + 0.55 C\) 。</li><li>其中 \( 5/8 \) 对应的分数为  \(5/8 + C \cdot \sqrt{\frac{\log(21)}{8}}  \approx 0.625 + 0.62 C\) 。</li><li>其中 \( 0/3 \) 对应的分数为  \(0/3 + C \cdot \sqrt{\frac{\log(21)}{3}}  \approx 0 + 1.00 C\) 。</li><li>可以注意到， \(C\) 越大，就会越照顾访问次数相对较少的子节点。</li></ul><p>如果  \(C\)  比较小，我们将会选择  \(7/10\) ，接着就要在  \(2/4\)  和 \( 5/6 \) 间选择。注意，由于现在是白棋走，需要把胜率估计倒过来：</p><ul><li>其中  \(2/4\)  对应的分数为  \((1-2/4) + C \cdot \sqrt{\frac{\log(10)}{4}}  \approx 0.5 + 0.76 C\) 。</li><li>其中  \(5/6\)  对应的分数为  \((1-5/6) + C \cdot \sqrt{\frac{\log(10)}{6}}  \approx 0.17 + 0.62 C\) 。</li></ul><p>那么我们下一步肯定应该选 2/4。所以说这张图是错误的，因为制图的人并没有注意到要把胜率倒过来。</p><br>## 3. 深度强化学习<br>### 3.1 神经网络与MCTS的结合<p>常见的MCTS分为4个步骤：选择，扩展，模拟和反向传播。</p><p>神经网络用来指导MCTS进行判断，主要目的是用神经网络的输出代替四个步骤中的扩展和模拟这两步。</p><p>神经网络的<strong>输出是落子概率和局面评估</strong>。从根节点开始，选择到叶节点，然后判断是否代表这结束，如果没有结束，则根据神经网络输出的评分进行更新，同时根据神经网络给出的落子策略进行扩展。如果结束，则根据玩家的胜负进行更新。</p><p><strong>但是对于传统的MCTS，我们没有好的策略，所以只能大规模的搜索。</strong>在到达叶节点所代表的局面的时候，我们需要使用<strong>随机策略进行多次模拟，一直模拟到对局结束</strong>才能得到局面的评估。这需要消耗大量的计算资源和时间。所以引入神经网络来代替模拟步骤。</p><p>所以总的来说，落子的选择整体是根据MCTS来的。神经网络的作用是帮助缩短MCTS所需要的时间。</p><blockquote><p>没有 MCTS 相当于职业棋手只凭棋感不做计算走快棋。<strong>神经网络提供几个候选的走法，MCTS 再算一算到底哪个点更好。</strong></p></blockquote><br>### 3.2 神经网络架构<p>由残差模块构成的 CNN，输入为 \(19\times 19\times 17\)</p><p>17 是 17 个特征，使用了既往 8 个回合的 16 个特征以及一个当前玩家信息特征  \((8 \times 2 + 1 = 17)\) : </p><p>$$s_t = [X_t , Y_t , X_{t−1}, Y_{t−1}, …, X_{t−7}, Y_{t−7} , C]$$</p><p><strong>其中  \(X_t\)  内包含的是当前棋手的数据：</strong></p><p>加入当前棋手执黑棋，那么此时棋盘上所有黑棋对应的位置取值1，白棋和空白位置取值0。类似的 \( Y_t \) 反映的就是白棋信息，当前棋盘上所有白棋的位置取值1，黑棋和空白处取值0。</p><p><strong>\(C\) 内的所有 \(（19\times 19）\) 个数据要么都是1，要么都是0，</strong>如果此时是黑棋要落子则取1，白棋落子则取0。</p><p>网络的共同部分多数是用 \(3\times 3\) 的卷积核（stride = 1)，256个特征数，后接 BatchNormalization 和 Relu 单元。每一个残差单元包括 (参见下图)：</p><ul><li><strong>策略端</strong>：输出特征数为 \(19\times 19+1\)，分别代表在棋盘上所有可能位置落子的可能性以及 Pass 的可能性。</li><li><strong>价值端</strong>：全连接一个256个特征的隐藏层，最后以tanh的激活方式输出 \([-1,1]\) 之间的值。 </li></ul><blockquote><p>网络的前20层左右，是常见的神经网络结构。然后跟着是两个“头”，一个头取走了前20层的输出，然后产生了下一步的落子概率，另一个头基于同样的数据，输出当前局面的获胜概率。</p></blockquote><br><div align="center">![](/img/2018-08-08-AlphaGO-Zero-101.jpg)<center><small><font color="gray">    </font></small></center></div><div><br></div><p><strong>训练数据：</strong>自我对弈产生大量的  \(( s,\pi,z )\) 数据对，通过 Mini-batch 采样。</p><p><strong>损失函数：</strong></p><p>$$ l=(z-v)^{2}-\pi^{T}log(p)+c||\theta||^{2} $$</p><ul><li>第一项：通过最小二乘最小化获胜概率误差      </li><li>第二项：通过交叉熵最大化先验走子概率与提升后走子概率一致性     </li><li>第三项：L2范数权值衰减防止过拟合。</li></ul><div> </div><br>### 3.3 过程细节<p>为了在 self play 每一步得到，MCTS 需要完成1600次搜索。搜索树中每一节点 \(s\) 针对合法操作保存以下数据结构 </p><p>$$\{N(s,a),W(s,a),Q(s,a),P(s,a)  \}$$</p><ul><li>\(s\ \)  树的每一个节点代表了一种棋盘布局</li><li>\(a\ \)  每一个边代表了在一种布局 \(s\) 下的一种落子方式</li><li>\(N(s,a)\ \) 记录边的访问次数  </li><li>\(W(s,a)\ \)  合计行动价值  </li><li>\(Q(s,a)\ \) 平均行动价值 </li><li>\(P(s,a)\ \)  选择该条边的先验概率</li></ul><p>多次模拟过程会在独立线程并行运行。搜索算法在 \(a，b，c\) 三步迭代多次后，根据搜索结果选择落子 \(d\) 。</p><br><div align="center">![](/img/2018-08-08-AlphaGO-Zero-6.jpg)<center><small><font color="gray">    </font></small></center></div><ul><li><strong>a</strong> 每次模拟选择的分支，有最大 \(Q+U\) , 其中 \(Q\) 是动作价值， \(U\) 是上限置信， \(U\) 依赖于一个存储在分支上的优先概率 \(P\) 和该分支的访问次数 \(N\) （每访问一次 \(N+1\)）</li><li><strong>b</strong> 扩展叶节点，神经网络 \((P(s, .), V(s)) = f_θ(s)\) 评估 \(s\) ; 将向量 \(P\) 的值被存储在 \(s\) 的扩展边上</li><li><strong>c</strong> 根据 \(V\) 更新动作价值（action-value) \(Q\)，反映所有该动作的子树的平均值</li><li><strong>d</strong> 一旦搜索结束，搜索概率 \(\pi\) 被返回，与  \(Ν^{(1/τ)}\)  成正比， \(N\) 是每个分支的访问次数，而 \(τ\) 是一个参数控制着温度（temperature）</li></ul><p>这里先知晓有这样的神经网络结构  \(（p,v）=f_{\theta}（s）\)  （初始状态参数  \(\theta\)  随机赋值）</p><p>在自我对弈的每一步，根据深度神经网络计算出落子概率（先验概率  \(p\)  ），如对状态  \(s_{1}\)  得到  \((p_{1},v_{1}) \) ；然后通过 MCTS（蒙特卡罗搜索树算法）进行 policy<br>improvement，MCTS 搜索的输出是当前状态  \(i\)  下不同位置落子的概率 \( \pi_{i} \) ，该落子概率会优于该状态下先验概率 \(p_{i}\)  ，然后基于 \( \pi_{i} \) 完成当前步骤落子，之后每步均如此过程直到完成当前对局得到最终结果 \(z（ z\in[-1,1] ）\) 。</p><div align="center">![](/img/2018-08-08-AlphaGO-Zero-7.jpg)<center><small><font color="gray">    </font></small></center></div><p>神经网络通过使用 MCTS 搜索的自我对弈强化学习来进行训练。一开始神经网络的参数被随机设置称  \(\theta_0 \) ，在随后的每一次迭代中 \( i\geq1 \) ,会通过自我对弈产生许多完整的棋局，在其中每一个完整棋局的一个时间步 \( T \) 时，都会利用上一个神经网络的参数来产生搜索策略 \( \pi_t = \alpha_{i-1}(s_t) \) ，并且用这个策略的采样产生实际自我对弈时的行为。</p><p>发生下列任意情况之一，游戏终止于时间 \( T \) ：</p><ul><li>双方都 Pass</li><li>搜索 value 降低至一个被 resignation（割舍？）的阈值</li><li>游戏对弈达到设定的最大步数</li></ul><p>游戏结束后，会给出一个最终奖励  \(r_T \in \{ -1, +1 \}\) , 每一个时间步 T 的数据以 \( (s_t, \pi_t, z_t) \) 的形式保存，其中  \(z_t = \pm r_T\)  是从  \(T\)  时刻玩家的立场得到的胜利者的奖励（是不是可以理解成：  \(T \) 时刻不管是白方还是黑方，只要最终赢得棋局， \( z_t = 1 \) 即成立？）。</p><blockquote><p>不过需要在一个完整的对局结束后才能确定这一局中每一个 \( (s, \pi, z) \) 中的  \(z\)  ，如果最后的胜者是 \( s \) 局面下的当前 player，则 \( z=1 \) ,如果最后的败者是 \( s\)  局面下的当前 player，则  \(z=-1\)  ,如果最后打平，则  \(z=0\) </p></blockquote><p>自我对弈过程中的最后几次迭代过程中产生的数据 \( (s,\pi,z) \) 将会以均等的概率被选中来训练神经网络。</p><p>AlphaGo Zero 里面的神经网络实际上是把 AlphaGo 里面的 Policy Network 和 Value Network 糅合在一起了，所以这个神经网络也有两个目标，神经网络的训练目标就是要尽可能的缩小两方面的差距：</p><ul><li>让网络输出的落子概率向量 \(p\) 和 MCTS 搜索输出  \(\pi\)  越接近越好</li><li>让网络预测的当前棋手的最终结果 \(v\) 和最终游戏赢家 \(z\) 越接近越好</li></ul><p>神经网络的损失函数由下式确定：  </p><p><font size="+1">  $$l = (z-v)^{2} - {\pi}^\top logP + c ||\theta||^2 $$<br>  </font></p><ul><li>\(c \) 是控制参数 L2 正则项的一个系数。</li></ul><p>网络训练得到的新参数会被用来知道下一轮迭代中自我对弈时的 MCTS 搜索。</p><p>AlphaGo Zero 每1000步会将一个神经网络存档，并且把这个存档和历史最优版本比较，如果胜率超过历史最优的55%，这个版本将会被更新为历史最优。并且在生成数据时，只使用历史最优的神经网络的 self－play 数据作为深度网络的训练数据。这样可以增加算法的优化速度。</p><br><h2 id="4-搜索阶段算法"><a href="#4-搜索阶段算法" class="headerlink" title="4. 搜索阶段算法"></a>4. 搜索阶段算法</h2><br><div align="center">![](/img/2018-08-08-AlphaGO-Zero-8.jpg)<center><small><font color="gray">    </font></small></center></div><br>### a 选择 Select<p>每一次模拟的第一个阶段起自搜索树的根节点  \(s_0\)  ，在第 L 个时间步结束于搜索树的叶节点  \(s_L\)  。对于其中的任意时间  \(t&lt;L\)  ，根据搜索树内的统计数据来决定选择哪一个模拟行为  </p><p>$$a_t = \underset{a}{argmax}(Q(s_t, a) + U(s_t, a))$$</p><p>其中：<br>$$U(s,a) = c_{puct}P(s,a)\frac{\sqrt{\sum_{b}{N(s,b)}}}{1+N(s,a)}$$ </p><ul><li>\(c_{puct}\) 是决定探索程度的一个系数</li></ul><blockquote><p>this search control strategy initially prefers actions with high prior probability and low visit count, but asympotically prefers actions with high action value.</p></blockquote><br>### b 扩展和评估 Expand & Evaluate<p>叶节点  \(s_L\)  将会等待来自神经网络的评估  \((d_i (p), v) = f_\theta (d_i (s_L )) \)，其中  \(d_i \) 是一个 dihedral reflection 或 rotation，  \(i\in[1,2,3,…,8] \) 。</p><p>其中通过一个1至8的随机数来表示双方向镜面和旋转（因为围棋在棋盘旋转和镜像之后的胜率估算情况是一样的，如下图所示）</p><div align="center">![](/img/2018-08-08-AlphaGO-Zero-9.jpg)<center><small><font color="gray">    </font></small></center></div><p>这些等待评估的状态会被送入一个队列，在神经网络评估队列里的状态时（使用 mini_batch_size=8），搜索将被暂时锁定。当神经网络得到结果后，该叶节点会被展开，同时每一条可能的边  \((s_L,a)\)  会以下面的数据进行初始化：  </p><p>$$\{ N(s_L,a)=0,W(s_L,a)=0,Q(s_L,a)=0,P(s_L,a)=P_a \}$$ </p><p> 同时来自神经网络的对该叶节点的价值估计也会影响路径中每一个节点的统计数据  \(W\)（见下），随后进行回溯过程。</p><blockquote><p>AlphaGo Zero 会根据前面的落子规则不断的落子，这就相当于棋手在脑海中进行推演。但是围棋的搜索空间即使对于计算机来说也是太大，AlphaGo zero 只会推演（仿真）到一定步数就停止了。假设最后的布局是 \(s’\) , 那么 AlphaGo Zero 会调用深度神经网络来预测这局推演结束时自己的胜率 \(v(s’) \) 。这样的推演过程会进行多次。</p></blockquote><br>### c 回溯 Backup<p>等一次推演结束后，AlphaGo zero 会根据推演的结果更新自己的知识，也就是值函数 \(Q(s,u)\) </p><p>对于 \( t \leq L \) ,每一个边的统计结果将被更新。</p><p> $$ N(s_t,a_t) = N(s_t,a_t)+1$$  </p><p> $$ W(s_t,a_t) = W(s_t,a_t)+v $$      </p><p> $$Q(s_t,a_t) = \frac{W(s_t,a_t)}{N(s_t,a_t)}$$</p><br><p>$$N(s,a) :\  记录边的访问次数  $$</p><p>$$W(s,a) :\   合计行动价值  $$</p><p>$$Q(s,a) :\ 平均行动价值 $$</p><br>### d 产生实际行为 Play<p>路径中所有节点统计数据得到更新后（搜索结束后）， AlphaGo Zero 在根节点  \(s\)  处选择 \(a\) 操作进行落子，根据最新的统计数据来产生一个实际的行为 \(\pi_{a}\) ，与访问次数成幂指数比例：<br>$$\pi(a|s)=\frac{N(s,a)^{1/\tau}}{\sum_{b}^{}{N(s,b)^{1/\tau}}}$$  </p><p>\(\pi_{a}\ (\pi(a|s))\)  是落子到位置 \(a\) 的概率</p><p>\(\tau \) 为温度参数，控制探索的程度， \( \tau\)  越大，不同走法间差异变小，探索比例增大，反之，则更多选择当前最优操作。</p><p>在随后的时间步 (time_steps) 中，这个搜索树将会继续使用，对应于实际所采取的行为的子节点将变成根节点，该子节点下的子树的统计数据将会被保留，而这颗树的其余部分将会丢弃 (discarded)。 </p><p>另外，如果该子树的根节点和最佳价值子节点的价值低于某一个阈值 \( v_{resign}\)，AlphaZero 将放弃搜索某子树。</p><br>## 参考<ol><li><a href="https://www.zhihu.com/question/263583260" target="_blank" rel="noopener">AlphaGo Zero去掉蒙特卡洛搜索树会怎么样?</a></li><li><a href="https://www.zhihu.com/question/289626696" target="_blank" rel="noopener">蒙特卡洛树搜索中神经网络是如何指导蒙特卡洛树搜索进行判断的</a></li><li><a href="https://zhuanlan.zhihu.com/p/32952677" target="_blank" rel="noopener">AlphaGo Zero 简明工作原理</a></li><li><a href="https://zhuanlan.zhihu.com/p/35103060" target="_blank" rel="noopener">初步认识AlphaGo Zero原理</a></li><li><a href="https://zhuanlan.zhihu.com/p/30707897" target="_blank" rel="noopener">AlphaGo Zero论文笔记</a></li><li><a href="https://zhuanlan.zhihu.com/p/30352003" target="_blank" rel="noopener">AlphaGo Zero解读</a></li><li><a href="https://github.com/junxiaosong/AlphaZero_Gomoku" target="_blank" rel="noopener">Code:AlphaZero_Gomoku</a></li></ol><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;1. 概述&lt;/h2&gt;&lt;p&gt;简单来说，AlphaGo Zero 的训练可以分为三个同时进行的阶段：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自我对战&lt;/li&gt;
&lt;
      
    
    </summary>
    
      <category term="强化学习" scheme="https://hhyz.me/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://hhyz.me/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 并行计算</title>
    <link href="https://hhyz.me/2018/08/07/tensorflow-MultiGPU/"/>
    <id>https://hhyz.me/2018/08/07/tensorflow-MultiGPU/</id>
    <published>2018-08-06T16:00:00.000Z</published>
    <updated>2019-07-02T17:05:20.709Z</updated>
    
    <content type="html"><![CDATA[<h2 id="多-GPU"><a href="#多-GPU" class="headerlink" title="多 GPU"></a>多 GPU</h2><h3 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h3><p>如果有两块卡，但是代码里不设置的话，默认把变量都放到 <code>device(&#39;/gpu:0&#39;)</code>，所以只有 gpu 0 在计算。</p><p>tensorflow 默认是占满显存的，然后等到程序需要用的时候直接拿来用，这个是 tensorflow 设计的一个机制，对于这一机制大家褒贬不一</p><h3 id="限制-GPU-资源"><a href="#限制-GPU-资源" class="headerlink" title="限制 GPU 资源"></a>限制 GPU 资源</h3><h4 id="动态申请显存"><a href="#动态申请显存" class="headerlink" title="动态申请显存"></a>动态申请显存</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="literal">True</span></span><br><span class="line">session = tf.Session(config=config)</span><br></pre></td></tr></table></figure><h4 id="限制GPU使用率"><a href="#限制GPU使用率" class="headerlink" title="限制GPU使用率"></a>限制GPU使用率</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=<span class="number">0.333</span>)</span><br><span class="line">config=tf.ConfigProto(gpu_options=gpu_options)</span><br><span class="line">session = tf.Session(config=config)</span><br></pre></td></tr></table></figure><p>其中0.333是你自己设置的想用百分之多少的显存。</p><h3 id="例一"><a href="#例一" class="headerlink" title="例一"></a>例一</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Multi GPU computing</span></span><br><span class="line"><span class="comment"># GPU:0 computes A^n</span></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/gpu:0'</span>):</span><br><span class="line">    <span class="comment">#compute A^n and store result in c2</span></span><br><span class="line">    a = tf.constant(A)</span><br><span class="line">    c2.append(matpow(a, n))</span><br><span class="line"></span><br><span class="line"><span class="comment">#GPU:1 computes B^n</span></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/gpu:1'</span>):</span><br><span class="line">    <span class="comment">#compute B^n and store result in c2</span></span><br><span class="line">    b = tf.constant(B)</span><br><span class="line">    c2.append(matpow(b, n))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">  sum = tf.add_n(c2) <span class="comment">#Addition of all elements in c2, i.e. A^n + B^n</span></span><br><span class="line"></span><br><span class="line">t1_2 = datetime.datetime.now()</span><br><span class="line"><span class="keyword">with</span> tf.Session(config=tf.ConfigProto(log_device_placement=log_device_placement)) <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># Runs the op.</span></span><br><span class="line">    sess.run(sum)</span><br><span class="line">t2_2 = datetime.datetime.now()</span><br></pre></td></tr></table></figure><h3 id="例二"><a href="#例二" class="headerlink" title="例二"></a>例二</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Place all ops on CPU by default</span></span><br><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">    tower_grads = []</span><br><span class="line">    reuse_vars = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># tf Graph input</span></span><br><span class="line">    X = tf.placeholder(tf.float32, [<span class="literal">None</span>, num_input])</span><br><span class="line">    Y = tf.placeholder(tf.float32, [<span class="literal">None</span>, num_classes])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop over all GPUs and construct their own computation graph</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_gpus):</span><br><span class="line">        <span class="keyword">with</span> tf.device(assign_to_device(<span class="string">'/gpu:&#123;&#125;'</span>.format(i), ps_device=<span class="string">'/cpu:0'</span>)):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Split data between GPUs</span></span><br><span class="line">            _x = X[i * batch_size: (i+<span class="number">1</span>) * batch_size]</span><br><span class="line">            _y = Y[i * batch_size: (i+<span class="number">1</span>) * batch_size]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Because Dropout have different behavior at training and prediction time, we</span></span><br><span class="line">            <span class="comment"># need to create 2 distinct computation graphs that share the same weights.</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Create a graph for training</span></span><br><span class="line">            logits_train = conv_net(_x, num_classes, dropout,</span><br><span class="line">                                    reuse=reuse_vars, is_training=<span class="literal">True</span>)</span><br><span class="line">            <span class="comment"># Create another graph for testing that reuse the same weights</span></span><br><span class="line">            logits_test = conv_net(_x, num_classes, dropout,</span><br><span class="line">                                   reuse=<span class="literal">True</span>, is_training=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Define loss and optimizer (with train logits, for dropout to take effect)</span></span><br><span class="line">            loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(</span><br><span class="line">                logits=logits_train, labels=_y))</span><br><span class="line">            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line">            grads = optimizer.compute_gradients(loss_op)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Only first GPU compute accuracy</span></span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                <span class="comment"># Evaluate model (with test logits, for dropout to be disabled)</span></span><br><span class="line">                correct_pred = tf.equal(tf.argmax(logits_test, <span class="number">1</span>), tf.argmax(_y, <span class="number">1</span>))</span><br><span class="line">                accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))</span><br><span class="line"></span><br><span class="line">            reuse_vars = <span class="literal">True</span></span><br><span class="line">            tower_grads.append(grads)</span><br><span class="line"></span><br><span class="line">    tower_grads = average_gradients(tower_grads)</span><br><span class="line">    train_op = optimizer.apply_gradients(tower_grads)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initializing the variables</span></span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Launch the graph</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(init)</span><br><span class="line">        step = <span class="number">1</span></span><br><span class="line">        <span class="comment"># Keep training until reach max iterations</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> range(<span class="number">1</span>, num_steps + <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># Get a batch for each GPU</span></span><br><span class="line">            batch_x, batch_y = mnist.train.next_batch(batch_size * num_gpus)</span><br><span class="line">            <span class="comment"># Run optimization op (backprop)</span></span><br><span class="line">            ts = time.time()</span><br><span class="line">            sess.run(train_op, feed_dict=&#123;X: batch_x, Y: batch_y&#125;)</span><br><span class="line">            te = time.time() - ts</span><br><span class="line">            <span class="keyword">if</span> step % display_step == <span class="number">0</span> <span class="keyword">or</span> step == <span class="number">1</span>:</span><br><span class="line">                <span class="comment"># Calculate batch loss and accuracy</span></span><br><span class="line">                loss, acc = sess.run([loss_op, accuracy], feed_dict=&#123;X: batch_x,</span><br><span class="line">                                                                     Y: batch_y&#125;)</span><br><span class="line">                print(<span class="string">"Step "</span> + str(step) + <span class="string">": Minibatch Loss= "</span> + \</span><br><span class="line">                      <span class="string">"&#123;:.4f&#125;"</span>.format(loss) + <span class="string">", Training Accuracy= "</span> + \</span><br><span class="line">                      <span class="string">"&#123;:.3f&#125;"</span>.format(acc) + <span class="string">", %i Examples/sec"</span> % int(len(batch_x)/te))</span><br><span class="line">            step += <span class="number">1</span></span><br><span class="line">        print(<span class="string">"Optimization Finished!"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate accuracy for 1000 mnist test images</span></span><br><span class="line">        print(<span class="string">"Testing Accuracy:"</span>, \</span><br><span class="line">            np.mean([sess.run(accuracy, feed_dict=&#123;X: mnist.test.images[i:i+batch_size],</span><br><span class="line">            Y: mnist.test.labels[i:i+batch_size]&#125;) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, len(mnist.test.images), batch_size)]))</span><br></pre></td></tr></table></figure><ol><li><a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/6_MultiGPU/multigpu_basics.ipynb" target="_blank" rel="noopener">multigpu_basics (code)</a>     </li><li><a href="https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/6_MultiGPU/multigpu_cnn.ipynb" target="_blank" rel="noopener">Multi-GPU Training Example (code)</a></li><li><a href="https://www.zhihu.com/question/56291568" target="_blank" rel="noopener">tensorflow 发现两块卡的显存都占用，但是实际上只有一块卡在运算</a></li></ol><h2 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h2><p>在进行 <code>tf.ConfigProto()</code> 初始化时，我们也可以通过设置 <code>intra_op_parallelism_threads</code> 参数和 <code>inter_op_parallelism_threads</code> 参数，来控制每个操作符op并行计算的线程个数。</p><p><strong>二者的区别在于:</strong></p><p> <code>intra_op_parallelism_threads</code> 控制运算符op内部的并行   </p><p>当运算符op为单一运算符，并且内部可以实现并行时，如矩阵乘法，<code>reduce_sum</code>之类的操作，可以通过设置 <code>intra_op_parallelism_threads</code> 参数来并行, intra 代表内部。   </p><p><code>inter_op_parallelism_threads</code> 控制多个运算符op之间的并行计算</p><p>当有多个运算符 op，并且他们之间比较独立，运算符和运算符之间没有直接的路径Path相连。Tensorflow 会尝试并行地计算他们，使用由 <code>inter_op_parallelism_threads</code> 参数来控制数量的一个线程池。</p><p>以上两个参数如果设置为0代表让系统设置合适的数值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto(device_count=&#123;<span class="string">"CPU"</span>: <span class="number">4</span>&#125;, <span class="comment"># limit to num_cpu_core CPU usage</span></span><br><span class="line">                inter_op_parallelism_threads = <span class="number">1</span>, </span><br><span class="line">                intra_op_parallelism_threads = <span class="number">4</span>,</span><br><span class="line">                log_device_placement=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session(config = config) <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># To Do</span></span><br></pre></td></tr></table></figure><p>实例比较，线程数为2和4，平均每个batch的运行时间：</p><p>当参数为intra_op_parallelism_threads = 2时, 每个step的平均运行时间从610ms降低到380ms。<br>当参数为intra_op_parallelism_threads = 4时, 每个step的平均运行时间从610ms降低到230ms。</p><p>总结，在固定CPUcore的资源限制下，通过合理设置线程thread个数可以明显提升tensorflow程序运行速度。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;多-GPU&quot;&gt;&lt;a href=&quot;#多-GPU&quot; class=&quot;headerlink&quot; title=&quot;多 GPU&quot;&gt;&lt;/a&gt;多 GPU&lt;/h2&gt;&lt;h3 id=&quot;简述&quot;&gt;&lt;a href=&quot;#简述&quot; class=&quot;headerlink&quot; title=&quot;简述&quot;&gt;&lt;/a&gt;简
      
    
    </summary>
    
      <category term="TensorFlow" scheme="https://hhyz.me/categories/TensorFlow/"/>
    
    
      <category term="TensorFlow" scheme="https://hhyz.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>强化学习（一）Deep Q-Network</title>
    <link href="https://hhyz.me/2018/08/05/RL/"/>
    <id>https://hhyz.me/2018/08/05/RL/</id>
    <published>2018-08-04T16:00:00.000Z</published>
    <updated>2019-07-02T17:05:20.706Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><blockquote><p>虽然将深度学习和增强学习结合的想法在几年前就有人尝试，但真正成功的开端就是DeepMind在NIPS 2013上发表的 <strong><a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1312.5602" target="_blank" rel="noopener">Playing Atari with Deep Reinforcement Learning</a></strong> 一文，在该文中第一次提出Deep Reinforcement Learning 这个名称，并且提出DQN（Deep Q-Network）算法，实现从纯图像输入完全通过学习来玩Atari游戏的成果。之后DeepMind在Nature上发表了改进版的DQN文章<strong>Human-level Control through Deep Reinforcement Learning</strong>，引起了广泛的关注，Deep Reinfocement Learning 从此成为深度学习领域的前沿研究方向。</p></blockquote><p><strong>智能体 Agent</strong> 来表示一个具备行为能力的物体，比如机器人，无人车，人等等。</p><p>那么增强学习考虑的问题就是智能体Agent和 <strong>环境 Environment</strong> 之间交互的任务。</p><blockquote><p>比如一个机械臂要拿起一个手机，那么机械臂周围的物体包括手机就是环境，机械臂通过外部的比如摄像头来感知环境，然后机械臂需要输出动作来实现拿起手机这个任务。再举玩游戏的例子，比如我们玩极品飞车游戏，我们只看到屏幕，这就是环境，然后我们输出动作（键盘操作）来控制车的运动。</p></blockquote><p>那么，不管是什么样的任务，<br>都包含了一系列的：</p><ul><li><strong>动作 Action</strong>           </li><li><strong>观察 Observation</strong>     </li><li><strong>反馈值 Reward</strong>   </li></ul><p>所谓的Reward就是Agent执行了动作与环境进行交互后，环境会发生变化，变化的好与坏就用Reward来表示。</p><blockquote><p>接下来这里用了Observation观察一词而不是环境那是因为Agent不一定能得到环境的所有信息，比如机械臂上的摄像头就只能得到某个特定角度的画面。因此，只能用Observation来表示Agent获取的感知信息。</p></blockquote><p>只能用 Observation 来表示 Agent 获取的感知信息</p><p>每个时间片，Agent 都是根据当前的观察来确定下一步的动作。观察 Observation 的集合就作为Agent的所处的 <strong>状态 State</strong>，因此，<strong>状态 State</strong> 和 <strong>动作 Action</strong> 存在映射关系，也就是一个 state 可以对应一个 action，或者对应不同动作的概率（常常用概率来表示，概率最高的就是最值得执行的动作）。状态与动作的关系其实就是输入与输出的关系，而状态 State 到动作 Action 的过程就称之为一个<strong>策略 Policy</strong>，一般用 \(\pi\) 表示，也就是需要找到以下关系：  </p><p>$$a=\pi(s)$$</p><p>或者</p><p>$$\pi(a|s)$$</p><p>其中 <strong>a 是 action，s 是 state</strong>。<br>第一种是一一对应的表示，第二种是概率的表示。</p><blockquote><p>增强学习的任务就是找到一个最优的策略Policy从而使Reward最多</p></blockquote><p>我们一开始并不知道最优的策略是什么，因此往往从随机的策略开始，使用随机的策略进行试验，就可以得到一系列的状态,动作和反馈：</p><p>$$\{s_1,a_1,r_1,s_2,a_2,r_2,…s_t,a_t,r_t\}$$</p><p>这就是一系列的 <strong>样本 Sample</strong>。增强学习的算法就是需要根据这些样本来改进 Policy，从而使得得到的样本中的 Reward 更好。由于这种让 Reward 越来越好的特性，所以这种算法就叫做增强学习Reinforcement Learning。</p><br>## 2. 马尔科夫决策过程<blockquote><p>MDP只需要用一句话就可以说明白，就是 <strong>“未来只取决于当前”</strong>，专业点说就是下一步的状态只取决于当前的状态，与过去的状态没有关系。</p></blockquote><p><strong>一个状态 \(S_t\) 是Markov当且仅当：</strong></p><p><font size="+1">   $$P(s_{t+1}|s_t)=P(s_{t+1}|s_t,s_{t-1},…s_1,s_0)$$  </font></p><p>P为概率。简单的说就是下一个状态仅取决于当前的状态和当前的动作。<br>增强学习的问题都可以模型化为MDP的问题</p><p><strong>因此 MDP 可以表示为一个元组</strong> \((S, A, P_{sa}, R)\) ：</p><ul><li>\(S\) ：所有可能状态的集合， \(s \in S\)，\(s\) 表示某个特定状态</li><li>\(A\) ：针对每个状态，我们都要做出动作，这些动作的集合就是 \(A\)； \(a \in A\)，有限动作 action 集合， \(a\) 表示某个特定动作</li><li>\(P_{sa}\) ：状态转换分布（statetransition distribution），如果我们在状态 \(s\) 中采取了动作 \(a\) ，系统会转移到一个新的状态，状态转换分布描述了转移到哪个状态的概率分布。</li><li>\(R\) ：回馈函数（rewardfunction），增强学习的核心概念，描述了动作能够产生的回报。比如 \(R_π(s,a)\) 描述了在状态 \(s\) 下采用策略 \(\pi\) 所对应的动作 \(a\) 的回报，也叫做立即回报，回馈函数可以有不同的表达形式。</li><li>\(\pi(s)\rightarrow a\)： 策略 policy，根据当前 state 来产生 action，可表现为  \(a=\pi(s) \) 或 \( \pi(a|s) = P[a|s]\)，后者表示某种状态下执行某个动作的概率</li></ul><blockquote><p>一个基本的 MDP 可以用 \((S,A,P)\) 来表示， \(S \) 表示状态， \(A\)  表示动作， \(P \) 表示状态转移概率，也就是根据当前的状态 \(s_t\) 和 \(a_t\) 转移到 \(s_{t+1}\) 的概率。</p></blockquote><p>如果我们知道了转移概率 P，也就是称为我们获得了 <strong>模型 Model</strong>，有了模型，未来就可以求解，那么获取最优的动作也就有可能，这种通过模型来获取最优动作的方法也就称为 Model-based 的方法。但是现实情况下，很多问题是很难得到准确的模型的，因此就有 Model-free 的方法来寻找最优的动作。</p><br>## 3. 价值函数<blockquote><p>既然一个状态对应一个动作，或者动作的概率，而有了动作，下一个状态也就确定了。这就意味着每个状态可以用一个确定的值来进行描述。可以由此判断一个状态是好的状态还是不好的状态。</p></blockquote><p>但是在选取最优策略的过程中，我们只看立即回报并不能判定哪种策略更优，我们希望的是在采取了策略 \(\pi\) 以后，可以使得整个状态序列的折扣回馈最大。</p><p>状态的好坏其实等价于对未来回报的期望，<strong>回报 Return</strong> 来表示某个时刻 t 的状态将具备的回报：</p><p><font size="+1">    $$G_t = R_{t+1} + \lambda R_{t+2} + … = \sum_{k=0}^\infty\lambda^kR_{t+k+1}$$ </font></p><ul><li>R 是 Reward 反馈</li><li>\(λ\) 是 discount factor 折扣因子，一般小于 1，就是说一般当下的反馈是比较重要的，时间越久，影响越小。</li></ul><blockquote><p>其中 \(λ\) 被称为折扣因子，在经济学上的解释叫做无风险折现率（risk-freeinterest rate），意思是马上得到的钱（回馈）比未来得到钱更有价值。</p></blockquote><p><strong>以上概念就组成了增强学习的完整描述：</strong>找到一种策略，使得我们能够根据状态 \(s_0, s_1, s_2, …\) 采取策略中对应的动作 \(a_0, a1, a2…，\) 并使 \(G_t\) 的期望值最大化</p><p>引出价值函数，对于获取最优的策略Policy这个目标，我们就会有两种方法：</p><ul><li>直接优化策略 \(\pi(a|s)\) 或者 \(a = \pi(s)\) 使得回报更高</li><li>通过估计 value function 来间接获得优化的策略。道理很简单，既然我知道每一种状态的优劣，那么我就知道我应该怎么选择了，而这种选择就是我们想要的策略。</li></ul><blockquote><p>但是现在为了理解DQN，我们将只关注第二种做法，就是估计value function的做法，因为DQN就是基于value function的算法。</p></blockquote><ol><li><strong>状态价值函数</strong> \(V\) ：从状态 \(x_0\)  开始, 所有的动作 \(a\) ，都是执行某个策略 \(π\) 的结果，最后求每个动作带来累积奖赏</li><li><strong>动作价值函数</strong> \(Q\) ：从状态 \(x_0\)  开始，先执行动作 \(a_0\) ， 然后再执行某个策略 \(π\) ，再求相应的积累奖赏</li></ol><br>### 3.1 State-Value function 状态价值函数<p>那么实际上除非整个过程结束，否则显然我们无法获取所有的 reward 来计算出每个状态的Return，因此，再引入一个 <strong>概念价值函数 Value Function</strong>，用 <strong>value function</strong>  \(v(s)\)  来表示一个状态未来的潜在价值。</p><p>从定义上看，<strong>value function</strong> 就是回报的期望：</p><p>$$v(s)  = \mathbb E[G_t|S_t = s]$$</p><p>$$V^{\pi}(s) =\mathbb E_{\pi}[ R(s_0, a_0) + γR(s_1, a_1)+ γ^2R(s_2, a_2) + … | s_0= s ]$$</p><p>这个函数也被称为<strong>状态价值函数(statevalue function)</strong>，记为 \(V_{\pi}(s)\)。 因为初始状态 \(s\) 和策略 \(\pi\) 是我们给定的，动作 \(a = \pi(s)\) 。</p><br>### 3.2 Action-Value function 动作价值函数<p><strong>我们更关心在某个状态下的不同动作的价值</strong>。显然。如果知道了每个动作的价值，那么就可以选择价值最大的一个动作去执行了。</p><p>这就是 <strong>Action-Value function</strong>  \(Q^\pi(s,a)\) 。那么同样的道理，也是使用 reward 来表示，只是这里的 reward 和之前的 reward 不一样：</p><p><strong>这里是执行完动作 action 之后得到的 reward，之前 state 对应的 reward 则是多种动作对应的 reward 的期望值</strong>。显然，动作之后的 reward 更容易理解。</p><p>那么，有了上面的定义，动作价值函数就为如下表示：</p><p>$$ Q^{\pi}(s,a) = \mathbb E[R_{t+1}+\lambda R_{t+2} + \lambda ^2R_{t+3} + …|S_t = s,A_t=a]$$</p><p>$$<br>\begin{align}<br>Q^\pi(s,a) &amp; =  \mathbb E[r_{t+1} + \lambda r_{t+2} + \lambda^2r_{t+3} + … |s,a] \\<br>&amp; = \mathbb E_{s^\prime}[r+\lambda Q^\pi(s^\prime,a^\prime)|s,a]<br>\end{align}<br>$$</p><p>这里要说明的是动作价值函数的定义，加了 \(\pi\) ,也就是说是<strong>在策略下的动作价值</strong>。因为对于每一个动作而已，都需要由策略根据当前的状态生成，因此必须有策略的支撑。而前面的价值函数则不一定依赖于策略。当然，如果定义 \(v^\pi(s)\) 则表示在策略 \(\pi\) 下的价值。</p><p>那么事实上我们会更多的使用动作价值函数而不是价值函数，因为动作价值函数更直观，更方便应用于算法当中。</p><br>## 4. Bellman 方程<blockquote><p>在上文我们介绍了 Value Function 价值函数，所以为了解决增强学习的问题，一个显而易见的做法就是我们需要估算 Value Function。是的，只要我们能够计算出价值函数，那么最优决策也就得到了。因此，问题就变成了如何计算 Value Function？</p></blockquote><p>$$P^a_{ss\prime} = P(S_{t+1}=s\prime|S_t =s, A_t =a)$$</p><p>还记得回报 Result 的基本定义吗？就是所有 Reward 的累加（带衰减系数 discount factor）</p><p>$$G_t = R_{t+1} + \lambda R_{t+2} + … = \sum_{k=0}^\infty\lambda^kR_{t+k+1}$$</p><p>那么 Value Function 该如何定义？也很简单，就是期望的回报啊！期望的回报越高，价值显然也就越大，也就越值得去选择。用数学来定义就是如下：</p><p>$$v(s) = \mathbb E[G_t|S_t = s]$$</p><p>$$v_{\pi}=\sum_{a\in A}P(a|s)\left(R^a_s+\lambda\sum_{s\prime \in S}P^a_{ss\prime}v_{\pi}(s\prime)\right)$$</p><p>接下来，我们把上式展开如下：</p><p>$$<br>\begin{align}<br> v(s) &amp; = \mathbb E[G_t|S_t = s] \\<br>      &amp; = \mathbb E[R_{t+1}+\lambda R_{t+2} + \lambda ^2R_{t+3} + …|S_t = s] \\<br>      &amp; = \mathbb E[R_{t+1}+\lambda (R_{t+2} + \lambda R_{t+3} + …)|S_t = s] \\<br>      &amp; = \mathbb E[R_{t+1} + \lambda G_{t+1}|S_t = s] \\<br>      &amp; = \mathbb E[R_{t+1} + \lambda v(S_{t+1})|S_t = s]<br>\end{align}<br>$$</p><p>因此，</p><p><font size="+1">   $$v(s) = \mathbb E[R_{t+1} + \lambda v(S_{t+1})|S_t = s]$$  </font></p><p>上面这个公式就是Bellman方程的基本形态。从公式上看，<strong>当前状态的价值和 下一步的价值以及当前的反馈Reward有关。</strong></p><p>它表明Value Function是可以<strong>通过迭代来进行计算的!!!</strong></p><p>总结一下：</p><p>$$v_{\pi}(s) = \mathbb E[R_{t+1} + \lambda v_{\pi}(S_{t+1})|S_t = s]$$</p><p>$$ q_{\pi}(s,a) = \mathbb E_{\pi}[R_{t+1} +\lambda q_\pi(S_{t+1},A_{t+1})|S_t =s,A_t = a]$$</p><br>## 5. 最优化<h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><p>先简单介绍一下动态规划，因为严格来说，值迭代与策略迭代是用来解决动态规划问题的两种规划方法。而强化学习又有另外一个昵称——就是<strong>拟动态规划</strong>。说白了强化学习就是模拟动态规划算法。</p><p>用一句话来总结动态规划就是，对一个复杂问题给出一个一般性的解决办法。它主要由两个性质:</p><ul><li><strong>最优子结构</strong>：最优解法能被分解到多个子问题中</li><li><strong>重叠子问题</strong>：子问题能重复多次且解法能被重复利用、</li></ul><p>马尔科夫决策过程（MDP）满足以上两个性质，所以任何 MDP 都可以用动态规划来解。动态规划与强化学习的区别就是<strong>动态规划假设 MDP 模型是全知的（即参数可知） 而 强化学习可以使 MDP 未知</strong>。</p><p>MDP需要解决的问题有两种：</p><ul><li>第一种是 prediction，它已知MDP的 \(S,A,P,R,γ\) 以及 policy，目标是算出在每个状态下的 value function(值函数其实就是问题的目标，一般都是跟 reward 有关的函数，例如 Atari 小游戏，一般值函数就是累计的得分的期望。目标一般就是最大化这个值函数。</li><li>而第二种是control，它已知 MDP 的 \(S,A,P,R,γ\) 但是 policy 未知（即动作  \(a_t\)  未知），因此它的目标不仅是计算出最优的 value function 而且要给出最优的 Policy。</li></ul><h3 id="5-1-Optimal-value-function-最优价值函数"><a href="#5-1-Optimal-value-function-最优价值函数" class="headerlink" title="5.1 Optimal value function 最优价值函数"></a>5.1 Optimal value function 最优价值函数</h3><blockquote><p>能计算动作价值函数是不够的，因为我们需要的是最优策略，现在求解最优策略等价于求解最优的 value function，找到了最优的 value function，自然而然策略也就是找到。（当然，这只是求解最优策略的一种方法，也就是 value-based approach，由于 DQN 就是 value-based，因此这里只讲这部分，以后我们会看到还有 policy-based 和 model-based 方法。一个就是直接计算策略函数，一个是估计模型，也就是计算出状态转移函数，从而整个MDP过程得解）</p></blockquote><p>首先是最优动作价值函数和一般的动作价值函数的关系：</p><p>$$V^*(s,a) = \max_\pi V^\pi(s,a)$$</p><p>$$Q^*(s,a) = \max_\pi Q^\pi(s,a)$$</p><p>也就是最优的动作价值函数就是所有策略下的动作价值函数的最大值。通过这样的定义就可以使最优的动作价值的唯一性，从而可以求解整个MDP。</p><p>那么套用上一节得到的 value function，可以得到</p><p>$$Q^*(s,a) = \mathbb E_{s^\prime}[r+\lambda \max _{a^\prime}Q^*(s^\prime,a^\prime)|s,a]$$</p><p>因为最优的Q值必然为最大值，所以，等式右侧的Q值必然为使 \(a′\) 取最大的Q值。</p><p>下面介绍基于Bellman方程的两个最基本的算法，策略迭代和值迭代。</p><div align="center">![](/img/2018-08-05-RL-1.jpg)![](/img/2018-08-05-RL-2.jpg)<center><small><font color="gray">    </font></small></center></div><h3 id="5-2-Policy-Iteration-策略迭代"><a href="#5-2-Policy-Iteration-策略迭代" class="headerlink" title="5.2 Policy Iteration 策略迭代"></a>5.2 Policy Iteration 策略迭代</h3><p><strong>策略迭代就是在policy未知的情况下，根据每次的reward学到最优policy。</strong></p><p>对一个具体的 MDP 问题，每次先初始化一个策略，根据这个策略计算值函数 \(v(s)\) , 通过这个re值函数来根据贪心策略更新策略，不断迭代最终得到最优策略与最优值函数。总结下来就两个阶段。</p><ul><li><strong>Policy evaluation</strong> ：根据每一次的给出策略估计 \(v_π\) </li><li><strong>Policy improvement</strong>：根据 Greedy poilcy 和之前得到的 \(v_π\) 获得当前策略 \(π′\) </li></ul><p>Policy Iteration的目的是通过迭代计算value function 价值函数的方式来使policy收敛到最优。</p><p><img src="/img/2018-08-05-RL-15.jpg" alt><br><img src="/img/2018-08-05-RL-16.jpg" alt></p><p><strong>给一个例子：</strong></p><p>下图是一个叫 Small Gridworld 的例子，左上角和右下角是终点， \(γ=1\) ，移动一步 reward 减少1，起始的 random policy 是朝每个能走的方向概率相同，先单独看左边一列，它表示在第 \(k\) 次迭代每个 state上value function 的值，这一列始终采用了 random policy，这里的 value function 就是通过 Bellman Expectation Equation 得到的，考虑 \(k=2\) 的情况， \(-1.7 = -1.0 + 2\times (1/3.0)(-1)\)，\(-2.0 = -1.0 + 4(1/4.0)\times (-1)\) 。而右边一列就是在当前的 value function 情况下通过 greedy 算法找到当前朝哪个方向走更好。</p><p><img src="/imf/2018-08-05-RL-17.jpg" alt><br><img src="/img/2018-08-05-RL-18.jpg" alt></p><p>Policy Iteration 本质上就是直接使用 Bellman 方程而得到的：</p><div align="center">![](/img/2018-08-05-RL-3.jpg)<center><small><font color="gray">    </font></small></center></div><h3 id="5-3-Value-Iteration-价值迭代"><a href="#5-3-Value-Iteration-价值迭代" class="headerlink" title="5.3 Value Iteration 价值迭代"></a>5.3 Value Iteration 价值迭代</h3><p>Value Iteration 则是使用 Bellman 最优方程得到：</p><div align="center">![](/img/2018-08-05-RL-4.jpg)<center><small><font color="gray">    </font></small></center></div><p>然后改变成迭代形式：</p><div align="center">![](/img/2018-08-05-RL-5.jpg)<center><small><font color="gray">    </font></small></center></div>值迭代就是在已知 policy 和 MDP 模型的情况下，根据策略获得最优值函数和最优策略。 只不过这是确定策略，在值函数 \\(v\_π\\) 取得最大值的 \\(a\_t\\) (策略) 通过每次迭代bellman方程获得 \\(v\_i\\) , 知道值函数收敛。图解如下：    <br><div align="center">![](/img/2018-08-05-RL-6.jpg)<center><small><font color="gray">    </font></small></center></div><br>## 6. Q-Value (Quality-Value)<p>Q Learning的思想完全根据value iteration得到。但要明确一点是value iteration每次都对所有的Q值更新一遍，也就是所有的状态和动作。但事实上在实际情况下我们没办法遍历所有的状态，还有所有的动作，我们只能得到有限的系列样本。因此，只能使用有限的样本进行操作。那么，怎么处理？Q Learning提出了一种更新Q值的办法：</p><p><font size="+1">  $$Q(S_{t},A_{t}) \leftarrow Q(S_{t},A_{t})+\alpha({R_{t+1}+\lambda \max _aQ(S_{t+1},a)} - Q(S_t,A_t))$$   </font></p><p>虽然根据value iteration计算出target Q值，但是这里并没有直接将这个Q值（是估计值）直接赋予新的Q，而是采用渐进的方式类似梯度下降，朝target迈近一小步，取决于α,这就能够减少估计误差造成的影响。类似随机梯度下降，最后可以收敛到最优的Q值。</p><p><strong>具体的算法如下：</strong></p><div align="center">![](/img/2018-08-05-RL-7.jpg)<center><small><font color="gray">    </font></small></center></div><p><strong>大致代码流程如下：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="comment"># 学习 100 回合</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        <span class="comment"># 初始化 state 的观测值</span></span><br><span class="line">        observation = env.reset()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 更新可视化环境</span></span><br><span class="line">            env.render()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL 大脑根据 state 的观测值挑选 action</span></span><br><span class="line">            action = RL.choose_action(str(observation))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 探索者在环境中实施这个 action, 并得到环境返回的下一个 state 观测值, reward 和 done (是否是掉下地狱或者升上天堂)</span></span><br><span class="line">            observation_, reward, done = env.step(action)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># RL 从这个序列 (state, action, reward, state_) 中学习</span></span><br><span class="line">            RL.learn(str(observation), action, reward, str(observation_))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将下一个 state 的值传到下一次循环</span></span><br><span class="line">            observation = observation_</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果掉下地狱或者升上天堂, 这回合就结束了</span></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 结束游戏并关闭窗口</span></span><br><span class="line">    print(<span class="string">'game over'</span>)</span><br><span class="line">    env.destroy()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 定义环境 env 和 RL 方式</span></span><br><span class="line">    env = Maze()</span><br><span class="line">    RL = QLearningTable(actions=list(range(env.n_actions)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开始可视化环境 env</span></span><br><span class="line">    env.after(<span class="number">100</span>, update)</span><br><span class="line">    env.mainloop()</span><br></pre></td></tr></table></figure><p><strong>注意：</strong></p><p>每一组 <code>(state, action, reward, state_)</code> 为一次序列</p><br>以我们回到之前的流程, 根据 Q 表的估计, 因为在 s1 中, a2 的值比较大, 通过之前的决策方法, 我们在 s1 采取了 a2, 并到达 s2, 这时我们开始更新用于决策的 Q 表, 接着我们并没有在实际中采取任何行为, 而是再想象自己在 s2 上采取了每种行为, 分别看看两种行为哪一个的 Q 值大, 比如说 Q(s2, a2) 的值比 Q(s2, a1) 的大, 所以我们把大的 Q(s2, a2) 乘上一个衰减值 gamma (比如是0.9) 并加上到达s2时所获取的奖励 R (这里还没有获取到我们的棒棒糖, 所以奖励为 0), 因为会获取实实在在的奖励 R , 我们将这个作为我现实中 Q(s1, a2) 的值, 但是我们之前是根据 Q 表估计 Q(s1, a2) 的值. 所以有了现实和估计值, 我们就能更新Q(s1, a2) , 根据 估计与现实的差距, 将这个差距乘以一个学习效率 alpha 累加上老的 Q(s1, a2) 的值 变成新的值. <p><strong>但时刻记住, 我们虽然用 maxQ(s2) 估算了一下 s2 状态, 但还没有在 s2 做出任何的行为, s2 的行为决策要等到更新完了以后再重新另外做. 这就是 off-policy 的 Q learning 是如何决策和学习优化决策的过程.</strong></p><div align="center">![](/img/2018-08-05-RL-8.jpg)<center><small><font color="gray">    </font></small></center></div><p>$$\text{update = learing_rate * (q_target - q_predict)}$$</p><p>$$\text{学习率 * (真实值 - 预测值)}$$</p><blockquote><p>我们想象 Qlearning 的机器人天生近视眼,  \(\gamma= 1\) 时, 机器人有了一副合适的眼镜, 在 s1 看到的 Q 是未来没有任何衰变的奖励, 也就是机器人能清清楚楚地看到之后所有步的全部价值, 但是当  \(\gamma= 0\) , 近视机器人没了眼镜, 只能摸到眼前的 reward, 同样也就只在乎最近的大奖励, 如果  \(\gamma\)  从 0 变到 1, 眼镜的度数由浅变深, 对远处的价值看得越清楚, 所以机器人渐渐变得有远见, 不仅仅只看眼前的利益, 也为自己的未来着想.</p></blockquote><br>## 7. Exploration and Exploitation 探索与利用<p>在上面的算法中，我们可以看到需要使用某一个policy来生成动作，也就是说这个policy不是优化的那个policy，所以Q-Learning算法叫做Off-policy的算法。另一方面，因为Q-Learning完全不考虑model模型也就是环境的具体情况，只考虑看到的环境及reward，因此是model-free的方法。 </p><p>回到policy的问题，那么要选择怎样的 policy 来生成 action 呢？有两种做法：</p><ul><li>随机的生成一个动作</li><li>根据当前的Q值计算出一个最优的动作，这个 policy \(\pi\) 称之为 <strong>greedy policy 贪婪策略</strong>。也就是 \(\pi(S_{t+1}) = arg\max _aQ(S_{t+1},a)\) </li></ul><p><strong>使用随机的动作就是 exploration</strong>，也就是探索未知的动作会产生的效果，有利于更新Q值，获得更好的policy。</p><p>而使用 <strong>greedy policy 也就是 target policy 则是 exploitation</strong>，利用policy，这个相对来说就不好更新出更好的Q值，但可以得到更好的测试效果用于判断算法是否有效。</p><p>将两者结合起来就是所谓的 \(\epsilon\ greedy\) 策略， \(\epsilon\) 一般是一个很小的值，作为<strong>选取随机动作的概率值</strong>。可以更改 \(\epsilon\) 的值从而得到不同的 exploration 和 exploitation 的比例。例如 \(\epsilon = 0.1\) 表示 90% 的时间是选择最优策略, 10% 的时间来探索.</p><blockquote><p>要注意一点就是 egreedy 的 \(\epsilon\) 是不断变小的，也就是随机性不断变小。怎么理解呢？就是一开始需要更多的探索，所以动作偏随机，慢慢的我们需要动作能够有效，因此减少随机。也就是越来越贪婪。<br><strong>例如：</strong><br>INITIAL_EPSILON = 0.5  # starting value of epsilon<br>FINAL_EPSILON = 0.01  # final value of epsilon</p></blockquote><p>这里需要说明的一点是使用 \(\epsilon-greedy\) 策略是一种极其简单粗暴的方法，对于一些复杂的任务采用这种方法来探索未知空间是不可取的。因此，最近有越来越多的方法来改进这种探索机制。</p><br>## 8. 详解Q-Learning<h3 id="8-1-Value-Function-Approximation-价值函数近似"><a href="#8-1-Value-Function-Approximation-价值函数近似" class="headerlink" title="8.1 Value Function Approximation 价值函数近似"></a>8.1 Value Function Approximation 价值函数近似</h3><p>在简单分析中，我们使用表格来表示Q(s,a)，但是这个在现实的很多问题上是几乎不可行的，因为状态实在是太多。使用表格的方式根本存不下。</p><p>我们有必要对状态的维度进行压缩，解决办法就是 <strong>价值函数近似 Value Function Approximation</strong></p><p>就是<strong>用一个函数来表示Q(s,a)</strong>，即：</p><p>$$Q(s,a) = f(s,a)$$</p><p>f可以是任意类型的函数，比如线性函数：</p><p>$$Q(s,a) = w_1s + w_2a + b$$<br>其中 \(w_1,w_2,b\) 是函数 \(f\) 的参数。</p><p>通过函数表示，我们就可以无所谓 \(s\) 到底是多大的维度，反正最后都通过矩阵运算降维输出为单值的 \(Q\) 。这就是价值函数近似的基本思路。</p><p>如果我们就用 \(w\) 来统一表示函数f的参数，那么就有</p><p>$$Q(s,a) = f(s,a,w)$$</p><p>为什么叫近似，因为我们并不知道 \(Q\) 值的实际分布情况，本质上就是用一个函数来近似 \(Q\) 值的分布，所以，也可以说是</p><p>$$Q(s,a)\approx f(s,a,w)$$</p><br>### 8.2 Q值神经网络化！<blockquote><p>用一个深度神经网络来表示这个函数 \(f\)，即我们可以将状态和动作当成神经网络的输入, 然后经过神经网络分析后得到动作的 Q 值, 这样我们就没必要在表格中记录 Q 值, 而是直接使用神经网络生成 Q 值.还有一种形式的是这样, 我们也能只输入状态值, 输出所有的动作值, 然后按照 Q learning 的原则, 直接选择拥有<strong>最大值的动作</strong>当做下一步要做的动作。一般使用第二种形式。</p></blockquote><p>以DQN为例，输入是经过处理的4个连续的84x84图像，然后经过两个卷积层，两个全连接层，最后输出包含每一个动作Q值的向量。</p><p>用神经网络来表示Q值非常简单，Q值也就是变成用Q网络（Q-Network）来表示。接下来就到了很多人都会困惑的问题，那就是怎么训练Q网络？？？</p><p>我们知道，神经网络的训练是一个最优化问题，最优化一个损失函数loss function，也就是标签和网络输出的偏差，目标是让损失函数最小化。为此，我们需要有样本，巨量的有标签数据，然后通过反向传播使用梯度下降的方法来更新神经网络的参数。</p><p>所以，要训练Q网络，我们要能够为Q网络提供有标签的样本。</p><p>所以，问题变成：</p><p><strong>如何为 Q 网络提供有标签的样本？<br>答案就是利用 Q-Learning 算法。</strong></p><p>回想一下 Q-Learning 算法，<br>$$Q(S_{t},A_{t}) \leftarrow Q(S_{t},A_{t})+\alpha({R_{t+1}+\lambda \max _aQ(S_{t+1},a)} - Q(S_t,A_t))$$</p><p>Q值的更新依靠什么？依靠的是利用 Reward 和 Q 计算出来的目标Q值：</p><p>$$\text{Target-Q :  }\ \ R_{t+1}+\lambda \max _aQ(S_{t+1},a)$$</p><p>因此，我们把目标Q值作为标签不就完了？我们的目标不就是<strong>让Q值趋近于目标Q值</strong>吗？<br>因此，Q网络训练的损失函数就是：</p><div align="center">![](/img/2018-08-05-RL-9.jpg)<center><small><font color="gray">  \\(s^\`,a^`\\) 即下一个状态和动作   </font></small></center></div><p>既然确定了损失函数，也就是cost，确定了获取样本的方式。那么DQN的整个算法也就成型了！接下来就是具体如何训练的问题了！</p><br>**前边提到**，每一组 `(state, action, reward, state_)` 为一次序列：<ul><li><code>state (observation)</code> 为目前状态，传递给 q-eval net 得到预计值 (即输入状态值, 输出所有的动作值)；     </li><li><code>state_</code> 为下一步状态，传递给 q-target net 得到目标值，之后可以得到 \(\max _aQ(S_{t+1},a)\) ，在之后得到 \(R_{t+1}+\lambda \max _aQ(S_{t+1},a)\) </li></ul><p><strong>最终的  \(loss\)  为：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))</span><br></pre></td></tr></table></figure><p><strong>再次注意，q-eval net 和 q-target net 网络结构完全一样，只不过参数更新不同！(即 Fixed Q-targets 方法)</strong></p><div align="center">![](/img/2018-08-05-RL-10.jpg)<center><small><font color="gray">    </font></small></center></div><br><div align="center">![](/img/2018-08-05-RL-11.jpg)<center><small><font color="gray">    </font></small></center></div><p>最基本的DQN，也就是NIPS 13版本的DQN：</p><div align="center">![](/img/2018-08-05-RL-12.jpg)<center><small><font color="gray">    </font></small></center></div><p><strong>那么上面的算法看起来那么长，其实就是反复试验，然后存储数据。接下来数据存到一定程度，就每次随机采用数据，进行梯度下降！</strong></p><p>也就是在DQN中增强学习 Q-Learning 算法和深度学习的 SGD 训练是同步进行的！<br>通过 Q-Learning 获取无限量的训练样本，然后对神经网络进行训练。</p><p><strong>整体代码结构大致如下：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_maze</span><span class="params">()</span>:</span></span><br><span class="line">    step = <span class="number">0</span>    <span class="comment"># 用来控制什么时候学习</span></span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> range(<span class="number">300</span>):</span><br><span class="line">        <span class="comment"># 初始化环境</span></span><br><span class="line">        observation = env.reset()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="comment"># 刷新环境</span></span><br><span class="line">            env.render()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># DQN 根据观测值选择行为</span></span><br><span class="line">            action = RL.choose_action(observation)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 环境根据行为给出下一个 state, reward, 是否终止</span></span><br><span class="line">            observation_, reward, done = env.step(action)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># DQN 存储记忆</span></span><br><span class="line">            RL.store_transition(observation, action, reward, observation_)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 控制学习起始时间和频率 (先累积一些记忆再开始学习)</span></span><br><span class="line">            <span class="keyword">if</span> (step &gt; <span class="number">200</span>) <span class="keyword">and</span> (step % <span class="number">5</span> == <span class="number">0</span>):</span><br><span class="line">                RL.learn()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 将下一个 state_ 变为 下次循环的 state</span></span><br><span class="line">            observation = observation_</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 如果终止, 就跳出循环</span></span><br><span class="line">            <span class="keyword">if</span> done:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            step += <span class="number">1</span>   <span class="comment"># 总步数</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># end of game</span></span><br><span class="line">    print(<span class="string">'game over'</span>)</span><br><span class="line">    env.destroy()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    env = Maze()</span><br><span class="line">    RL = DeepQNetwork(env.n_actions, env.n_features,</span><br><span class="line">                      learning_rate=<span class="number">0.01</span>,</span><br><span class="line">                      reward_decay=<span class="number">0.9</span>,</span><br><span class="line">                      e_greedy=<span class="number">0.9</span>,</span><br><span class="line">                      replace_target_iter=<span class="number">200</span>,  <span class="comment"># 每 200 步替换一次 target_net 的参数</span></span><br><span class="line">                      memory_size=<span class="number">2000</span>, <span class="comment"># 记忆上限</span></span><br><span class="line">                      <span class="comment"># output_graph=True   # 是否输出 tensorboard 文件</span></span><br><span class="line">                      )</span><br><span class="line">    env.after(<span class="number">100</span>, run_maze)</span><br><span class="line">    env.mainloop()</span><br><span class="line">    RL.plot_cost()  <span class="comment"># 观看神经网络的误差曲线</span></span><br></pre></td></tr></table></figure><br>### 8.3 Experience Replay 经验回放<blockquote><p>Q learning 是一种 off-policy 离线学习法, 它能学习当前经历着的, 也能学习过去经历过的, 甚至是学习别人的经历. 所以每次 DQN 更新的时候, 我们都可以随机抽取一些之前的经历进行学习. 随机抽取这种做法打乱了经历之间的相关性, 也使得神经网络更新更有效率。</p></blockquote><p>其将系统探索环境得到的数据储存起来，然后随机采样样本更新深度神经网络的参数（有了一个记忆库之后再开始学习）。</p><div align="center">![](/img/2018-08-05-RL-13.jpg)<center><small><font color="gray">    </font></small></center></div><p>Experience Replay 的<strong>动机</strong>是：</p><ul><li>深度神经网络作为有监督学习模型，要求数据满足独立同分布，</li><li>但 Q Learning 算法得到的样本前后是有关系的。为了打破数据之间的关联性，Experience Replay 方法通过存储-采样的方法将这个关联性打破了。</li></ul><p>之所以加入 experience replay 是因为样本是从游戏中的连续帧获得的，这与简单的 reinforcement learning 问题（比如maze）相比，样本的关联性大了很多，如果没有 experience replay，算法在连续一段时间内基本朝着同一个方向做 gradient descent，那么同样的步长下这样直接计算 gradient 就有可能不收敛。<strong>因此 experience replay 是从一个 memory pool 中随机选取了一些 experience，然后再求梯度</strong>，从而避免了这个问题。</p><blockquote><p>原文的实验中指出mini batch是32，而replay memory存了最近的1000000帧。</p></blockquote><br>### 8.4 Fixed Q-targets<p>Fixed Q-targets 也是一种打乱相关性的机理, 如果使用 fixed Q-targets, 我们就会在 DQN 中使用到<strong>两个结构完全相同但参数不同的神经网络</strong> (有时差), 预测 Q 估计 的神经网络 <strong>(evaluate net)</strong> 具备<strong>最新的参数</strong>, 而预测 Q 现实 的神经网络 <strong>(target net)</strong> 使用的<strong>参数则是很久以前的</strong>。 </p><p>例如一开始有两个完全一样的网络，一个进行训练，另一个不训练，到了训练10000次后，把训练过的网络参数完全复制给冻结的网络，之后仍是一个训练，持续更新参数，一个冻结，每10000次才更新一次。</p><p><code>target_net</code> 用于预测 <code>q_target</code> 目标值, 他不会及时更新参数. </p><p><code>eval_net</code> 用于预测 <code>q_eval</code> 估计值, 这个神经网络拥有最新的神经网络参数</p><div align="center">![](/img/2018-08-05-RL-14.jpg)<p><img src="/img/2018-08-05-RL-19.jpg" alt></p><h3 id="8-5-总结"><a href="#8-5-总结" class="headerlink" title="8.5 总结"></a>8.5 总结</h3><p>在 Q-Learning 算法中，计算经验得分的公式如下：</p><p>$$\text{Q(state, action) = Q(state, action) + }\alpha\text{ (R(state, action) + }\gamma \text{ Max[Q(next state, all actions)] - Q(state, action))}$$</p><p>当  \(\alpha\)  的值是  \(1\)  时，公式如下：</p><p>$$\text{Q(state, action) = R(state, action) +} \gamma\text{ Max[Q(next state, all actions)]}$$</p><ul><li><code>state</code>： 表示 Agent 当前状态。</li><li><code>action</code>： 表示 Agent 在当前状态下要做的行为。</li><li><code>next state</code>： 表示 Agent 在 state 状态下执行了 action 行为后达到的新的状态。</li><li><code>Q(state, action)</code>： 表示 Agent 在 state 状态下执行了 action 行为后学习到的经验，也就是经验分数。</li><li><code>R(state, action)</code>： 表示 Agent 在 state 状态下做 action 动作后得到的即时奖励分数。</li><li><code>Max[Q(next state, all actions)]</code>： 表示 Agent 在 next state 状态下，自我的经验中，最有价值的行为的经验分数。</li><li><code>Gamma</code>：  \(\gamma\) ，表示折损率，也就是未来的经验对当前状态执行 action 的重要程度。</li></ul><br>**算法流程:**<p>Agent 通过经验去学习。Agent将会从一个状态到另一个状态这样去探索，直到它到达目标状态。我们称每一次这样的探索为一个场景（episode）。</p><p>每个场景就是 Agent 从起始状态到达目标状态的过程。每次 Agent 到达了目标状态，程序就会进入到下一个场景中。</p><ol><li><p>初始化 Q 矩阵，并将初始值设置成 0</p></li><li><p>设置好参数 γ 和得分矩阵 R</p></li><li><p>循环遍历场景（episode）：</p><ol><li><p>随机初始化一个状态 s</p></li><li><p>如果未达到目标状态，则循环执行以下几步：</p><ol><li>在当前状态 s 下，随机选择一个行为 a</li><li>执行行为 a 得到下一个状态 s`</li><li>使用  \(\text{Q(state, action) = R(state, action) +} \gamma\text{ Max[Q(next state, all actions)]}\) 公式计算  \(\text{Q(state, action)}\) </li><li>将当前状态 s 更新为 s`</li></ol></li></ol></li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://zhuanlan.zhihu.com/p/21262246" target="_blank" rel="noopener">DQN 从入门到放弃</a></li><li><a href="https://blog.csdn.net/suoyan1539/article/details/79571010" target="_blank" rel="noopener">经验回放（Experience replay）</a></li><li><a href="http://www.algorithmdog.com/drl#i" target="_blank" rel="noopener">强化学习系列之九:Deep Q Network (DQN)</a></li><li><a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/" target="_blank" rel="noopener">莫烦 PYTHON 强化学习 Reinforcement Learning</a></li></ol><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-前言&quot;&gt;&lt;a href=&quot;#1-前言&quot; class=&quot;headerlink&quot; title=&quot;1. 前言&quot;&gt;&lt;/a&gt;1. 前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;虽然将深度学习和增强学习结合的想法在几年前就有人尝试，但真正成功的开端就是DeepMind在N
      
    
    </summary>
    
      <category term="强化学习" scheme="https://hhyz.me/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://hhyz.me/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>MATLAB 入门</title>
    <link href="https://hhyz.me/2018/07/17/matlab/"/>
    <id>https://hhyz.me/2018/07/17/matlab/</id>
    <published>2018-07-16T16:00:00.000Z</published>
    <updated>2019-07-09T17:38:55.063Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-基础"><a href="#1-基础" class="headerlink" title="1 基础"></a>1 基础</h2><h3 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h3><p><code>%我是注释</code></p><h3 id="帮助"><a href="#帮助" class="headerlink" title="帮助"></a>帮助</h3><p><code>help 函数名或命令名</code></p><h3 id="清理窗口信息的命令"><a href="#清理窗口信息的命令" class="headerlink" title="清理窗口信息的命令"></a>清理窗口信息的命令</h3><ul><li><code>clc</code>：清除命令窗口内的内容，即只清扫屏幕，但不清除内存中已存在的变量</li><li><code>clf</code>：擦除 Matlab 的当前图形窗口中的图形</li><li><code>clear</code>：清除内存中的指定变量或函数</li><li><code>clear all</code>：清除内存中所有的变量和函数</li></ul><h3 id="已预定义的常量"><a href="#已预定义的常量" class="headerlink" title="已预定义的常量"></a>已预定义的常量</h3><ul><li><code>eps</code> ：计算机的最小整数</li><li><code>pi</code> ：圆周率 \(pi\) </li><li><code>inf</code>  或 <code>Inf</code>：无穷大 \(∞\) </li><li><code>NaN</code> ：不定量</li><li><code>i</code>  或  <code>j</code> ：虚数单位，但可以重新被定义为别的变量(如果没有赋值就直接使用，则默认为是虚数单位)</li></ul><br>## 2 数组<h3 id="一维数组"><a href="#一维数组" class="headerlink" title="一维数组"></a>一维数组</h3><ul><li><code>X = A ：step ：B</code></li></ul><p>当没有指定step时，系统默认 <code>step=1</code>。</p><ul><li>生成4个数，起始为1，末尾为2的等差数列</li></ul><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; linspace(<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">    <span class="number">1.0000</span>    <span class="number">1.3333</span>    <span class="number">1.6667</span>    <span class="number">2.0000</span></span><br></pre></td></tr></table></figure><ul><li><code>x=logspace(a, b, n)</code></li></ul><p>功能：logspace(a, b, n)生成一个(1xn)数组，数据的第一个元素值为a，最后一个元素为b，n是总采样点数。需要注意的是，此时产生的数组元素在10^a 到10^b上并不是均匀分布的，而形成一个对数曲线。</p><h3 id="二维数组"><a href="#二维数组" class="headerlink" title="二维数组"></a>二维数组</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; [<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>; <span class="number">4</span> <span class="number">5</span> <span class="number">6</span>; <span class="number">7</span> <span class="number">8</span> <span class="number">9</span>]</span><br><span class="line"></span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span></span><br><span class="line">     <span class="number">4</span>     <span class="number">5</span>     <span class="number">6</span></span><br><span class="line">     <span class="number">7</span>     <span class="number">8</span>     <span class="number">9</span></span><br></pre></td></tr></table></figure><h3 id="三维数组"><a href="#三维数组" class="headerlink" title="三维数组"></a>三维数组</h3><p>xx</p><h3 id="多维数组操作"><a href="#多维数组操作" class="headerlink" title="多维数组操作"></a>多维数组操作</h3><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; a = [<span class="number">1</span> <span class="number">2</span> <span class="number">3</span>];</span><br><span class="line">&gt;&gt; b = [<span class="number">4</span> <span class="number">5</span> <span class="number">6</span>];</span><br><span class="line"></span><br><span class="line">&gt;&gt; [a,b]</span><br><span class="line">&gt;&gt; </span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span>     <span class="number">4</span>     <span class="number">5</span>     <span class="number">6</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; [a;b]</span><br><span class="line"></span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span></span><br><span class="line">     <span class="number">4</span>     <span class="number">5</span>     <span class="number">6</span></span><br></pre></td></tr></table></figure><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; A = [<span class="number">1</span> <span class="number">2</span>; <span class="number">3</span> <span class="number">4</span>];</span><br><span class="line">&gt;&gt; B = [<span class="number">5</span> <span class="number">6</span>; <span class="number">7</span> <span class="number">8</span>];</span><br><span class="line">&gt;&gt; [A, B]</span><br><span class="line"></span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span>     <span class="number">5</span>     <span class="number">6</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span>     <span class="number">7</span>     <span class="number">8</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; [A;B]</span><br><span class="line"></span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span></span><br><span class="line">     <span class="number">3</span>     <span class="number">4</span></span><br><span class="line">     <span class="number">5</span>     <span class="number">6</span></span><br><span class="line">     <span class="number">7</span>     <span class="number">8</span></span><br></pre></td></tr></table></figure><h4 id="cat"><a href="#cat" class="headerlink" title="cat()"></a>cat()</h4><p>用来联结数组</p><p><code>C = cat(dim, A, B)</code>       按dim来联结A和B两个数组。</p><p><code>C = cat(dim, A1, A2, A3, ...)</code>    按dim联结所有输入的数组。</p><p><code>a=cat(3,A,B)</code> 左括号后的3表示构造出的矩阵维数；在新的矩阵中第1、2维就是A和B这两个矩阵的行数和列数，第3维是A和B这两个矩阵的矩阵个数，即为2</p><p><code>cat(2, A, B)</code> 相当于 [A, B];</p><p><code>at(1, A, B)</code> 相当于 [A; B].</p><h4 id="size"><a href="#size" class="headerlink" title="size()"></a>size()</h4><p>按照行-列-页的顺序，返回数组A每一维上的大小</p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; a</span><br><span class="line"></span><br><span class="line">a =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span></span><br><span class="line">     </span><br><span class="line">&gt;&gt; size(a)</span><br><span class="line"></span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">3</span></span><br></pre></td></tr></table></figure><h4 id="ndims"><a href="#ndims" class="headerlink" title="ndims()"></a>ndims()</h4><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;</span> a</span><br><span class="line"></span><br><span class="line">a =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;</span> ndims(a)</span><br><span class="line"></span><br><span class="line">ans =</span><br><span class="line"></span><br><span class="line">     <span class="number">2</span></span><br></pre></td></tr></table></figure><p>返回数组A具有的维度值</p><h4 id="whos"><a href="#whos" class="headerlink" title="whos"></a>whos</h4><p>返回当前工作区的各个变量的详细信息</p><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; whos</span><br><span class="line">  Name      Size            Bytes  Class     Attributes</span><br><span class="line"></span><br><span class="line">  A         <span class="number">2</span><span class="keyword">x</span><span class="number">2</span>                <span class="number">32</span>  <span class="keyword">double</span>              </span><br><span class="line">  B         <span class="number">2</span><span class="keyword">x</span><span class="number">2</span>                <span class="number">32</span>  <span class="keyword">double</span>              </span><br><span class="line">  a         <span class="number">1</span><span class="keyword">x</span><span class="number">3</span>                <span class="number">24</span>  <span class="keyword">double</span>              </span><br><span class="line">  ans       <span class="number">1</span><span class="keyword">x</span><span class="number">1</span>                 <span class="number">8</span>  <span class="keyword">double</span>              </span><br><span class="line">  b         <span class="number">1</span><span class="keyword">x</span><span class="number">3</span>                <span class="number">24</span>  <span class="keyword">double</span></span><br></pre></td></tr></table></figure><br>## 3 线性代数<h3 id="常量矩阵命令"><a href="#常量矩阵命令" class="headerlink" title="常量矩阵命令"></a>常量矩阵命令</h3><p><code>zeros(m,n)</code>：产生一个  \(m\times n\)  零矩阵<br><code>zeros(n)</code>：产生一个  \(n\)  阶零方阵<br><code>ones(m,n)</code>：产生一个所有元素为  \(1\)  的  \(m\times n\)  矩阵<br><code>ones(n)</code>：产生一个所有元素为  \(1\)  的  \(n\)  阶方阵<br><code>eye(n)</code>：产生一个  \(n\)  阶单位阵</p><h3 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h3><p>Matlab 最擅长于线性代数中关于矩阵的各种运算，常用的运算符有：</p><ol><li><code>+</code> ：两矩阵和的运算</li><li><code>-</code> ：两矩阵减的运算</li><li><code>*</code> ：两矩阵乘积的运算</li><li><code>.*</code> ：两矩阵各相应位置元素乘积的运算</li><li><code>./</code> ：两矩阵各相应位置元素相除的运算</li><li><code>A&#39;</code> ：矩阵 A 的转置</li><li><code>inv(A)</code> ：矩阵 A 的逆阵</li><li><code>eig(A)</code> ：矩阵 A 的特征值</li><li><code>[V,D]=eig(A)</code>：给出由矩阵 A 的特征向量组成的矩阵 V(以列向量排列)和由对应的特征值组成的对角阵 D(特征值为对角线元素)</li><li><code>sum(A,1)</code>：对矩阵 A 的每列元素求和，给出求和值的行向量</li><li><code>sum(A,2)</code>：对矩阵 A 的每行元素求和，给出求和值的列向量</li><li><code>sum(X)</code>：对向量 X=(x1,x2,…….,xn)的分量求和</li><li><code>cumsum(X)</code>：给出向量 X 的累加和</li><li><code>length(X)</code>：给出向量 X 的维数，即其分量的个数</li><li><code>norm(X)</code>：给出向量 X 的范数，即向量的长度</li></ol><br>## 4 结构<h3 id="if"><a href="#if" class="headerlink" title="if"></a>if</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">格式<span class="number">1</span>：</span><br><span class="line">    <span class="keyword">if</span> 条件</span><br><span class="line">        语句组</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">格式<span class="number">2</span>：</span><br><span class="line">    <span class="keyword">if</span> 条件</span><br><span class="line">        语句组<span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        语句组<span class="number">2</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">格式<span class="number">3</span>：</span><br><span class="line">    <span class="keyword">if</span> 条件<span class="number">1</span></span><br><span class="line">        语句组<span class="number">1</span></span><br><span class="line">    <span class="keyword">elseif</span> 条件<span class="number">2</span></span><br><span class="line">        语句组<span class="number">2</span></span><br><span class="line">    .....</span><br><span class="line">    <span class="keyword">elseif</span> 条件m</span><br><span class="line">        语句组m</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        语句组m+<span class="number">1</span></span><br><span class="line">    <span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="switch"><a href="#switch" class="headerlink" title="switch"></a>switch</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">switch</span> 表达式</span><br><span class="line">    <span class="keyword">case</span> 值<span class="number">1</span></span><br><span class="line">        语句组<span class="number">1</span></span><br><span class="line">    <span class="keyword">case</span> 值<span class="number">2</span></span><br><span class="line">        语句组<span class="number">2</span></span><br><span class="line">    ....</span><br><span class="line">    <span class="keyword">case</span> 值m</span><br><span class="line">        语句组m</span><br><span class="line">    <span class="keyword">otherwise</span> <span class="comment">%其他值</span></span><br><span class="line">        语句组m+<span class="number">1</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="for"><a href="#for" class="headerlink" title="for"></a>for</h3><p>表达式1 的值是循环变量的初值，表达式2 的值是循环变量的增量，表达式3 的值为循环变量的终值。如果增量为1 时，表达式2 可以省略不写</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> 循环变量=表达式<span class="number">1</span>：表达式<span class="number">2</span>：表达式<span class="number">3</span></span><br><span class="line">    循环体语句</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="while"><a href="#while" class="headerlink" title="while"></a>while</h3><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> 条件</span><br><span class="line">    循环体语句</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><br>## 5 画图<h3 id="二维"><a href="#二维" class="headerlink" title="二维"></a>二维</h3><p><code>plot(x, y)</code></p><p><strong>例：在区间 [0,2π] 内绘制正弦曲线 y=sinx 的语句</strong></p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=<span class="number">0</span>:<span class="built_in">pi</span>/<span class="number">100</span>:<span class="number">2</span>*<span class="built_in">pi</span>;</span><br><span class="line">y=<span class="built_in">sin</span>(x);</span><br><span class="line"><span class="built_in">plot</span>(x,y)</span><br></pre></td></tr></table></figure><p>指定线型与颜色</p><p><code>plot(x,y1,&#39;cs1&#39;,x,y2,&#39;cs2&#39;,....)</code></p><p>它以公共向量 x 为 X 轴，分别以 y1,y2，… 为 Y 轴在同一幅图内绘制出多条曲线，同时可以指定它们的不同颜色与不同线性。</p><p>每条曲线的颜色和线型用字符串 ‘cs’ 来指定，其中 c 表示颜色，而 s 表示线型，线型可以是线或者标记，线和标记可同时使用。它们的位置次序可随意，如缺省的话，则默认颜色为蓝色、线型为实型。它们的符号如下所示。</p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-基础&quot;&gt;&lt;a href=&quot;#1-基础&quot; class=&quot;headerlink&quot; title=&quot;1 基础&quot;&gt;&lt;/a&gt;1 基础&lt;/h2&gt;&lt;h3 id=&quot;注释&quot;&gt;&lt;a href=&quot;#注释&quot; class=&quot;headerlink&quot; title=&quot;注释&quot;&gt;&lt;/a&gt;注释&lt;/h
      
    
    </summary>
    
      <category term="编程语言" scheme="https://hhyz.me/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/"/>
    
    
      <category term="MATLAB" scheme="https://hhyz.me/tags/MATLAB/"/>
    
  </entry>
  
  <entry>
    <title>SSD 目标检测（七）</title>
    <link href="https://hhyz.me/2018/07/07/SSD/"/>
    <id>https://hhyz.me/2018/07/07/SSD/</id>
    <published>2018-07-06T16:00:00.000Z</published>
    <updated>2019-07-09T17:06:06.855Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>论文地址：<a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="noopener">SSD: Single Shot MultiBox Detector</a></p></blockquote><h2 id="1-设计理念"><a href="#1-设计理念" class="headerlink" title="1 设计理念"></a>1 设计理念</h2><h3 id="1-1-采用卷积进行检测"><a href="#1-1-采用卷积进行检测" class="headerlink" title="1.1 采用卷积进行检测"></a>1.1 采用卷积进行检测</h3><p>与Yolo最后采用全连接层不同，SSD直接采用卷积对不同的特征图来进行提取检测结果。对于形状为  \(m\times n \times p\)  的特征图，只需要采用  \(3\times 3 \times p\)  这样比较小的卷积核得到检测值。</p><br><h3 id="1-2-设置先验框"><a href="#1-2-设置先验框" class="headerlink" title="1.2 设置先验框"></a>1.2 设置先验框</h3><p>而SSD借鉴了Faster R-CNN中anchor的理念，每个单元设置尺度或者长宽比不同的先验框，预测的边界框（bounding boxes）是以这些先验框为基准的，在一定程度上减少训练难度。一般情况下，每个单元会设置多个先验框，其尺度和长宽比存在差异，如图5所示，可以看到每个单元使用了4个不同的先验框，图片中猫和狗分别采用最适合它们形状的先验框来进行训练，后面会详细讲解训练过程中的先验框匹配原则。</p><div align="center">![](/img/2018-07-07-SSD-1.jpg)<center><small><font color="gray">  SSD的先验框  </font></small></center></div><p>对于每个单元的每个先验框，都有两个不同的检验值。</p><p>第一部分是<strong>各个类别的置信度或者评分</strong>，在预测过程中，置信度最高的那个类别就是边界框所属的类别，特别地，当第一个置信度值最高时，表示边界框中并不包含目标。</p><blockquote><p>值得注意的是 SSD 将背景也当做了一个特殊的类别，如果检测目标共有  \(c\)  个类别，SSD 其实需要预测  \(c+1\)  个置信度值，其中第一个置信度指的是不含目标或者属于背景的评分。后面当我们说  \(c\)  个类别置信度时，请记住里面包含背景那个特殊的类别，即真实的检测类别只有  \(c-1\)  个。</p></blockquote><p>第二部分就是<strong>边界框的 location</strong>，包含4个值  \((cx, cy, w, h)\)  ，分别表示边界框的中心坐标以及宽高。</p><p>但是真实预测值其实只是边界框相对于先验框的转换值。先验框位置用  \(d=(d^{cx}, d^{cy}, d^w, d^h) \) 表示，其对应边界框用  \(b=(b^{cx}, b^{cy}, b^w, b^h)\)  表示，那么边界框的预测值  \(l\)  其实是  \(b\)  相对于  \(d\)  的转换值：</p><p>$$l^{cx} = (b^{cx} - d^{cx})/d^w, \space l^{cy} = (b^{cy} - d^{cy})/d^h$$</p><p>$$l^{w} = \log(b^{w}/d^w), \space l^{h} = \log(b^{h}/d^h)$$</p><p>习惯上，我们称上面这个过程为边界框的编码（encode），预测时，你需要反向这个过程，即进行解码（decode），从预测值 l 中得到边界框的真实位置 b ：</p><p>$$b^{cx}=d^w l^{cx} + d^{cx}, \space b^{cy}=d^y l^{cy} + d^{cy}$$</p><p>$$b^{w}=d^w \exp(l^{w}), \space b^{h}=d^h \exp(l^{h})$$</p><br><p><strong>综上所述</strong>，对于一个大小  \(m\times n\)  的特征图，共有  \(mn\)  个单元，每个单元设置的先验框数目记为  \(k\)  ，那么每个单元共需要  \((c+4)k\)  个预测值，所有的单元共需要  \((c+4)kmn\)  个预测值，由于 SSD 采用卷积做检测，所以就需要  \((c+4)k\)  个卷积核完成这个特征图的检测过程</p><br><h3 id="1-3-网络结构"><a href="#1-3-网络结构" class="headerlink" title="1.3 网络结构"></a>1.3 网络结构</h3><div align="center">![](/img/2018-07-07-SSD-2.jpg)<center><small><font color="gray">    </font></small></center></div><blockquote><p><strong>SSD利用了多尺度的特征图做检测</strong></p><p>对每次卷积后输出的 \(38×38,19×19\) 等大小的特征图都保存下来，并都进行进一步的操作</p></blockquote><p>SSD采用VGG16作为基础模型，然后在 VGG-16 的基础上新增了卷积层来获得更多的特征图以用于检测。分别将VGG16的全连接层 fc6 和 fc7 转换成  \(3\times3\)  卷积层 conv6 和  \(1\times1 \) 卷积层 conv7，为了配合这种变化，采用了一种 <code>Atrous Algorithm</code>，其实就是 conv6 采用<strong>扩展卷积或带孔卷积</strong>（Dilation Conv），其在不增加参数与模型复杂度的条件下指数级扩大卷积的视野，其使用扩张率(dilation rate)参数，来表示扩张的大小，</p><div align="center">![](/img/2018-07-07-SSD-3.jpg)<center><small><font color="gray">    </font></small></center></div><ul><li>(a)是普通的 \( 3\times3 \) 卷积，其视野就是 \( 3\times3 \) </li><li>(b)是扩张率为1，此时视野变成  \(7\times7\)  </li><li>(c)扩张率为3时，视野扩大为  \(15\times15\)  ，但是视野的特征更稀疏了。</li></ul><p>Conv6 采用 \( 3\times3 \) 大小但  \(\text{dilation rate=6}\)  的扩展卷积。</p><p>之后移除 dropout 层和 fc8 层，并新增一系列卷积层，在检测数据集上做 finetuing。其中 VGG16 中的 Conv4_3 层将作为用于检测的第一个特征图。</p><blockquote><p>conv4_3层特征图大小是 38\times38 ，但是该层比较靠前，其norm较大，所以在其后面增加了一个 <strong>L2 Normalization</strong> 层，以保证和后面的检测层差异不是很大，这个和 Batch Normalization 层不太一样，其仅仅是对每个像素点在 channle 维度做归一化，而 Batch Normalization 层是在  \(\text{[batch_size, width, height]}\)  三个维度上做归一化。归一化后一般设置一个可训练的放缩变量 gamma，使用TF可以这样简单实现</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># l2norm (not bacth norm, spatial normalization)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">l2norm</span><span class="params">(x, scale, trainable=True, scope=<span class="string">"L2Normalization"</span>)</span>:</span></span><br><span class="line">    n_channels = x.get_shape().as_list()[<span class="number">-1</span>]</span><br><span class="line">    l2_norm = tf.nn.l2_normalize(x, [<span class="number">3</span>], epsilon=<span class="number">1e-12</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope):</span><br><span class="line">        gamma = tf.get_variable(<span class="string">"gamma"</span>, shape=[n_channels, ], dtype=tf.float32,</span><br><span class="line">                                initializer=tf.constant_initializer(scale),</span><br><span class="line">                                trainable=trainable)</span><br><span class="line">        <span class="keyword">return</span> l2_norm * gamma</span><br></pre></td></tr></table></figure><h2 id="2-训练过程"><a href="#2-训练过程" class="headerlink" title="2 训练过程"></a>2 训练过程</h2><h3 id="先验框匹配"><a href="#先验框匹配" class="headerlink" title="先验框匹配"></a>先验框匹配</h3><p>在训练过程中，首先要确定训练图片中的ground truth（真实目标）与哪个先验框来进行匹配，与之匹配的先验框所对应的边界框将负责预测它。</p><p>在Yolo中，ground truth的中心落在哪个单元格，该单元格中与其IOU最大的边界框负责预测它。但是在SSD中却完全不一样，SSD的先验框与ground truth的匹配原则主要有两点。</p><p><strong>第一个原则是：</strong>对于图片中每个ground truth，找到与其IOU最大的先验框，该先验框与其匹配，这样，可以保证每个ground truth一定与某个先验框匹配。</p><blockquote><p>通常称与ground truth匹配的先验框为正样本（其实应该是先验框对应的预测box，不过由于是一一对应的就这样称呼了），反之，若一个先验框没有与任何ground truth进行匹配，那么该先验框只能与背景匹配，就是负样本。</p><p>一个图片中ground truth是非常少的， 而先验框却很多，如果仅按第一个原则匹配，很多先验框会是负样本，正负样本极其不平衡，所以需要第二个原则。</p></blockquote><p><strong>第二个原则是：</strong>对于剩余的未匹配先验框，若某个 ground truth 的  \(\text{IOU}\)  大于某个阈值（一般是0.5），那么该先验框也与这个 ground truth 进行匹配。</p><blockquote><p>这意味着某个 ground truth 可能与多个先验框匹配，这是可以的。但是反过来却不可以，<strong>因为一个先验框只能匹配一个 ground truth</strong>，如果多个 ground truth 与某个先验框 \( \text{IOU} \) 大于阈值，那么先验框只与 \( \text{IOU} \) 最大的那个先验框进行匹配。</p><p>第二个原则一定在第一个原则之后进行</p></blockquote><div align="center">![](/img/2018-07-07-SSD-4.jpg)<center><small><font color="gray"> 绿色的GT是ground truth，红色为先验框，FP表示负样本，TP表示正样本   </font></small></center></div><p>尽管一个 ground truth 可以与多个先验框匹配，但是 ground truth 相对先验框还是太少了，所以负样本相对正样本会很多。为了保证正负样本尽量平衡，SSD 采用了 <code>hard negative mining</code>，就是<strong>对负样本进行抽样</strong>，抽样时按照置信度误差（预测背景的置信度越小，误差越大）进行降序排列，选取误差的较大的top-k作为训练的负样本，以保证正负样本比例接近 \(1:3\) 。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>损失函数定义为<strong>位置误差</strong>（locatization loss， loc）与<strong>置信度误差</strong>（confidence loss, conf）的加权和：</p><p><font size="+1">  $$L(x, c, l, g) = \frac{1}{N}(L_{conf}(x,c) + \alpha L_{loc}(x,l,g))$$   </font></p><ul><li>\(N\)  是先验框的正样本数量</li><li>\(x^p_{ij}\in { 1,0 } \) 为一个指示参数，当  \(x^p_{ij}= 1\)  时表示第  \(i\)  个先验框与第  \(j\)  个 ground truth 匹配，并且 ground truth 的类别为  \(p\)  </li><li>\(c\)  为类别置信度预测值 </li><li>\(l\)  为先验框的所对应边界框的位置预测值</li><li>\(g\)  是 ground truth 的位置参数</li></ul><p>对于<strong>位置误差</strong>，其采用 Smooth L1 loss，定义如下：</p><div align="center">![](/img/2018-07-07-SSD-5.jpg)<center><small><font color="gray">    </font></small></center></div><font size="+1">  $$smooth\_{L\_{1}}(x) =\begin{cases}0.5x^2, & \text{if |x| < 1}\\\\|x|-0.5, & \text{otherwise}\end{cases}$$  </font><p>对于<strong>置信度误差</strong>，其采用 softmax loss:</p><div align="center">![](/img/2018-07-07-SSD-6.jpg)<center><small><font color="gray">    </font></small></center></div><h3 id="数据扩增"><a href="#数据扩增" class="headerlink" title="数据扩增"></a>数据扩增</h3><br><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><p><a href="https://zhuanlan.zhihu.com/p/33544892" target="_blank" rel="noopener">目标检测|SSD原理与实现</a></p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1512.02325&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SSD: Single Shot MultiBox Detector&lt;/a&gt;&lt;/p&gt;
&lt;/
      
    
    </summary>
    
      <category term="深度学习" scheme="https://hhyz.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="目标检测" scheme="https://hhyz.me/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>L0 L1 L2 正则化</title>
    <link href="https://hhyz.me/2018/06/29/regularization/"/>
    <id>https://hhyz.me/2018/06/29/regularization/</id>
    <published>2018-06-28T16:00:00.000Z</published>
    <updated>2019-07-02T17:05:20.654Z</updated>
    
    <content type="html"><![CDATA[<h2 id="范数"><a href="#范数" class="headerlink" title="范数"></a>范数</h2><p><font size="+1">  $$\Vert x \Vert_p:=\left(\sum^n_{i=1}\vert x_i \vert^p\right)^{\frac {1}{p}}$$<br>   </font></p><ul><li>\(L1\ 范数：当\ p=1\ 时，表示某个向量中所有元素绝对值之和\) </li><li>\(L2\ 范数：当\ p=2\ 时，表示某个向量中所有元素平方和再开根， 也就是欧几里得距离公式\)    </li></ul><p>对于线性回归模型，使用 L1 正则化的模型建叫做 <strong>Lasso 回归</strong>，使用 L2 正则化的模型叫做 <strong>Ridge 回归（岭回归）</strong>。</p><h2 id="L1"><a href="#L1" class="headerlink" title="L1"></a>L1</h2><p>$$ \mathop{argmin} \limits_{w}{\frac{1}{2n_{samples}}} \Vert X_w - y\Vert^2_2+\alpha\Vert w \Vert_1$$</p><p><strong>作用</strong></p><p>L1 正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择，一定程度上，L1也可以防止过拟合。</p><p>通常越大的 \(\lambda\) 可以让代价函数在参数为0时取到最小值</p><blockquote><p>稀疏矩阵指的是很多元素为 0，只有少数元素是非零值的矩阵，即得到的线性回归模型的大部分系数都是 0。</p><p> 通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。<br>在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，或者贡献微小（因为它们前面的系数是0或者是很小的值，即使去掉对模型也没有什么影响），此时我们就可以只关注系数是非零值的特征。这就是稀疏模型与特征选择的关系。</p></blockquote><h2 id="L2"><a href="#L2" class="headerlink" title="L2"></a>L2</h2><p><strong>岭回归计算公式</strong></p><p>$$ \mathop{argmin} \limits_{w}{\frac{1}{2n_{samples}}} \Vert X_w - y\Vert^2_2+\alpha\Vert w \Vert_2^2$$</p><p>$$J(\theta) = {\frac{1}{2m}}\left[\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum^{n}_{j=1}\theta^2_j\right]$$</p><p>如果发生过拟合， 参数 \(\theta\) 一般是比较大的值， 加入惩罚项后， 只要控制 \(\lambda\) 的大小，当 \(\lambda\) 很大时， \(\theta_1\) 到 \(\theta_n\) 就会很小，即达到了约束数量庞大的特征的目的。</p><p><strong>作用</strong></p><p>L2正则化可以防止模型过拟合（overfitting）</p><blockquote><p>拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。</p><p>因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』。</p></blockquote><h2 id="L0"><a href="#L0" class="headerlink" title="L0"></a>L0</h2><p><strong>L0范数是指向量中非0的元素的个数。</strong></p><p>如果我们用 L0 范数来规则化一个参数矩阵 W 的话，就是希望 W 的大部分元素都是 0，换句话说，就是让参数W是稀疏的。</p><blockquote><p>通常使参数稀疏都是用 L1 范数实现，L1 范数也有个美称叫“稀疏规则算子”（Lasso regularization）。既然 L0 可以实现稀疏，为什么不用 L0，而要用 L1 呢？个人理解一是因为 L0 范数很难优化求解（NP难问题），二是 L1 范数是 L0 范数的最优凸近似，而且它比L0范数要容易优化求解。</p></blockquote><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;范数&quot;&gt;&lt;a href=&quot;#范数&quot; class=&quot;headerlink&quot; title=&quot;范数&quot;&gt;&lt;/a&gt;范数&lt;/h2&gt;&lt;p&gt;&lt;font size=&quot;+1&quot;&gt;  $$\Vert x \Vert_p:=\left(\sum^n_{i=1}\vert x_i \vert
      
    
    </summary>
    
      <category term="深度学习" scheme="https://hhyz.me/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="https://hhyz.me/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
</feed>
